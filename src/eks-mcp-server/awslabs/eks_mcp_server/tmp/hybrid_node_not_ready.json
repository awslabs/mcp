{
  "Troubleshooting Steps": [
    "Verify that a compatible CNI (Cilium or Calico) is installed and running with 'kubectl get pods -n kube-system -l k8s-app=cilium' or 'kubectl get pods -n kube-system -l k8s-app=calico-node' and check their status",
    "Check CNI pod logs for error messages and configuration issues with 'kubectl logs -n kube-system <pod-name>' where pod-name is from the previous command result",
    "Verify CNI configuration matches the RemotePodNetwork CIDR configured for the cluster using the 'get_hybrid_node_vpc_config' tool to retrieve CIDRs, then compare with 'kubectl get configmap -n kube-system cilium-config -o yaml' or 'kubectl get ippools.crd.projectcalico.org -o yaml'",
    "For Cilium, check BGP status with 'kubectl exec -n kube-system <cilium-pod-name> -- cilium bgp peers' and 'kubectl exec -n kube-system <cilium-pod-name> -- cilium bgp routes' if using BGP",
    "For Calico, verify IP pools match the expected pod CIDR range with 'kubectl get ippools.crd.projectcalico.org -o yaml' and compare with the RemotePodNetwork CIDR",
    "Check for pending Certificate Signing Requests (CSRs) that need approval with 'kubectl get csr' and approve them if needed with 'kubectl certificate approve <name>'",
    "Verify route table configurations in AWS VPC for RemoteNodeNetwork and RemotePodNetwork CIDRs using the 'get_hybrid_node_vpc_config' tool to check if routes exist for pod and node networks",
    "Examine security group rules with 'aws ec2 describe-security-groups --group-ids <sg-id>' to ensure proper communication between the EKS control plane and hybrid nodes. The control plane security group ID can be found with 'aws eks describe-cluster --name <cluster-name> --query cluster.resourcesVpcConfig.clusterSecurityGroupId'",
    "Examine kubelet systemd unit file with 'cat /etc/systemd/system/kubelet.service' and 'cat /etc/systemd/system/kubelet.service.d/*.conf' to verify configuration",
    "Check kubelet startup flags and environment variables using 'ps -ef | grep kubelet' and 'sudo systemctl show kubelet -p Environment' to verify proper cluster endpoint, certificate paths, and network plugin settings",
    "Review kubelet logs with 'journalctl -u kubelet | grep -E \"Error|Failed|Warning\"' to identify specific errors causing the NotReady state, or use 'get_k8s_events' tool with kind='Node' to check node-related events",
    "Verify that kubelet has proper permissions to access required certificate files and directories with 'ls -la /var/lib/kubelet/pki/' and 'ls -la /etc/kubernetes/pki/' to check commonly used certificate paths",
    "Restart kubelet on the hybrid node with 'sudo systemctl restart kubelet' if node conditions indicate kubelet issues, then check status with 'kubectl get node <node-name> -o yaml' to verify conditions"
  ],
  "description": "Hybrid nodes successfully register with the Amazon EKS cluster but remain in NotReady status. This typically occurs due to Container Network Interface (CNI) issues, network connectivity problems between the control plane and kubelet, or certificate issues. A compatible CNI (Cilium or Calico) must be properly configured for hybrid nodes to become ready.",
  "symptoms": [
    "Node appears in cluster but status shows NotReady",
    "NetworkUnavailable=true in node conditions",
    "CNI pods crash or fail to start",
    "Pod network communication fails",
    "Messages about network plugin not ready in kubelet logs",
    "Connection timeouts between control plane and kubelet",
    "KubeletNotReady events in node events",
    "Failed to contact API server when waiting for CSR approval",
    "Kubelet service fails to start or frequently restarts",
    "Authentication errors in kubelet logs when communicating with API server",
    "Certificate path or permission errors in kubelet logs",
    "Kubelet reporting incorrect node IP or hostname configuration"
  ],
  "references": [
    "https://docs.aws.amazon.com/eks/latest/userguide/hybrid-nodes-cni.html",
    "https://docs.aws.amazon.com/eks/latest/userguide/hybrid-nodes-networking.html"
  ]
}
