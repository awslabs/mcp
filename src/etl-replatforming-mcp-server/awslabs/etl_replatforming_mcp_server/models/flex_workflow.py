# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional


@dataclass
class TriggerConfig:
    """Data availability or event trigger configuration"""

    trigger_type: str  # "file", "s3", "sql", "http", "event"
    target: str  # File path, S3 key, SQL query, URL, event pattern
    poke_interval: int = 300  # seconds
    timeout: int = 3600  # seconds


@dataclass
class Schedule:
    """Enhanced schedule with time-based and data-based triggers"""

    # Time-based scheduling
    type: str  # "cron", "rate", "event", "manual"
    expression: Optional[str] = None  # cron/rate expression
    timezone: Optional[str] = 'UTC'
    start_date: Optional[str] = None
    end_date: Optional[str] = None

    # Data/event-based triggers
    data_triggers: List[TriggerConfig] = field(default_factory=list)
    event_triggers: List[Dict[str, Any]] = field(default_factory=list)

    # Execution control
    catchup: bool = True
    max_active_runs: int = 1


@dataclass
class DataSource:
    """Data source/sink configuration"""

    type: str  # e.g., "sql_server", "blob_storage", "s3", "redshift"
    connection_string: Optional[str] = None
    dataset: Optional[str] = None
    table: Optional[str] = None
    query: Optional[str] = None
    parameters: Dict[str, Any] = field(default_factory=dict)


@dataclass
class LoopItems:
    """Loop items configuration"""

    source: str  # "static", "task_output", "range"
    values: Optional[List[Any]] = None  # Static values
    task_id: Optional[str] = None  # Source task ID
    path: Optional[str] = None  # JSONPath for task output
    start: Optional[int] = None  # Range start
    end: Optional[int] = None  # Range end
    step: Optional[int] = 1  # Range step


@dataclass
class TaskLoop:
    """Task loop configuration"""

    type: str  # "for_each", "range", "while"
    items: Optional[LoopItems] = None  # For for_each and range loops
    condition: Optional[str] = None  # For while loops
    item_variable: str = 'item'  # Variable name for current item
    max_iterations: Optional[int] = None  # Safety limit
    max_concurrency: Optional[int] = None  # Parallel execution limit
    on_failure: str = 'fail'  # "fail", "continue", "break"


@dataclass
class TaskDependency:
    """Task dependency representation with conditional logic support"""

    task_id: str
    condition: str = 'success'  # success, failure, always, or expression
    condition_type: str = 'status'  # "status", "expression"
    condition_path: Optional[str] = None  # JSONPath for output values


@dataclass
class TaskGroup:
    """Task logical grouping"""

    group_id: str
    tooltip: Optional[str] = None
    parent_group: Optional[str] = None


@dataclass
class Task:
    """Generic task representation"""

    id: str
    name: str
    type: str  # Free-form string, not restricted to enum
    command: Optional[str] = None
    script: Optional[str] = None
    parameters: Dict[str, Any] = field(default_factory=dict)
    timeout: Optional[int] = None
    retries: int = 0
    retry_delay: int = 300
    depends_on: List['TaskDependency'] = field(default_factory=list)
    loop: Optional[TaskLoop] = None  # Loop configuration
    # Enhanced attributes for better framework support
    source: Optional[DataSource] = None  # For copy/data movement tasks
    sink: Optional[DataSource] = None  # For copy/data movement tasks
    connection_id: Optional[str] = None  # Connection reference (replaces linked_service)
    compute_target: Optional[str] = None  # Execution environment
    resource_requirements: Dict[str, Any] = field(default_factory=dict)  # CPU, memory, etc.
    script_file: Optional[str] = (
        None  # Path/name of external script file for orchestration platforms
    )
    ai_generated: bool = False  # Mark if task was generated by AI
    ai_confidence: Optional[float] = None  # AI confidence score (0.0-1.0) for AI-generated content

    # Universal orchestration features
    task_group: Optional[TaskGroup] = None  # Logical grouping
    trigger_rule: str = (
        'all_success'  # all_success, all_failed, one_success, one_failed, none_failed
    )
    batch_size: Optional[int] = None  # For parallel processing
    monitoring_metrics: List[str] = field(default_factory=list)  # Metric names to publish

    def __post_init__(self):
        """Auto-generate ID if not provided"""
        if not self.id:
            # Generate ID from name, sanitized
            self.id = self.name.lower().replace(' ', '_').replace('-', '_')
            # Remove special characters
            import re

            self.id = re.sub(r'[^a-zA-Z0-9_]', '', self.id)


@dataclass
class ErrorHandling:
    """Error handling configuration"""

    on_failure: str = 'fail'  # fail, continue, retry
    notification_emails: List[str] = field(default_factory=list)
    max_retries: int = 0
    retry_delay: int = 300
    # Enhanced error handling
    notification_webhooks: List[str] = field(default_factory=list)
    escalation_policy: Optional[str] = None
    custom_error_handlers: Dict[str, str] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization"""
        return {
            'on_failure': self.on_failure,
            'notification_emails': self.notification_emails,
            'max_retries': self.max_retries,
            'retry_delay': self.retry_delay,
            'notification_webhooks': self.notification_webhooks,
            'escalation_policy': self.escalation_policy,
            'custom_error_handlers': self.custom_error_handlers,
        }


@dataclass
class ParsingInfo:
    """Information about parsing completeness and ignored elements"""

    source_framework: str
    parsing_completeness: float
    parsing_method: str = 'deterministic'
    ignored_elements: List[Dict[str, Any]] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    suggestions: List[str] = field(default_factory=list)
    llm_confidence: Optional[float] = None  # AI parsing confidence (0.0-1.0)
    llm_enhancements: List[str] = field(default_factory=list)  # What LLM enhanced
    missing_fields: List[str] = field(default_factory=list)  # Fields that need AI enhancement


@dataclass
class FlexWorkflow:
    """FLEX workflow representation for conversion between frameworks"""

    name: str
    description: Optional[str] = None
    schedule: Optional[Schedule] = None
    tasks: List[Task] = field(default_factory=list)

    error_handling: Optional[ErrorHandling] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    # Enhanced workflow attributes
    parameters: Dict[str, Any] = field(default_factory=dict)  # Workflow parameters
    variables: Dict[str, Any] = field(default_factory=dict)  # Workflow variables
    concurrency: Optional[int] = None  # Max concurrent executions
    tags: List[str] = field(default_factory=list)  # Workflow tags/labels
    parsing_info: Optional[ParsingInfo] = None  # Parsing completeness information

    # Universal orchestration features
    task_groups: List[TaskGroup] = field(default_factory=list)  # Logical task groupings
    connections: Dict[str, str] = field(default_factory=dict)  # connection_id -> connection_type
    global_timeout: Optional[int] = None  # Workflow timeout in seconds

    def get_missing_fields(self) -> List[str]:
        """Return list of missing REQUIRED fields that need AI enhancement

        Note: Schedule and error_handling are now OPTIONAL since many frameworks
        store scheduling externally (EventBridge, Data Factory triggers, etc.)
        """
        missing = []

        # Check for placeholder commands that need AI enhancement
        for task in self.tasks:
            # Branch tasks don't need commands since they're just routing logic
            if task.type == 'branch':
                continue

            has_command = task.command and task.command.strip() and task.command.strip() != '?'
            has_script = task.script and task.script.strip() and task.script.strip() != '?'

            if not has_command and not has_script:
                missing.append('task_commands')
                break
            elif has_command and task.command and self._is_placeholder_command(task.command):
                missing.append('task_commands')
                break

        # Check for missing parameters (if referenced in tasks)
        if not self.parameters and self._has_parameter_references():
            missing.append('parameters')

        # Check for missing mandatory orchestration fields
        missing.extend(self._get_missing_mandatory_orchestration_fields())

        return list(set(missing))  # Remove duplicates

    def _get_missing_mandatory_orchestration_fields(self) -> List[str]:
        """Check for missing mandatory orchestration fields that impact generator quality

        Note: After analysis, no orchestration fields are truly universal and mandatory.
        - connection_id: Only Airflow uses explicit connection IDs
        - trigger_rule: Only Airflow has internal trigger rules
        - task_groups: Only Airflow has explicit task grouping

        Other frameworks handle these concepts differently:
        - Step Functions: Embeds connection info in resource parameters
        - ADF: Uses external linkedServiceName references
        """
        return []  # No truly universal mandatory orchestration fields

    def get_missing_orchestration_features(self) -> List[str]:
        """Return list of missing advanced orchestration features that could be enhanced"""
        # TODO: Implement advanced orchestration feature detection
        # Should analyze connection_id, trigger_rules, task_groups, monitoring_metrics
        # and return list of missing features that AI could enhance
        return []

    def get_missing_optional_fields(self) -> List[str]:
        """Return list of missing OPTIONAL fields that could be enhanced by AI"""
        # TODO: Implement optional field enhancement
        # Should detect missing schedule, error_handling, parameters, variables
        # and return list for AI enhancement of optional workflow features
        return []

    def _has_parameter_references(self) -> bool:
        """Check if tasks reference parameters that don't exist"""
        for task in self.tasks:
            if task.command and ('${' in task.command or '{{' in task.command):
                return True
            if task.script and ('${' in task.script or '{{' in task.script):
                return True
        return False

    def _is_placeholder_command(self, command: str) -> bool:
        """Check if command is a placeholder that needs AI enhancement"""
        if not command:
            return True

        command = command.strip().lower()

        # Common placeholder patterns
        placeholder_patterns = [
            '?',
            'todo',
            'tbd',
            'placeholder',
            'fill_me',
            'replace_me',
            'invoke_function()',
            'run_batch_job()',
            'execute_task()',
            'process_data()',
        ]

        # Check for exact matches or very short/generic commands
        if command in placeholder_patterns or len(command) < 5:
            return True

        # Check for generic function calls without parameters
        import re

        if re.match(r'^[a-z_]+\(\)$', command):
            return True

        return False

    def is_complete(self) -> bool:
        """Check if workflow has all required information for conversion

        A workflow is complete if:
        1. Deterministic parsing achieved 100% completeness, OR
        2. No required fields are missing (after AI enhancement)
        """
        # Check deterministic parsing completeness first
        if self.parsing_info and self.parsing_info.parsing_completeness >= 1.0:
            return True

        # Fall back to missing fields check (for AI-enhanced workflows)
        return len(self.get_missing_fields()) == 0

    def get_task_by_id(self, task_id: str) -> Optional[Task]:
        """Get task by ID"""
        return next((task for task in self.tasks if task.id == task_id), None)

    def to_dict_with_placeholders(
        self, missing_fields: List[str], target_framework: Optional[str] = None
    ) -> Dict[str, Any]:
        """Convert FlexWorkflow to dictionary with '?' placeholders and descriptions for missing fields"""
        result = self.to_dict()

        # Add placeholders with descriptions for missing fields
        for missing_field in missing_fields:
            if missing_field == 'schedule_configuration':
                result['schedule'] = {
                    'type': 'cron',
                    'expression': '? # REQUIRED: Cron expression (e.g., "0 9 * * *" for daily at 9 AM, "0 */6 * * *" for every 6 hours)',
                    'timezone': 'UTC',
                    '_description': 'Schedule defines when this workflow runs. Use standard cron format: minute hour day month weekday',
                }
            elif missing_field.startswith('tasks[') and '].command_or_script' in missing_field:
                # Extract task index from "tasks[3].command_or_script"
                import re

                match = re.search(r'tasks\[(\d+)\]\.command_or_script', missing_field)
                if match:
                    task_index = int(match.group(1))
                    if task_index < len(result.get('tasks', [])):
                        task = result['tasks'][task_index]
                        task_type = task.get('type', 'unknown')
                        task['command'] = self._get_target_aware_placeholder(
                            task_type, target_framework
                        )
            elif missing_field.startswith('task_') and missing_field.endswith(
                '_execution_details'
            ):
                # Extract task_id safely
                parts = missing_field.split('_')
                if len(parts) >= 2:
                    task_id = parts[1]
                    # Find the task and add placeholder
                    for task in result.get('tasks', []):
                        if task['id'] == task_id:
                            task['command'] = self._get_target_aware_placeholder(
                                task['type'], target_framework
                            )
            elif missing_field.startswith('task_') and missing_field.endswith('_sql_query'):
                # Extract task_id safely
                parts = missing_field.split('_')
                if len(parts) >= 2:
                    task_id = parts[1]
                    for task in result.get('tasks', []):
                        if task['id'] == task_id:
                            task['command'] = self._get_target_aware_placeholder(
                                task['type'], target_framework
                            )
            elif missing_field == 'task_commands':
                # Handle generic task_commands missing field by adding placeholders to all tasks with missing commands
                for task in result.get('tasks', []):
                    if not task.get('command') or task.get('command', '').strip() in [
                        '',
                        '?',
                    ]:
                        task['command'] = self._get_target_aware_placeholder(
                            task['type'], target_framework
                        )
            elif missing_field == 'schedule':
                # Add schedule placeholder
                result['schedule'] = {
                    'type': 'cron',
                    'expression': '? # REQUIRED: Cron expression (e.g., "0 9 * * *" for daily at 9 AM, "0 */6 * * *" for every 6 hours)',
                    'timezone': 'UTC',
                    '_description': 'Schedule defines when this workflow runs. Use standard cron format: minute hour day month weekday',
                }
            elif missing_field == 'error_handling':
                # Add error handling placeholder
                result['error_handling'] = {
                    'on_failure': '? # REQUIRED: "fail", "continue", or "retry"',
                    'notification_emails': [],
                    'max_retries': 0,
                    'retry_delay': 300,
                    '_description': 'Error handling configuration for workflow failures',
                }
            elif missing_field == 'parameters':
                # Add parameters placeholder
                result['parameters'] = {
                    '?': '? # REQUIRED: Add workflow parameters referenced in tasks',
                    '_description': 'Workflow parameters that can be referenced in task commands using ${param_name} syntax',
                }
            # Note: No orchestration-specific placeholders needed since no fields are truly universal mandatory

        return result

    def _get_target_aware_placeholder(
        self, task_type: str, target_framework: Optional[str]
    ) -> str:
        """Generate target-framework-aware placeholder for missing task commands"""
        base_msg = 'See FLEX_SPECIFICATION.md for framework mapping details'

        if target_framework == 'airflow':
            if task_type == 'sql':
                return f'? # REQUIRED: SQL query for RedshiftSQLOperator or PostgreSQLOperator. {base_msg}'
            elif task_type == 'python':
                return f'? # REQUIRED: Python function name for PythonOperator. {base_msg}'
            elif task_type == 'bash':
                return f'? # REQUIRED: Shell command for BashOperator. {base_msg}'
            elif task_type == 'ParallelTaskOperator':
                return f'? # REQUIRED: Not directly supported in Airflow - use task groups or parallel task dependencies instead. {base_msg}'
            else:
                return f'? # REQUIRED: Command/script for {task_type} (will map to PythonOperator in Airflow). {base_msg}'
        elif target_framework == 'step_functions':
            if task_type == 'sql':
                return f'? # REQUIRED: SQL query for Lambda function or Batch job. {base_msg}'
            elif task_type == 'python':
                return f'? # REQUIRED: Python code for Lambda function. {base_msg}'
            elif task_type == 'bash':
                return f'? # REQUIRED: Shell command for Batch job or ECS task. {base_msg}'
            else:
                return f'? # REQUIRED: Command/script for {task_type} (will map to Lambda or Batch in Step Functions). {base_msg}'
        else:
            # Generic placeholder when target framework is unknown
            if task_type == 'sql':
                return f'? # REQUIRED: SQL query or statement to execute. {base_msg}'
            elif task_type == 'python':
                return f'? # REQUIRED: Python script or function call. {base_msg}'
            elif task_type == 'bash':
                return f'? # REQUIRED: Shell command or script to execute. {base_msg}'
            else:
                return f'? # REQUIRED: Command/script for {task_type} task. {base_msg}'

    def to_dict(self) -> Dict[str, Any]:
        """Convert FlexWorkflow to dictionary for JSON serialization"""
        result = {
            'name': self.name,
            'description': self.description,
            'metadata': self.metadata,
            'parameters': self.parameters,
            'variables': self.variables,
            'concurrency': self.concurrency,
            'tags': self.tags,
        }

        # Handle schedule
        if self.schedule:
            result['schedule'] = {
                'type': self.schedule.type,
                'expression': self.schedule.expression,
                'timezone': self.schedule.timezone,
                'start_date': self.schedule.start_date,
                'end_date': self.schedule.end_date,
                'data_triggers': [
                    {
                        'trigger_type': dt.trigger_type,
                        'target': dt.target,
                        'poke_interval': dt.poke_interval,
                        'timeout': dt.timeout,
                    }
                    for dt in self.schedule.data_triggers
                ],
                'event_triggers': self.schedule.event_triggers,
                'catchup': self.schedule.catchup,
                'max_active_runs': self.schedule.max_active_runs,
            }

        # Handle tasks
        result['tasks'] = []
        for task in self.tasks:
            task_dict = {
                'id': task.id,
                'name': task.name,
                'type': task.type,
                'command': task.command,
                'script': task.script,
                'script_file': task.script_file,
                'parameters': task.parameters,
                'timeout': task.timeout,
                'retries': task.retries,
                'retry_delay': task.retry_delay,
                'depends_on': [
                    {
                        'task_id': dep.task_id,
                        'condition': dep.condition,
                        'condition_type': dep.condition_type,
                        'condition_path': dep.condition_path,
                    }
                    for dep in task.depends_on
                ],
                'connection_id': task.connection_id,
                'compute_target': task.compute_target,
                'resource_requirements': task.resource_requirements,
                'ai_generated': task.ai_generated,
                'ai_confidence': task.ai_confidence,
                'trigger_rule': task.trigger_rule,
                'batch_size': task.batch_size,
                'monitoring_metrics': task.monitoring_metrics,
            }

            # Handle task group
            if task.task_group:
                task_dict['task_group'] = {
                    'group_id': task.task_group.group_id,
                    'tooltip': task.task_group.tooltip,
                    'parent_group': task.task_group.parent_group,
                }

            # Handle loop configuration
            if task.loop:
                loop_dict = {
                    'type': task.loop.type,
                    'item_variable': task.loop.item_variable,
                    'max_iterations': task.loop.max_iterations,
                    'max_concurrency': task.loop.max_concurrency,
                    'on_failure': task.loop.on_failure,
                    'condition': task.loop.condition,
                }

                if task.loop.items:
                    loop_dict['items'] = {
                        'source': task.loop.items.source,
                        'values': task.loop.items.values,
                        'task_id': task.loop.items.task_id,
                        'path': task.loop.items.path,
                        'start': task.loop.items.start,
                        'end': task.loop.items.end,
                        'step': task.loop.items.step,
                    }

                task_dict['loop'] = loop_dict

            if task.source:
                task_dict['source'] = {
                    'type': task.source.type,
                    'connection_string': task.source.connection_string,
                    'dataset': task.source.dataset,
                    'table': task.source.table,
                    'query': task.source.query,
                    'parameters': task.source.parameters,
                }

            if task.sink:
                task_dict['sink'] = {
                    'type': task.sink.type,
                    'connection_string': task.sink.connection_string,
                    'dataset': task.sink.dataset,
                    'table': task.sink.table,
                    'query': task.sink.query,
                    'parameters': task.sink.parameters,
                }

            result['tasks'].append(task_dict)

        # Handle task groups
        if self.task_groups:
            result['task_groups'] = [
                {
                    'group_id': tg.group_id,
                    'tooltip': tg.tooltip,
                    'parent_group': tg.parent_group,
                }
                for tg in self.task_groups
            ]

        # Handle connections
        if self.connections:
            result['connections'] = self.connections

        # Handle global timeout
        if self.global_timeout:
            result['global_timeout'] = self.global_timeout

        # Handle error handling
        if self.error_handling:
            result['error_handling'] = {
                'on_failure': self.error_handling.on_failure,
                'notification_emails': self.error_handling.notification_emails,
                'max_retries': self.error_handling.max_retries,
                'retry_delay': self.error_handling.retry_delay,
                'notification_webhooks': self.error_handling.notification_webhooks,
                'escalation_policy': self.error_handling.escalation_policy,
                'custom_error_handlers': self.error_handling.custom_error_handlers,
            }

        # Handle parsing info
        if self.parsing_info:
            result['parsing_info'] = {
                'source_framework': self.parsing_info.source_framework,
                'parsing_completeness': self.parsing_info.parsing_completeness,
                'parsing_method': self.parsing_info.parsing_method,
                'ignored_elements': self.parsing_info.ignored_elements,
                'warnings': self.parsing_info.warnings,
                'suggestions': self.parsing_info.suggestions,
                'llm_confidence': self.parsing_info.llm_confidence,
                'llm_enhancements': self.parsing_info.llm_enhancements,
                'missing_fields': self.parsing_info.missing_fields,
            }

        return result

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'FlexWorkflow':
        """Create FlexWorkflow from dictionary with proper deserialization"""
        # Handle schedule
        schedule = None
        if data.get('schedule'):
            schedule_data = data['schedule']

            # Handle data triggers
            data_triggers = []
            for dt_data in schedule_data.get('data_triggers', []):
                data_triggers.append(
                    TriggerConfig(
                        trigger_type=dt_data['trigger_type'],
                        target=dt_data['target'],
                        poke_interval=dt_data.get('poke_interval', 300),
                        timeout=dt_data.get('timeout', 3600),
                    )
                )

            schedule = Schedule(
                type=schedule_data['type'],
                expression=schedule_data.get('expression'),
                timezone=schedule_data.get('timezone', 'UTC'),
                start_date=schedule_data.get('start_date'),
                end_date=schedule_data.get('end_date'),
                data_triggers=data_triggers,
                event_triggers=schedule_data.get('event_triggers', []),
                catchup=schedule_data.get('catchup', True),
                max_active_runs=schedule_data.get('max_active_runs', 1),
            )

        # Handle tasks
        tasks = []
        for task_data in data.get('tasks', []):
            # Handle data source/sink
            source = None
            if task_data.get('source'):
                source_data = task_data['source']
                source = DataSource(
                    type=source_data['type'],
                    connection_string=source_data.get('connection_string'),
                    dataset=source_data.get('dataset'),
                    table=source_data.get('table'),
                    query=source_data.get('query'),
                    parameters=source_data.get('parameters', {}),
                )

            sink = None
            if task_data.get('sink'):
                sink_data = task_data['sink']
                sink = DataSource(
                    type=sink_data['type'],
                    connection_string=sink_data.get('connection_string'),
                    dataset=sink_data.get('dataset'),
                    table=sink_data.get('table'),
                    query=sink_data.get('query'),
                    parameters=sink_data.get('parameters', {}),
                )

            # Handle dependencies
            depends_on = []
            for dep_data in task_data.get('depends_on', []):
                depends_on.append(
                    TaskDependency(
                        task_id=dep_data['task_id'],
                        condition=dep_data.get('condition', 'success'),
                        condition_type=dep_data.get('condition_type', 'status'),
                        condition_path=dep_data.get('condition_path'),
                    )
                )

            # Handle task group
            task_group = None
            if task_data.get('task_group'):
                tg_data = task_data['task_group']
                task_group = TaskGroup(
                    group_id=tg_data['group_id'],
                    tooltip=tg_data.get('tooltip'),
                    parent_group=tg_data.get('parent_group'),
                )

            # Handle loop configuration
            loop = None
            if task_data.get('loop'):
                loop_data = task_data['loop']
                items = None
                if loop_data.get('items'):
                    items_data = loop_data['items']
                    items = LoopItems(
                        source=items_data['source'],
                        values=items_data.get('values'),
                        task_id=items_data.get('task_id'),
                        path=items_data.get('path'),
                        start=items_data.get('start'),
                        end=items_data.get('end'),
                        step=items_data.get('step', 1),
                    )

                loop = TaskLoop(
                    type=loop_data['type'],
                    items=items,
                    condition=loop_data.get('condition'),
                    item_variable=loop_data.get('item_variable', 'item'),
                    max_iterations=loop_data.get('max_iterations'),
                    max_concurrency=loop_data.get('max_concurrency'),
                    on_failure=loop_data.get('on_failure', 'fail'),
                )

            task = Task(
                id=task_data['id'],
                name=task_data['name'],
                type=task_data['type'],  # Free-form string
                command=task_data.get('command'),
                script=task_data.get('script'),
                script_file=task_data.get('script_file'),
                parameters=task_data.get('parameters', {}),
                timeout=task_data.get('timeout'),
                retries=task_data.get('retries', 0),
                retry_delay=task_data.get('retry_delay', 300),
                depends_on=depends_on,
                loop=loop,
                source=source,
                sink=sink,
                connection_id=task_data.get('connection_id'),
                task_group=task_group,
                trigger_rule=task_data.get('trigger_rule', 'all_success'),
                batch_size=task_data.get('batch_size'),
                monitoring_metrics=task_data.get('monitoring_metrics', []),
                compute_target=task_data.get('compute_target'),
                resource_requirements=task_data.get('resource_requirements', {}),
                ai_generated=task_data.get('ai_generated', False),
                ai_confidence=task_data.get('ai_confidence'),
            )
            tasks.append(task)

        # Handle error handling
        error_handling = None
        if data.get('error_handling'):
            eh_data = data['error_handling']
            error_handling = ErrorHandling(
                on_failure=eh_data.get('on_failure', 'fail'),
                notification_emails=eh_data.get('notification_emails', []),
                max_retries=eh_data.get('max_retries', 0),
                retry_delay=eh_data.get('retry_delay', 300),
                notification_webhooks=eh_data.get('notification_webhooks', []),
                escalation_policy=eh_data.get('escalation_policy'),
                custom_error_handlers=eh_data.get('custom_error_handlers', {}),
            )

        # Handle parsing info
        parsing_info = None
        if data.get('parsing_info'):
            pi_data = data['parsing_info']
            parsing_info = ParsingInfo(
                source_framework=pi_data['source_framework'],
                parsing_completeness=pi_data['parsing_completeness'],
                parsing_method=pi_data.get('parsing_method', 'deterministic'),
                ignored_elements=pi_data.get('ignored_elements', []),
                warnings=pi_data.get('warnings', []),
                suggestions=pi_data.get('suggestions', []),
                llm_confidence=pi_data.get('llm_confidence'),
                llm_enhancements=pi_data.get('llm_enhancements', []),
                missing_fields=pi_data.get('missing_fields', []),
            )

        return cls(
            name=data['name'],
            description=data.get('description'),
            schedule=schedule,
            tasks=tasks,
            error_handling=error_handling,
            metadata=data.get('metadata', {}),
            parameters=data.get('parameters', {}),
            variables=data.get('variables', {}),
            concurrency=data.get('concurrency'),
            tags=data.get('tags', []),
            parsing_info=parsing_info,
            task_groups=[TaskGroup(**tg) for tg in data.get('task_groups', [])],
            connections=data.get('connections', {}),
            global_timeout=data.get('global_timeout'),
        )

    def is_mandatory_field_missing(self) -> bool:
        """Check if any mandatory orchestration fields are missing"""
        # TODO: Use this convenience method instead of len(get_missing_fields()) > 0
        # throughout the codebase for cleaner API
        return len(self.get_missing_fields()) > 0

    def should_trigger_ai_enhancement(self) -> bool:
        """Check if AI enhancement should be triggered

        AI enhancement should only be triggered when deterministic parsing
        is incomplete (< 100%), not when optional orchestration fields are missing.
        """
        if not self.parsing_info:
            return False

        return self.parsing_info.parsing_completeness < 1.0
